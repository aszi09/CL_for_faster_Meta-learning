{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 20:42:08.585206: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "/home/aszi/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/aszi/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(1, 2), but only accepts 2 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n",
      "/home/aszi/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(1, 2, 3, 4), but only accepts 4 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "from aa_train_utils.model_utils import create_model, save_model_params, load_model_params\n",
    "from aa_train_utils.dataset_generation import joint, uniform, f6, f5, f2, RegressionDataset , generate_noisy_split_trainingdata\n",
    "from aa_train_utils.spl_curriculum import SPL_curriculum \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util\n",
    "from jax.scipy.stats.norm import logpdf\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "import netket as nk\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock\n",
    "#from dataloader import MixtureDataset\n",
    "\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical cross entropy accuracy metrics, using the log likelihood of gaussians and a given target point y.\n",
    "\n",
    "\n",
    "def cross_entropy_error(model, params, x_context, y_context, x_target, y_target , rng , k):\n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=k, rngs={'default': rng})\n",
    "\n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "\n",
    "    # Lets compute the log likelihood of the target points given the means and stds\n",
    "\n",
    "    print(full_y.shape, y_means.shape, y_stds.shape, \"printing the shapes, they should be just an array of values\")\n",
    "    log_pdf = logpdf(full_y, jnp.squeeze(y_means),jnp.squeeze(y_stds)) \n",
    "    return -jnp.mean(log_pdf)\n",
    "\n",
    "\n",
    "\n",
    "def RMSE_means(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=k, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "    \n",
    "    return jnp.sqrt(jnp.mean((y_means - full_y)**2))\n",
    "\n",
    "\n",
    "def STD_residuals(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=k, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "\n",
    "    return abs(full_y - y_means) / y_stds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "\n",
    "    - Add validation set to the training loop and log it (In distribution , out of task distribution validations?)\n",
    "\n",
    "    - Determine how to make sure that SPL curricula and Baseline trains on the same amount of data, as currently SPL doesnt train for same steps with the same epoch number (Maybe introduce a trainined_step_number to cut the training?)\n",
    "\n",
    "    - Create the empirical cross entropy difficulty measure and use it for the validation set as well. \n",
    "\n",
    "    - SPL curriculum call with best_params or just params for the loss calculation based ordering?\n",
    "\n",
    "    - Additionally maybe increase the difficulty of the dataset a bit , for smoother learning curve, more interesting results? \n",
    "        - Could also look into creating a more diverse dataset , the split dataset generator would allow for that easily. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# There can be several scenarios with the periodic eval and the training step restriction,\n",
    "\n",
    "# Firstly , and most importantly, we must not train more than the restriction steps. \n",
    "\n",
    "# Secondly, if the eval happens and flows into and over the restriction number, the restriction will only be enforced in the next epoch. \n",
    "\n",
    "# Thirdly, if the eval is too small of a period , and it fits multuplicatively into the number of batches per epoch, it will only run once and not every period within that epoch. \n",
    "\n",
    "\n",
    "def train_spl_curriculum(dataset_key_int,dataloader_key_int, dataset_size, training_step_number, eval_dataset_size, eval_intervals, sampler_ratios, chunk_size, save_path ,  model_name, start_rate, growth_epochs):\n",
    "    \n",
    "    \"\"\" Training function for the SPL curriculum based Neural Process model training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Lets define the training functions here and not in their own files, because I couldnt make them modular enough.\n",
    "    # (The posterior loss was relying on the global variable model, I tried creating a partial with the params not included to have the scan carry over a new param based partial to the step function but it wasnt working, this works for now)\n",
    "\n",
    "    def posterior_loss(\n",
    "        params: flax.typing.VariableDict,\n",
    "        batch,\n",
    "        key: flax.typing.PRNGKey,\n",
    "    ):\n",
    "        key_data, key_model = jax.random.split(key)\n",
    "        \n",
    "\n",
    "\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        x_test = batch[2]\n",
    "        y_test = batch[3]\n",
    "        # Compute ELBO over batch of datasets\n",
    "        elbos = jax.vmap(\n",
    "        partial(\n",
    "                model.apply,\n",
    "                params,  \n",
    "                beta=kl_penalty,\n",
    "                k=num_posterior_mc,\n",
    "                method=model.elbo\n",
    "        ) \n",
    "        )(\n",
    "            X, y, x_test, y_test, rngs={'default': jax.random.split(key_model, X.shape[0])}\n",
    "        )\n",
    "        \n",
    "        return -elbos.mean()\n",
    "\n",
    "    @jax.jit\n",
    "    def step(\n",
    "        theta: flax.typing.VariableDict, \n",
    "        opt_state: optax.OptState,\n",
    "        current_batch,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "    ) -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "        # Implements a generic SGD Step\n",
    "        \n",
    "        # value, grad = jax.value_and_grad(posterior_loss_filtered, argnums=0)(theta, random_key)\n",
    "        value, grad = jax.value_and_grad(posterior_loss, argnums=0)(theta, current_batch, random_key )\n",
    "        \n",
    "        updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "        \n",
    "        return theta, opt_state, value\n",
    "\n",
    "\n",
    "    def body_batch(carry, batch):\n",
    "        params, opt_state, key = carry\n",
    "        key_carry, key_step = jax.random.split(key)\n",
    "\n",
    "        X, x_test = jnp.split(batch[0], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        y, y_test = jnp.split(batch[1], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        params, opt_state, value = step(params, opt_state, (X,y, x_test,y_test ), key_step )\n",
    "\n",
    "        return (params, opt_state, key_carry ), value\n",
    "\n",
    "    jax.jit\n",
    "    def scan_train(params, opt_state, key,  batches):\n",
    "        \n",
    "        last, out = jax.lax.scan(body_batch, (params, opt_state, key ), batches)\n",
    "\n",
    "        params, opt_state, _ = last\n",
    "        \n",
    "        return params, opt_state, out\n",
    "\n",
    "    torch.manual_seed(dataloader_key_int) # Setting the seed for the dataloader\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    num_context_samples = 64\n",
    "    num_target_samples = 32\n",
    "    batch_size = 128\n",
    "    kl_penalty = 1e-4\n",
    "    num_posterior_mc = 1\n",
    "\n",
    "\n",
    "    # First lets create the dataset, \n",
    "    # Lets hardcode it for now, and then we can make it more flexible later on\n",
    "    \n",
    "    sampler_noise = partial(\n",
    "        joint, \n",
    "        WhiteNoise(f6, 0.1), \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    sampler_clean = partial(\n",
    "        joint, \n",
    "        f6, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    out_task_sampler = partial(\n",
    "        joint, \n",
    "        f5, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    samplers = [sampler_noise, sampler_clean]\n",
    "\n",
    "    dataset_key = jax.random.PRNGKey(dataset_key_int)\n",
    "    dataset = RegressionDataset(generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, chunk_size , dataset_key))\n",
    "\n",
    "    # Lets setup the SPL curriculum\n",
    "\n",
    "    rng , curricula_key = jax.random.split(dataset_key)\n",
    "    spl_curricula = SPL_curriculum(start_rate, growth_epochs , dataset, batch_size, curricula_key)\n",
    "\n",
    "\n",
    "\n",
    "    # Lets initalize the model we are going to train\n",
    "\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    model , params = create_model(key)\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "    in_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the in task errors for the model\n",
    "    out_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the out of task errors for the model\n",
    "    training_steps = 0\n",
    "\n",
    "    for i in (pbar := tqdm.trange(10 ,desc='Optimizing params. ')):\n",
    "        \n",
    "        rng, key = jax.random.split(rng)\n",
    "        _ , eval_epoch_key = jax.random.split(rng)\n",
    "        model_partial_loss_function = partial(model.apply, params, beta=kl_penalty, k=num_posterior_mc, method=model.elbo) \n",
    "        \n",
    "\n",
    "\n",
    "        batches = jnp.asarray( jax.tree_util.tree_map(lambda tensor : tensor.numpy(), [batch for batch in spl_curricula.data_curriculum(model_partial_loss_function, i, num_context_samples)]))\n",
    "        # params_new, opt_state, loss = step(params, opt_state, key)\n",
    "        \n",
    "        # Right now the number of batches might lead us to over train for the training_step_number, or overtrain for the eval_intervals\n",
    "\n",
    "        # First take care of the case where we might be overtraining for the eval_intervals. \n",
    "\n",
    "        batches_until_eval = eval_intervals - (training_steps % eval_intervals)\n",
    "        batches_until_end = training_step_number - training_steps\n",
    "        if batches_until_end < len(batches):\n",
    "            batches = batches[:batches_until_end]\n",
    "\n",
    "        # If the batches until eval is less than the batches until end, we eval, then train until batches_until_end - batches_until_eval\n",
    "        # If the batches until eval is more than the batches until end, we train batches_until_end\n",
    "        # if the batches until eval is == to batches_until_end, we eval and then let the training end.\n",
    "        # If the batches until eval is less than the batches until end , but the batches_until_end - (len(batches) - batches_until_eval) is less than 0, we train the rest of the batches and end the training.\n",
    "        # if the batches_until_end is negative or 0 we break the training loop.  \n",
    "        print(\"batches_until_eval\", batches_until_eval, \"batches_until_end\", batches_until_end, \"len(batches)\", len(batches), \"training_steps\", training_steps )\n",
    "\n",
    "        # Okay so if the eval period can fit inside multiple times into a len(batches) it should be run that many times. \n",
    "        # It can be done so if \n",
    "        # if the len(batches) / eval_intervals is larger than 2 . \n",
    "\n",
    "\n",
    "        if batches_until_eval < len(batches):\n",
    "            # then get the slice to make up the eval_intervals\n",
    "            \n",
    "            trained_steps_within_eval = 0\n",
    "             \n",
    "            batch_slice_pre_eval = eval_intervals - ( training_steps % eval_intervals )\n",
    "            batch_slice = batch_slice_pre_eval \n",
    "            loss_array_eval = []\n",
    "            params_new = params\n",
    "            for i in range(0,1+((len(batches)-batch_slice_pre_eval) // eval_intervals)):\n",
    "                \n",
    "\n",
    "                print(\"current eval loop number\", i , \"currently trained steps within eval\", trained_steps_within_eval , \"current batch slice\", (trained_steps_within_eval, (trained_steps_within_eval+batch_slice)) ) \n",
    "                params_new, opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:(trained_steps_within_eval+batch_slice)])\n",
    "\n",
    "                loss_array_eval.extend(loss_arr)  # dont lose the loss values upon next batch training\n",
    "                trained_steps_within_eval += batch_slice\n",
    "                batch_slice = eval_intervals\n",
    "\n",
    "                eval_epoch_key, eval_inkey_data, eval_outkey_data, eval_model_key = jax.random.split(eval_epoch_key, 4)\n",
    "                intask_x_eval, intask_y_eval = jax.vmap(sampler_clean)(jax.random.split(eval_inkey_data, eval_dataset_size)) \n",
    "                intask_x_eval, intask_y_eval = intask_x_eval[..., None], intask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(intask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(intask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                in_task_errors['ece'].append(ece_errors.mean())\n",
    "                in_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                in_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "                # Now lets do the out of task evaluation (f for now like the original notebook)\n",
    "                outtask_x_eval, outtask_y_eval = jax.vmap(out_task_sampler)(jax.random.split(eval_outkey_data, eval_dataset_size))\n",
    "                outtask_x_eval, outtask_y_eval = outtask_x_eval[..., None], outtask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(outtask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(outtask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                out_task_errors['ece'].append(ece_errors.mean())\n",
    "                out_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                out_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            # Now we can train the rest of the batches\n",
    "            \n",
    "            # with trained_steps_within_eval start slicing, only train if len(batches) - trained_steps_within_eval > 0\n",
    "            if len(batches) - trained_steps_within_eval > 0: \n",
    "                print(\"training on the rest of the remaining batches after eval\", len(batches)-trained_steps_within_eval)\n",
    "                params_new , opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:])\n",
    "                loss_array_eval.extend(loss_arr)\n",
    "            else:\n",
    "                print(\"Eval period was the last period, no more training, eval intervals fit perfectly within this batch.\")\n",
    "\n",
    "            \n",
    "            loss_arr = jnp.asarray(loss_array_eval)\n",
    "        else: \n",
    "            params_new, opt_state, loss_arr = scan_train(params, opt_state, key,batches)\n",
    "        \n",
    "        # Update the training steps\n",
    "        # Since this variable is only used inside the function and never later , it doesnt matter for the training_step_number restriction if it overcounts.  \n",
    "        # Although it would so pay attention if implementation changes.\n",
    "        print(training_steps, len(batches))\n",
    "        training_steps+= len(batches)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        losses.extend(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "        \n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "        \n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "\n",
    "        if(training_steps >= training_step_number):\n",
    "            break\n",
    "    # Lets save what we need to save for the model and training. \n",
    "\n",
    "    ### After training we should save  \n",
    "        # the model parameters with a name that we know how it was trained\n",
    "        # the losses and other relevant information accrued during training\n",
    "        # the curriculum weight log for the dataset\n",
    "\n",
    "    # Saving the model params\n",
    "    # We could also save opt_state here for later training\n",
    "    # Also after trying out whether the training would continue saving and loading the params back in I saw change in printed loss. Not sure why that is the case,\n",
    "    # Even if I restore the opt_state as well. Regardless, the model continued training so saving the params is enough to use the model for evaluation later on. \n",
    "    \n",
    "    print(\"printing losses length\", len(losses))\n",
    "    save_model_params(best_params,save_path, model_name) \n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_curricula_logs.pkl'), 'wb') as f:\n",
    "        if(len(spl_curricula.weight_log)>0):\n",
    "            pickle.dump({\"curricula_weights\": spl_curricula.weight_log , \"curricula_losses\": spl_curricula.epoch_losses_log}, f)\n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_training_metrics.pkl'), 'wb') as f:\n",
    "        pickle.dump({\"training_loss\" : losses, \"training_intask_errors\": in_task_errors, \"training_outtask_errors\": out_task_errors }, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lets also define a function for the baseline training, so that we can compare the two models later on.\n",
    "\n",
    "def train_np_baseline(dataset_key_int,dataloader_key_int, dataset_size, training_step_number, eval_intervals, eval_dataset_size, sampler_ratios, chunk_size, save_path ,  model_name):\n",
    "\n",
    "    \"\"\" Training function for the SPL curriculum based Neural Process model training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Lets define the training functions here and not in their own files, because I couldnt make them modular enough.\n",
    "    # (The posterior loss was relying on the global variable model, I tried creating a partial with the params not included to have the scan carry over a new param based partial to the step function but it wasnt working, this works for now)\n",
    "\n",
    "    def posterior_loss(\n",
    "        params: flax.typing.VariableDict,\n",
    "        batch,\n",
    "        key: flax.typing.PRNGKey,\n",
    "    ):\n",
    "        key_data, key_model = jax.random.split(key)\n",
    "\n",
    "\n",
    "\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        x_test = batch[2]\n",
    "        y_test = batch[3]\n",
    "        # Compute ELBO over batch of datasets\n",
    "        elbos = jax.vmap(\n",
    "        partial(\n",
    "                model.apply,\n",
    "                params,\n",
    "                beta=kl_penalty,\n",
    "                k=num_posterior_mc,\n",
    "                method=model.elbo\n",
    "        )\n",
    "        )(\n",
    "            X, y, x_test, y_test, rngs={'default': jax.random.split(key_model, X.shape[0])}\n",
    "        )\n",
    "\n",
    "        return -elbos.mean()\n",
    "\n",
    "    @jax.jit\n",
    "    def step(\n",
    "        theta: flax.typing.VariableDict,\n",
    "        opt_state: optax.OptState,\n",
    "        current_batch,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "    ) -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "        # Implements a generic SGD Step\n",
    "\n",
    "        # value, grad = jax.value_and_grad(posterior_loss_filtered, argnums=0)(theta, random_key)\n",
    "        value, grad = jax.value_and_grad(posterior_loss, argnums=0)(theta, current_batch, random_key )\n",
    "\n",
    "        updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "\n",
    "        return theta, opt_state, value\n",
    "\n",
    "\n",
    "    def body_batch(carry, batch):\n",
    "        params, opt_state, key = carry\n",
    "        key_carry, key_step = jax.random.split(key)\n",
    "\n",
    "        X, x_test = jnp.split(batch[0], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        y, y_test = jnp.split(batch[1], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        params, opt_state, value = step(params, opt_state, (X,y, x_test,y_test ), key_step )\n",
    "\n",
    "        return (params, opt_state, key_carry ), value\n",
    "\n",
    "    jax.jit\n",
    "    def scan_train(params, opt_state, key,  batches):\n",
    "\n",
    "        last, out = jax.lax.scan(body_batch, (params, opt_state, key ), batches)\n",
    "\n",
    "        params, opt_state, _ = last\n",
    "\n",
    "        return params, opt_state, out\n",
    "\n",
    "\n",
    "    torch.manual_seed(dataloader_key_int) # Setting the seed for the dataloader\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    num_context_samples = 64\n",
    "    num_target_samples = 32\n",
    "    batch_size = 128\n",
    "    kl_penalty = 1e-4\n",
    "    num_posterior_mc = 1\n",
    "\n",
    "\n",
    "    # First lets create the dataset,\n",
    "    # Lets hardcode it for now, and then we can make it more flexible later on\n",
    "\n",
    "    sampler_noise = partial(\n",
    "        joint,\n",
    "        WhiteNoise(f6, 0.1),\n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    sampler_clean = partial(\n",
    "        joint,\n",
    "        f6,\n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    out_task_sampler = partial(\n",
    "        joint, \n",
    "        f5, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    samplers = [sampler_noise, sampler_clean]\n",
    "\n",
    "    dataset_key = jax.random.PRNGKey(dataset_key_int)\n",
    "    dataset = RegressionDataset(generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, chunk_size , dataset_key))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Lets setup the SPL curriculum\n",
    "\n",
    "    rng , curricula_key = jax.random.split(dataset_key)\n",
    "\n",
    "\n",
    "    # Lets initalize the model we are going to train\n",
    "\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    model , params = create_model(key)\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "    in_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the in task errors for the model\n",
    "    out_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the out of task errors for the model\n",
    "    training_steps = 0\n",
    "\n",
    "    eval_intervals = 100\n",
    "    for i in (pbar := tqdm.trange(10 ,desc='Optimizing params. ')):\n",
    "\n",
    "        rng, key = jax.random.split(rng)\n",
    "        _ , eval_epoch_key = jax.random.split(rng)\n",
    "        \n",
    "\n",
    "        batches = jnp.asarray( jax.tree_util.tree_map(lambda tensor : tensor.numpy(), [batch for batch in dataloader]))\n",
    "        # params_new, opt_state, loss = step(params, opt_state, key)\n",
    "\n",
    "        batches_until_eval = eval_intervals - (training_steps % eval_intervals)\n",
    "        batches_until_end = training_step_number - training_steps\n",
    "        if batches_until_end < len(batches):\n",
    "            batches = batches[:batches_until_end]\n",
    "\n",
    "        # If the batches until eval is less than the batches until end, we eval, then train until batches_until_end - batches_until_eval\n",
    "        # If the batches until eval is more than the batches until end, we train batches_until_end\n",
    "        # if the batches until eval is == to batches_until_end, we eval and then let the training end.\n",
    "        # If the batches until eval is less than the batches until end , but the batches_until_end - (len(batches) - batches_until_eval) is less than 0, we train the rest of the batches and end the training.\n",
    "        # if the batches_until_end is negative or 0 we break the training loop.  \n",
    "        print(\"batches_until_eval\", batches_until_eval, \"batches_until_end\", batches_until_end, \"len(batches)\", len(batches), \"training_steps\", training_steps )\n",
    "\n",
    "        # Okay so if the eval period can fit inside multiple times into a len(batches) it should be run that many times. \n",
    "        # It can be done so if \n",
    "        # if the len(batches) / eval_intervals is larger than 2 . \n",
    "\n",
    "\n",
    "        if batches_until_eval < len(batches):\n",
    "            # then get the slice to make up the eval_intervals\n",
    "            \n",
    "            trained_steps_within_eval = 0\n",
    "             \n",
    "            batch_slice_pre_eval = eval_intervals - ( training_steps % eval_intervals )\n",
    "            batch_slice = batch_slice_pre_eval \n",
    "            loss_array_eval = []\n",
    "            params_new = params\n",
    "            for i in range(0,1+((len(batches)-batch_slice_pre_eval) // eval_intervals)):\n",
    "                \n",
    "\n",
    "                print(\"current eval loop number\", i , \"currently trained steps within eval\", trained_steps_within_eval , \"current batch slice\", (trained_steps_within_eval, (trained_steps_within_eval+batch_slice)) ) \n",
    "                params_new, opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:(trained_steps_within_eval+batch_slice)])\n",
    "\n",
    "                loss_array_eval.extend(loss_arr)  # dont lose the loss values upon next batch training\n",
    "                trained_steps_within_eval += batch_slice\n",
    "                batch_slice = eval_intervals\n",
    "\n",
    "                eval_epoch_key, eval_inkey_data, eval_outkey_data, eval_model_key = jax.random.split(eval_epoch_key, 4)\n",
    "                intask_x_eval, intask_y_eval = jax.vmap(sampler_clean)(jax.random.split(eval_inkey_data, eval_dataset_size)) \n",
    "                intask_x_eval, intask_y_eval = intask_x_eval[..., None], intask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(intask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(intask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                in_task_errors['ece'].append(ece_errors.mean())\n",
    "                in_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                in_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "                # Now lets do the out of task evaluation (f for now like the original notebook)\n",
    "                outtask_x_eval, outtask_y_eval = jax.vmap(out_task_sampler)(jax.random.split(eval_outkey_data, eval_dataset_size))\n",
    "                outtask_x_eval, outtask_y_eval = outtask_x_eval[..., None], outtask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(outtask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(outtask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                out_task_errors['ece'].append(ece_errors.mean())\n",
    "                out_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                out_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            # Now we can train the rest of the batches\n",
    "            \n",
    "            # with trained_steps_within_eval start slicing, only train if len(batches) - trained_steps_within_eval > 0\n",
    "            if len(batches) - trained_steps_within_eval > 0: \n",
    "                print(\"training on the rest of the remaining batches after eval\", len(batches)-trained_steps_within_eval)\n",
    "                params_new , opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:])\n",
    "                loss_array_eval.extend(loss_arr)\n",
    "            else:\n",
    "                print(\"Eval period was the last period, no more training, eval intervals fit perfectly within this batch.\")\n",
    "\n",
    "            \n",
    "            loss_arr = jnp.asarray(loss_array_eval)\n",
    "        else: \n",
    "            params_new, opt_state, loss_arr = scan_train(params, opt_state, key,batches)\n",
    "        \n",
    "        # Update the training steps\n",
    "        # Since this variable is only used inside the function and never later , it doesnt matter for the training_step_number restriction if it overcounts.  \n",
    "        # Although it would so pay attention if implementation changes.\n",
    "        print(training_steps, len(batches))\n",
    "        training_steps+= len(batches)\n",
    "        losses.extend(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "\n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "\n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "\n",
    "    # Lets save what we need to save for the model and training.\n",
    "\n",
    "    ### After training we should save\n",
    "        # the model parameters with a name that we know how it was trained\n",
    "        # the losses and other relevant information accrued during training\n",
    "        # the curriculum weight log for the dataset\n",
    "\n",
    "    # Saving the model params\n",
    "    # We could also save opt_state here for later training\n",
    "    # Also after trying out whether the training would continue saving and loading the params back in I saw change in printed loss. Not sure why that is the case,\n",
    "    # Even if I restore the opt_state as well. Regardless, the model continued training so saving the params is enough to use the model for evaluation later on.\n",
    "\n",
    "    save_model_params(best_params,save_path, model_name)\n",
    "\n",
    "    with open(os.path.join(save_path, model_name + '_training_metrics.pkl'), 'wb') as f:\n",
    "        pickle.dump({\"training_loss\" : losses, \"training_intask_errors\": in_task_errors, \"training_outtask_errors\": out_task_errors }, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (1280,)\n",
      "batches_until_eval 128 batches_until_end 300 len(batches) 10 training_steps 0\n",
      "0 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1088:  10%|█         | 1/10 [00:10<01:38, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 0 curr_data_rate 0 epoch number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1088:  10%|█         | 1/10 [00:15<02:16, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted indices shape (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_spl_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./spl_training_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspl_model_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 167\u001b[0m, in \u001b[0;36mtrain_spl_curriculum\u001b[0;34m(dataset_key_int, dataloader_key_int, dataset_size, training_step_number, eval_dataset_size, eval_intervals, sampler_ratios, chunk_size, save_path, model_name, start_rate, growth_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m _ , eval_epoch_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    163\u001b[0m model_partial_loss_function \u001b[38;5;241m=\u001b[39m partial(model\u001b[38;5;241m.\u001b[39mapply, params, beta\u001b[38;5;241m=\u001b[39mkl_penalty, k\u001b[38;5;241m=\u001b[39mnum_posterior_mc, method\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39melbo) \n\u001b[0;32m--> 167\u001b[0m batches \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray( jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m tensor : tensor\u001b[38;5;241m.\u001b[39mnumpy(), [batch \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspl_curricula\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_partial_loss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_context_samples\u001b[49m\u001b[43m)\u001b[49m]))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# params_new, opt_state, loss = step(params, opt_state, key)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Right now the number of batches might lead us to over train for the training_step_number, or overtrain for the eval_intervals\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# First take care of the case where we might be overtraining for the eval_intervals. \u001b[39;00m\n\u001b[1;32m    174\u001b[0m batches_until_eval \u001b[38;5;241m=\u001b[39m eval_intervals \u001b[38;5;241m-\u001b[39m (training_steps \u001b[38;5;241m%\u001b[39m eval_intervals)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/aa_train_utils/spl_curriculum.py:58\u001b[0m, in \u001b[0;36mSPL_curriculum.data_curriculum\u001b[0;34m(self, loss_partial, epoch, num_context_samples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_log\u001b[38;5;241m.\u001b[39mappend(sorted_indices)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_losses_log\u001b[38;5;241m.\u001b[39mappend(losses)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSubset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "\n",
    "train_spl_curriculum(0,0, 128*100, 300, 100, 128, [0.3,0.7], 128, \"./spl_training_data/\", \"spl_model_0\", 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 7.1476:  10%|█         | 1/10 [00:04<00:44,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3355:  20%|██        | 2/10 [00:09<00:39,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.0826:  30%|███       | 3/10 [00:14<00:34,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.9729:  40%|████      | 4/10 [00:20<00:30,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7804:  50%|█████     | 5/10 [00:25<00:26,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6939:  60%|██████    | 6/10 [00:31<00:21,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5483:  70%|███████   | 7/10 [00:35<00:15,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.4905:  80%|████████  | 8/10 [00:36<00:07,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5315:  90%|█████████ | 9/10 [00:37<00:02,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.3186: 100%|██████████| 10/10 [00:38<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(7.1476481, dtype=float64), Array(2.33553561, dtype=float64), Array(2.35164553, dtype=float64), Array(2.28304682, dtype=float64), Array(2.18469931, dtype=float64), Array(2.08259002, dtype=float64), Array(2.09686869, dtype=float64), Array(2.0965406, dtype=float64), Array(2.06030111, dtype=float64), Array(2.03384646, dtype=float64), Array(1.98928973, dtype=float64), Array(2.01009892, dtype=float64), Array(1.97294493, dtype=float64), Array(1.93222295, dtype=float64), Array(1.95511106, dtype=float64), Array(1.92082819, dtype=float64), Array(1.87994243, dtype=float64), Array(1.97160641, dtype=float64), Array(1.89423986, dtype=float64), Array(1.88922796, dtype=float64), Array(1.78038898, dtype=float64), Array(1.76223514, dtype=float64), Array(1.75034777, dtype=float64), Array(1.7406877, dtype=float64), Array(1.76540039, dtype=float64), Array(1.82032126, dtype=float64), Array(1.72370332, dtype=float64), Array(1.80020471, dtype=float64), Array(1.69390789, dtype=float64), Array(1.72623411, dtype=float64), Array(1.75179461, dtype=float64), Array(1.6200606, dtype=float64), Array(1.68566509, dtype=float64), Array(1.6842936, dtype=float64), Array(1.77545442, dtype=float64), Array(1.64176648, dtype=float64), Array(1.70559326, dtype=float64), Array(1.54833016, dtype=float64), Array(1.73563969, dtype=float64), Array(1.67229073, dtype=float64), Array(1.67198467, dtype=float64), Array(1.59097982, dtype=float64), Array(1.74595717, dtype=float64), Array(1.61302486, dtype=float64), Array(1.70031881, dtype=float64), Array(1.59751633, dtype=float64), Array(1.61919879, dtype=float64), Array(1.49048661, dtype=float64), Array(1.50591696, dtype=float64), Array(1.75732077, dtype=float64), Array(1.65964276, dtype=float64), Array(1.70124198, dtype=float64), Array(1.60438169, dtype=float64), Array(1.55197745, dtype=float64), Array(1.67485363, dtype=float64), Array(1.60775849, dtype=float64), Array(1.56743329, dtype=float64), Array(1.53147323, dtype=float64), Array(1.57408963, dtype=float64), Array(1.55923479, dtype=float64), Array(1.46480728, dtype=float64), Array(1.5656397, dtype=float64), Array(1.63061821, dtype=float64), Array(1.64993646, dtype=float64), Array(1.53245828, dtype=float64), Array(1.55442011, dtype=float64), Array(1.60937642, dtype=float64), Array(1.48813517, dtype=float64), Array(1.31863036, dtype=float64), Array(1.48985407, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 281, 1140,  572, 1169,  134,  137,  972,  305,  149,  184,  425,\n",
      "        728,  772,  584,  671,  309,  685,  558,  326,  770,  418,  940,\n",
      "        252,  820,  850, 1099,  712,  751,  557,  873,  428,   72, 1231,\n",
      "        681,  789,  795,    2, 1233,  124, 1203, 1178,  930,  877,  509,\n",
      "        511,  706,   11,  441,  406,  321,  357,   92,  911,   34,  328,\n",
      "         46,  945,   43,  383,  294,  340,  876,  668,  455,  198, 1006,\n",
      "        941,  868,  931, 1138,   28,  551,  371, 1171,  969,  583,  238,\n",
      "         44,   50,  280,  875,  986, 1227,  248,  366,  206, 1166,  358,\n",
      "        575,  141,  957,  607,  150,   63,  836,   36,  522,  392, 1068,\n",
      "        115,  701, 1062,  250,  355, 1243, 1198,  510,  102, 1260, 1238,\n",
      "       1043, 1228,  320,  414,  791, 1070,  104, 1086,   30, 1029, 1269,\n",
      "        423,  741,  415,  816, 1270,    9,  528], dtype=int64), Array([ 371,  104,  986,  248, 1233,   72,    2,  320,  733,  258,  957,\n",
      "        399,  706,  983,  252, 1108, 1171,  534, 1088, 1181,  285, 1265,\n",
      "         11,  149,  508,  366,  607, 1074,  363,  786,  875,  461,  491,\n",
      "        716,  857,  394,  839,  861, 1043, 1260, 1121,  187, 1029,   56,\n",
      "        751, 1051,  584,  633, 1250,  338,  551,  671,  583,    6,  973,\n",
      "        842,   23,  455,  689,  876,  572,  186,  791, 1151,  417,  580,\n",
      "       1198,  837, 1023,  546,   46,  574, 1195,  549,  354,   65,  198,\n",
      "        763,  476,  206,  136,  952,  835,  283,   79, 1071,  260,  418,\n",
      "       1228,  930,  150,   49,  111,  962,  184, 1223,  383,  923,  714,\n",
      "        355,    8, 1077, 1072,  402,  974,  741,  859, 1018,  557,  511,\n",
      "        423,  329,  464,  785,  984, 1000, 1206,  918, 1053,  703,   91,\n",
      "         48, 1187,  199, 1061,  877,  359,  302, 1022,  564,  870,  441,\n",
      "        344,  369,  501,  284, 1021,  495,  594, 1263, 1210, 1234, 1057,\n",
      "       1203,  217,  840,  236, 1002,  116, 1014,  220,  240,  947,  137,\n",
      "        907,  954,  655,  514, 1045,  309,  178,  712,  977,  575,  105,\n",
      "        686, 1052,  195,  664, 1227,  373, 1066,  940, 1128, 1274,  675,\n",
      "        946,  851,  849,  860,  305,   89,  286,  425,  261,  130, 1271,\n",
      "        668,   36, 1067, 1040,   84,  908,  532,  386,  159,  710,  442,\n",
      "       1004, 1229, 1277,  231,  836,  301,  937, 1168,  224, 1099,  330,\n",
      "        368,  737,  880,  882,  398,  874,  440,  624,  428,  707,  804,\n",
      "        685, 1068,  850,  673,  126,  128, 1231,  602,  124,  787, 1257,\n",
      "       1202,  357, 1169, 1118, 1226,  163,  719, 1097,  663,   37, 1221,\n",
      "        902,  579, 1177,   25,  894, 1028,  478,  935, 1178,   28,  872,\n",
      "        695,  265,  789,  795,  100,  729,  776,  331,  404,  204, 1247,\n",
      "        281,  911,  467,  144,  739,  629,  933,   19,   68,  180,  683,\n",
      "        976,  323,  994,  235,  219,  274,  753,  537,    9,  506,  280,\n",
      "        722,  895,  509,  388,  775,  743,  941,  674,   47,   24,  680,\n",
      "       1064, 1246,  705,  121,  293,  635,  718,  975,  277,   55,  972,\n",
      "       1024,  970,  713,  161, 1182,   43,  697,  306,  479, 1138,  603,\n",
      "       1154, 1173,  405, 1140,   57, 1191,  797, 1069,  666, 1208,  250,\n",
      "        647,  196,  649, 1063,  101,  545,  420,  326, 1180,  400,  834,\n",
      "        135,  980,  819,  244,  312,  630,  803,  345, 1073, 1086,  110,\n",
      "       1003, 1163, 1127, 1212, 1184,  573], dtype=int64), Array([ 607,  716,  248,  186,  741,  116,  557,  511, 1053,  302,  918,\n",
      "        923,  574, 1210, 1067,    8,  149,  952,  383,   65, 1198, 1115,\n",
      "         49,  703,  508, 1250, 1002,  973,  363,  369,  655,  714,  178,\n",
      "       1271,  399,  105,  859,  417, 1187,  689, 1263,   72, 1086,  274,\n",
      "        791, 1040, 1121,   56,  633,  198,  546, 1211, 1018,  786,  712,\n",
      "         25, 1072,  670,  882,  440,  584,  849, 1029,  371,  338,  908,\n",
      "        737, 1260,  231,  283, 1000,   79, 1173,  743,  418,  763,  861,\n",
      "        388, 1068,  842, 1233,  104, 1206,  334,  284,  308,  394,  671,\n",
      "        329, 1043, 1223,  940,  722, 1228,  206, 1052, 1195, 1096, 1108,\n",
      "         23, 1051,  491,  305, 1221,  204,  479,  344,  136, 1097, 1057,\n",
      "       1021,  876,  240,  285, 1088,  795, 1061,  187,  346, 1099, 1231,\n",
      "        729,  603,  296,  685,  751, 1063,  706,  130,  357,   33,  935,\n",
      "        957,  753,  461,  626, 1171,  349, 1184,  355,   11,  978, 1234,\n",
      "         31,  840,   91,  583,  402, 1238,  509, 1227,  837, 1178,  733,\n",
      "        426,  260,  301,  905,  551,  144,  441,  188, 1022,  549,  710,\n",
      "        159,  384,  870,   57,  947,  552,  531,  880,  664, 1074,  515,\n",
      "        883,  851,  219,  834, 1257, 1203,  564,  309,  629,  594,  252,\n",
      "       1266,  313,  630,  972,  971,  580,   74,  241,  673,  423,  856,\n",
      "        455,  470,  983,   37,  902, 1158,  663, 1216,  442,   24,  611,\n",
      "        323, 1160,  835, 1189,   48,  954,  877,   89,   58,  896,  265,\n",
      "        425,  135,  537, 1064,  967,  674,  941,  875,   41,  617,  974,\n",
      "         75,  545,  184, 1176,  739,  679,  984, 1183,  141,  428,  137,\n",
      "        124, 1192,  197,  374,  857, 1138,  192,  911,  320, 1151,  111,\n",
      "        644,  100,  638,  767,  150,  199,  600,  731, 1180, 1001, 1217,\n",
      "         98,  366,  133,  506, 1062,  801,  277,  362,  930,    7,  912,\n",
      "        825, 1168, 1014,  565,   10,  245,  213,  413,  960,  294,  497,\n",
      "       1066,  889, 1128,  450,  375,  326, 1079, 1190,  534,  180,  828,\n",
      "        575, 1264,  533, 1076,  719, 1166,  328,  585,  286,  796,  937,\n",
      "        699,  250, 1261,  878,  787,  799,  258,  788, 1265,  500,   36,\n",
      "       1279,  210,  963,  121,  140, 1100,  412,  339,  695,   34, 1222,\n",
      "       1077,  587,  263,  403,  330, 1194,  933, 1019,  749, 1028,  156,\n",
      "         96,  293,  151, 1254,  525,  747,  359,  214, 1070,  635,  806,\n",
      "        862,  303,  271,  126,  661,  492,  734,  992,  406, 1025,  914,\n",
      "       1032,  884, 1050, 1071,   86,  619, 1272,  161,   66, 1146,  745,\n",
      "       1181,  808,  602,   52,  514, 1202,  651,  289,  965,  839,  848,\n",
      "       1220,  987,  648,  899, 1046,  292,  464,  532,  304, 1130, 1075,\n",
      "        377,    0,  217,  639, 1274, 1049,   43,  732,  142,   51,  207,\n",
      "        327,   62,  538,  608,  173, 1208, 1150,  354,  478,  174, 1207,\n",
      "       1004,  704, 1123, 1118,  169,  113, 1179,  891,  776,  894,  986,\n",
      "        230, 1201,  676,  872,  833,  475,  779,  820,  427, 1030, 1101,\n",
      "        802,  681,  713,   97,  555, 1012,   94, 1085,  314,  264,  904,\n",
      "        312,  641, 1182,  690,  132, 1163, 1137,  386,  297,   71,  886,\n",
      "         61,  183,  755,  809,    3,  228,  798,  850,  962,  613,   22,\n",
      "        804, 1126, 1136,  408,  873, 1015,  975,  697,  147, 1059, 1156,\n",
      "       1142,  797, 1154,   95,  395,  773,  945,  421, 1095,  401,  816,\n",
      "        202, 1199, 1252,  195,  895,  936,  887, 1268, 1169, 1277,  841,\n",
      "        419, 1232,  830,  298,   84, 1193, 1278,  968,  907,  281,  969,\n",
      "        430,  495,  762,  588, 1196,  754,  416,  792,   13,  507,  496,\n",
      "       1117,  483,  391, 1246,  643,   77,  209,  335, 1157, 1215,  606,\n",
      "       1073,  853, 1235, 1035,  387,  503,  453,   90,  123, 1212,   26,\n",
      "       1007, 1253,  698,  393,  683,  780,   39,  114,  805,  193, 1058,\n",
      "        364,  586,  270, 1170,  486,  827,  288,  759,  476,  628,  829,\n",
      "       1045,  148,  976, 1209, 1197,  247, 1275,  373,  215,  266,  916,\n",
      "       1240,  980,  997, 1033, 1205], dtype=int64), Array([1202,  791,  252,   49,  441,   72, 1187,  371,    2,  402,  857,\n",
      "        305,   79,  363,  186,  455,  137,  464,  338,  786,  206,  366,\n",
      "       1000,  689, 1223, 1088, 1051,  575,    8,  633,  258,  546,  837,\n",
      "        557,  706,  184,  248, 1171,  859,  584,  751,  954,  461,  714,\n",
      "       1043, 1228, 1029, 1234,   56,  344,  136, 1018, 1233,  369, 1071,\n",
      "        467,  716,  564,  882, 1121,  532, 1260,  947,  986,   11,  204,\n",
      "        417,  199, 1198, 1206,  973,  508,  983,  185,  877,  309,  104,\n",
      "         65,   48,  285,  386,  198,  149,  583,  952,  354,  580,  655,\n",
      "        607,  302,  930,  870, 1072,  231, 1108,  795,  671,  418,  861,\n",
      "       1074, 1250,  394,  511,  110,  383,  284,  840,   46,  399,  918,\n",
      "       1178,  935, 1023,  664,  697,  478,  111,   23,  329,  851, 1169,\n",
      "        283,  880,  763,  355,  663,  495,  703, 1195,  876,  479,  974,\n",
      "        122,  260,   84,  875,  277,  440, 1210,  105,  957,  839,  413,\n",
      "        941,  685, 1061,   89, 1066, 1138,  911, 1265,  835,   36,  312,\n",
      "        984,  603,   91, 1257,  187,  574,  602,  842, 1067,  978, 1021,\n",
      "        578,  719,  357, 1203,  733,  908,  549, 1115,   25,  552, 1271,\n",
      "        178,  240,  134,  124,  729,  722,  753,  301,  128,  359,  116,\n",
      "        787,  739, 1022,  281, 1196, 1014,  224,  428,  710, 1057,  195,\n",
      "       1277, 1180,  509,  972,  940,   24,  326,  133,  159, 1099, 1128,\n",
      "        594,  491,  713, 1052, 1097,  537, 1235, 1025, 1053, 1028,  587,\n",
      "        614,  130,  962,  849, 1163,  365, 1050,  686,  712,  764,  551,\n",
      "        442,  320,   53, 1073,  741,  423,  197,  845, 1002, 1177, 1040,\n",
      "        630,  514,  899, 1068, 1238,  150,  923,  674,  737,  553, 1154,\n",
      "        458,  624,  217,  425,  834, 1212, 1274,   28,   86,  820, 1231,\n",
      "        946,  745,  476,  668,  373,  860,  219, 1077,  743,  683, 1173,\n",
      "       1263,  349,  779,  736,  323, 1227,  558,  265, 1004,  100, 1175,\n",
      "        173,  789,  905,  179,  647, 1181,  747,   57,  345, 1151,   37,\n",
      "        902,  501,  772,  612,   92,  673,  629,  313,  291,  975,  776,\n",
      "        196,  388,  933,  640,  681,  306,  328,  707, 1140, 1134,  327,\n",
      "       1208,  296,  688,  126,  749,  850, 1082,   47,  181,   77,  380,\n",
      "       1063,  987,  250, 1246, 1211,  311,  384, 1030,  700, 1086,   30,\n",
      "        994, 1199,  421,  499,  161,  135, 1254,  907,    6, 1229,  316,\n",
      "        280,  545,  660,  498,  489, 1011,  275,   34,  483,  535,  752,\n",
      "        572, 1226, 1272,  246,  466,  634,  237, 1279,    0,  836,  232,\n",
      "        294,  855, 1098, 1221,  141,  274,  404,  506,  585,  611,  748,\n",
      "        379,  230,  806,  695,  408,  635,  477,   52,  802,  995, 1033,\n",
      "        170,  672,  406,  270,  809,  271,  503,  894,  628, 1214,  656,\n",
      "       1149,  610,  808, 1215,  534,  372,  374,  214, 1113,  690,  937,\n",
      "        788, 1062,   66,  348, 1166,  830,  831,  797,  331, 1130,  210,\n",
      "        912,  261,  398,  682,  262,   94, 1137,  342, 1158,  308,  203,\n",
      "        945,  213,   63,  120,  426,  450,  106,  785, 1003,  286,  163,\n",
      "       1100,  445, 1159,  675,  427,   96,  330,   75,  669,  236, 1222,\n",
      "        732,  742,  701,  515,  678,  799,  631,  727,  115,  419, 1266,\n",
      "        816,  891,  896, 1218, 1275,  579, 1207,  756,  694,  140,  336,\n",
      "        677,    9,   16,  475,  895, 1008, 1168,   74,  971, 1024,  965,\n",
      "        724,  616,  847,  448, 1019,  900,  435,  778,   43,  268, 1192,\n",
      "        144,  734,  436, 1101,  735,  804, 1217,  626,  220,  387,  885,\n",
      "        961, 1240, 1065, 1118, 1190,  873,  676,  525, 1046, 1042,  223,\n",
      "        530,  539,  523,  188,  174,   95,  518,  922,  256, 1197,  889,\n",
      "        192,  272,  463,  169,  758,  550, 1232,  801,  481, 1035, 1095,\n",
      "        556, 1157,  803, 1132,  447,  340, 1045,  754,  416,  862,  917,\n",
      "        726,  335, 1079,  818,  924,  798,  290, 1143,  244,  949,  193,\n",
      "        303,  661,  318,  129,  932,  177,  639,  864,   55, 1216,  717,\n",
      "        670,  456, 1261,   39,  276, 1081, 1012,   80,  339,  102,  593,\n",
      "        773,  625,  951,  667,  609,  868,   18,  176, 1005,  936,  746,\n",
      "       1027,  469,  617, 1236,  300,  160,  565,  967,  777,   99,  993,\n",
      "        871,  194,  968,  465,  606,  235, 1155,  381,  540, 1239,  521,\n",
      "        618,  942,  598,  702,   67, 1174,  295,  770,  832,   69,  487,\n",
      "       1070,   90,  953,   76,   20,  430,  201, 1006, 1064,   73,  619,\n",
      "        409,  577, 1010,   33, 1039,  107,  229,    5,  520, 1114, 1044,\n",
      "       1103, 1273,   50, 1244,  805,  337,  325,  382,  969,  146,  517,\n",
      "        766,   51,  958,  377,   54,  691,  666,  771,  502,    4,  604,\n",
      "       1224,  212,  662,  699,  680,  708, 1047,  887,  650, 1153,   41,\n",
      "       1076,  636,  744,  825,  249,  542,  155,  297, 1084,  812,  955,\n",
      "         13, 1093,   61,   31,    1,  925,   19,  529,  108,  315,    7,\n",
      "        914,   83,  589,  504,  460, 1204,  621,  172,  627, 1205,  643,\n",
      "        921, 1055,  869,  238, 1165,  705,  360, 1249,  704,  814,    3,\n",
      "        601,  903,  234,  926,  257,  127,  872,  913,  883, 1170, 1139,\n",
      "        109,  322, 1083,  783,  171, 1102,  642,  715, 1142,  856,  118,\n",
      "        720,  142,  595, 1016,  620,  443,  990,  874,  513,  659,  852,\n",
      "        757,   78, 1220,  470,  638,  590,  343,  225,  944,  527,  482,\n",
      "        519, 1090,  282, 1243,  368,  826,  396,  226,   22,  321,  405,\n",
      "        920,  121, 1182, 1156,  310,   85, 1146, 1270,  910,  298, 1241,\n",
      "        901,   17,  811,   98,  496, 1150, 1269,  897,  264,  216,   38,\n",
      "        762,  927,  139,  571,   27], dtype=int64), Array([ 918,  371,  607, ..., 1251,  784,   14], dtype=int64), Array([861, 791, 918, ..., 970, 866, 245], dtype=int64)], 'curricula_losses': [Array([ -5.38987106,  -2.94617798, -18.10935558, ...,  -2.25079339,\n",
      "        -1.69209681,  -2.34305441], dtype=float64), Array([-2.06629521, -2.10011473, -2.9687758 , ..., -2.31396661,\n",
      "       -1.73724974, -2.15671581], dtype=float64), Array([-2.26129482, -1.95812545, -2.19121241, ..., -2.22145158,\n",
      "       -2.21779706, -2.29954356], dtype=float64), Array([-2.03047332, -1.71054489, -3.43493375, ..., -2.35399024,\n",
      "       -1.64849493, -2.03472525], dtype=float64), Array([-1.98982298, -1.90491019, -2.06342793, ..., -2.02266812,\n",
      "       -1.65731696, -1.75585831], dtype=float64), Array([-1.98422424, -1.76240061, -2.36730867, ..., -1.98298545,\n",
      "       -1.52062393, -1.64540718], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 43.9267:  10%|█         | 1/10 [00:05<00:46,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n",
      "1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3328:  20%|██        | 2/10 [00:10<00:40,  5.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1716:  30%|███       | 3/10 [00:15<00:35,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.0261:  40%|████      | 4/10 [00:20<00:31,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.8847:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7222:  60%|██████    | 6/10 [00:32<00:21,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5986:  70%|███████   | 7/10 [00:36<00:15,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6276:  80%|████████  | 8/10 [00:37<00:07,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5606:  90%|█████████ | 9/10 [00:38<00:02,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5358: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(43.92671504, dtype=float64), Array(3.21197485, dtype=float64), Array(2.33278906, dtype=float64), Array(2.30164024, dtype=float64), Array(2.25242969, dtype=float64), Array(2.17163677, dtype=float64), Array(2.19064591, dtype=float64), Array(2.10653289, dtype=float64), Array(2.15299826, dtype=float64), Array(2.06303583, dtype=float64), Array(2.02612052, dtype=float64), Array(2.06965327, dtype=float64), Array(2.05155638, dtype=float64), Array(1.92429648, dtype=float64), Array(2.00917265, dtype=float64), Array(1.88470089, dtype=float64), Array(1.92459251, dtype=float64), Array(1.98697541, dtype=float64), Array(1.90410637, dtype=float64), Array(1.95357093, dtype=float64), Array(1.92323463, dtype=float64), Array(1.83792049, dtype=float64), Array(1.87664576, dtype=float64), Array(1.83417827, dtype=float64), Array(1.82337746, dtype=float64), Array(1.83150837, dtype=float64), Array(1.72215115, dtype=float64), Array(1.82796389, dtype=float64), Array(1.81553478, dtype=float64), Array(1.73938135, dtype=float64), Array(1.81679469, dtype=float64), Array(1.78553251, dtype=float64), Array(1.80730219, dtype=float64), Array(1.75225759, dtype=float64), Array(1.70127653, dtype=float64), Array(1.59860924, dtype=float64), Array(1.68554348, dtype=float64), Array(1.75222787, dtype=float64), Array(1.89135645, dtype=float64), Array(1.72505939, dtype=float64), Array(1.71821412, dtype=float64), Array(1.78142873, dtype=float64), Array(1.64924104, dtype=float64), Array(1.62764021, dtype=float64), Array(1.74417552, dtype=float64), Array(1.72942225, dtype=float64), Array(1.65379459, dtype=float64), Array(1.76424813, dtype=float64), Array(1.70222458, dtype=float64), Array(1.72337913, dtype=float64), Array(1.66643199, dtype=float64), Array(1.66760917, dtype=float64), Array(1.72653228, dtype=float64), Array(1.72982824, dtype=float64), Array(1.67138854, dtype=float64), Array(1.67235486, dtype=float64), Array(1.64257353, dtype=float64), Array(1.56059215, dtype=float64), Array(1.64446367, dtype=float64), Array(1.63097598, dtype=float64), Array(1.70658941, dtype=float64), Array(1.53577278, dtype=float64), Array(1.68028235, dtype=float64), Array(1.5564923, dtype=float64), Array(1.67317365, dtype=float64), Array(1.61325945, dtype=float64), Array(1.65740626, dtype=float64), Array(1.66942587, dtype=float64), Array(1.60222527, dtype=float64), Array(1.56375706, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 267, 1017,  171, 1084,  166,  253,  126, 1193,  932,  142,  568,\n",
      "        789,  153, 1211, 1101,  457, 1175,  591, 1159, 1063, 1165,   71,\n",
      "        796,  508,   68,  353,   15,  458,  964,  832,  818, 1058,  628,\n",
      "       1126,  721,  286,  954,  581, 1237,  228, 1076,  355,  488,  678,\n",
      "        377,  530,  765,  513,  860, 1208, 1170,  164,  769,  439, 1259,\n",
      "        503, 1077,  886,  319, 1172,  921,  615, 1160, 1072, 1010,  999,\n",
      "         27,   98, 1120,  981,   83,  620,  388,  339, 1016,  573, 1004,\n",
      "       1029,  472,  916,  398,  894,  558,  109,  723,  991,  870,  170,\n",
      "        372,  441,  603, 1019,  971,  175, 1195,  421,  356,  785,  774,\n",
      "        743,  217,  868,  882,  405,  969,   29,  241,  925,  527,  304,\n",
      "        807,  756,  481,  464, 1081,  344,  154, 1002, 1134,  544,  979,\n",
      "         54, 1168, 1106, 1070, 1118,  594, 1023], dtype=int64), Array([1191,  539,  912,  319,  761, 1126,  732, 1067, 1049,  532,  712,\n",
      "         54,  421, 1195, 1017,  627,  789,  827, 1240, 1145, 1223,  657,\n",
      "        937,  161,  402,  610,  266,  784,  886,  153,  508,  479,  617,\n",
      "         15,  520, 1070,  377,  519,  655,  439,   71,  393, 1118,  396,\n",
      "        636,  232,  416,  173, 1193,   45, 1166,  839,  512, 1208,  162,\n",
      "        739,  812, 1039,  946,  274, 1096,  785,  206, 1159,  123,  126,\n",
      "        221,  289,  625,  442,  953,  171, 1160,  478,  916, 1077,  808,\n",
      "         82,  718, 1155,  268,  756,  553, 1010, 1227, 1206,  699, 1229,\n",
      "       1220,  419,  700,  647,  353,  572,  290,  481,  580, 1127, 1040,\n",
      "        359,   28,   46,  971,  372,  164, 1052, 1116, 1031, 1158,  355,\n",
      "        956,  412,  294,  823,  723, 1134,  457,  176, 1170,  488,  378,\n",
      "       1188,  874,  961,   89,  721,  735, 1244,  367,  944,  573, 1084,\n",
      "        856,  203, 1072,  933,  801,  554,  904,  513, 1029, 1202,  890,\n",
      "        528,  970,  296,  505, 1129, 1251,  510, 1203, 1120,   29,  364,\n",
      "         14, 1152,   31,  743,    0,  131,  155, 1058,  568,  615,  850,\n",
      "        613,  589,   41,  954,  775,  720,  628, 1211,  558, 1013,   66,\n",
      "        666,  800,  503,  485,  847,  157,  753,  533,  320,  741,   86,\n",
      "        240,  701,  249,  805,  300, 1178, 1233,  958,  889, 1046,  401,\n",
      "        121,  356,  267,  607,  930,  619, 1004,   47, 1066,  932, 1137,\n",
      "        223, 1215,  494,  612,   87,  663,  154,  147,  678,  626,  645,\n",
      "        602,  733,  487,  411,  347,  136, 1009,  841,  593,  730,  271,\n",
      "        832, 1106, 1140,  333,  960,  624, 1141, 1028,  540, 1164,  774,\n",
      "        665,  387,  651,  653,  231,  331,  906,  940,   94,  452,  328,\n",
      "       1024,  950,  389,  490,  545,  119,  820,  959,  361,  386, 1005,\n",
      "        116,  722, 1110,  901,  282,  925, 1168,  837,  441,  893,  129,\n",
      "        313,  208,  693,  908,  236,   93,  245, 1022, 1085,  530,   20,\n",
      "        972,  363,   26,  543, 1263, 1083,  175,  193,  905,  822,  265,\n",
      "        252,  371,  861,  835,  109, 1001, 1199, 1257, 1103,  309,   59,\n",
      "        380,  317, 1210,  369,  992,  142,   24,  948,  418,   81, 1064,\n",
      "        769,   18,  748,  632,  591,  708, 1048,  211,  458, 1075, 1165,\n",
      "       1278, 1274,  477,  166,  964,  408,  599, 1186, 1071,  312,  941,\n",
      "        464,  830, 1016,  382,  957,  276,  462,  247,  344,  670, 1138,\n",
      "        286,  684, 1219,  262,  776,  885], dtype=int64), Array([ 147,  267,   86,  961,  396,  789, 1049,  602,  970,   93,   71,\n",
      "        721,  808,  121,  558, 1118,  164,  162,  333,    0,  171,  801,\n",
      "         82,  807,   45, 1031,  540,  412,  791, 1160,  344,  593,  353,\n",
      "        627, 1096,  944,  372,  657, 1068,  136,  478, 1219,  680,   64,\n",
      "        157,  956, 1058,  175,  886,  289,  309,  833,  134,  510,  574,\n",
      "       1052,  732,  520,  545,  847,  653,  361, 1017, 1129, 1274,   94,\n",
      "        479,   54,  628, 1170, 1195, 1084,  203,  735,  610,  619,  194,\n",
      "        591, 1067,  367,  932,  874,  741, 1028,  323,  645,  173,  580,\n",
      "        532, 1168, 1229,  133,  496,  418, 1213, 1178,  207, 1166, 1013,\n",
      "         15,  701, 1029,   98,  441, 1040,  730,  842,  505,  421,  953,\n",
      "        805,  756,  126,  849,  312,  163, 1223,  400,  553, 1070,  775,\n",
      "        901,  531,  765,   46, 1262, 1220, 1244,   14,  172, 1247,  284,\n",
      "        369,  543,   74,  363,  916, 1258,  712,  800, 1015, 1206,  624,\n",
      "        929, 1181,  166,  525,   47,  380, 1236,  521,  108,  547,  462,\n",
      "       1159,   65,  477, 1193,  766, 1126,  154,  784,  900,  140,  946,\n",
      "        387,  841,  416, 1279,  718, 1134,  595,  452, 1140, 1055,   41,\n",
      "       1212,  472,  131,  439, 1191, 1233,  761, 1263,  488,  486,  579,\n",
      "        774, 1137,  743,   28, 1156,  116,  885,   89,  221,  835,  512,\n",
      "         87,  382,   16,  145,  319, 1186,  912,  485,  276,  436,  355,\n",
      "        636, 1046, 1240,  908,  640,  442, 1151,  832,  720,  686,  702,\n",
      "        820,  320,  503,  457, 1016,  129, 1010,   68,  176,  411,  647,\n",
      "        797, 1144,  550,  959,  530,  708,  431,  803,  665, 1110,  750,\n",
      "        812, 1024, 1074,  357,  113,  589, 1152,  655,  300,  855,  311,\n",
      "        199,  717,  515, 1180,  792,  972,  875,  254, 1257, 1020, 1065,\n",
      "       1098,  632, 1203,  948,  443,  282,  663,  308, 1164,   49,  641,\n",
      "        977,  197,  188,  925, 1145, 1066,  272,  268,  895,  568,  689,\n",
      "       1081,  185,  405,  670,   75,  406,  119,  573,  270,  112, 1119,\n",
      "       1173, 1069, 1249,  693, 1138,  742,  315, 1157,  324,  769,  891,\n",
      "        377,  508, 1241,  685, 1083, 1182,  123,  233, 1057,   62,  955,\n",
      "        578,  588, 1215,  408, 1273, 1155,  814, 1050,   55,  752,  981,\n",
      "        313, 1120,  716, 1097,  318,  979,  971,  118,  275,  230,   78,\n",
      "       1139,  930,  903,  626,  837,  734,  159,  358,  738,  630,    4,\n",
      "        933,  402,   56, 1163,  793,  226,  599,  212,  446,  376,  328,\n",
      "        678,  770,  368,  225,  722,  238,  872,  317, 1092,  210,  861,\n",
      "        666,  314,  684, 1216, 1082,  206, 1072,  179,  498,  252,  787,\n",
      "       1060,   29, 1142,  711,  101,  870,  151,  990, 1234, 1077, 1136,\n",
      "        905,  556,   67,  539,  826,  650,  879,   18, 1132,  931,  117,\n",
      "       1075,  301,  918,   66,  945,  869,  795,  757, 1088,  978,   39,\n",
      "        719,  890,  191,  345, 1172, 1005,  733,  214,  338,  965,  850,\n",
      "         80,  165, 1211, 1246, 1269,  354,  753,  364,  940,  283,  904,\n",
      "        115,  852,  697,  919,  389, 1101, 1264, 1209, 1158,  603,  866,\n",
      "        980, 1245,  998,  984,  153,  935,  231,  699,   22, 1102, 1063,\n",
      "         50,  383,  744, 1266, 1184, 1032,  851,  739,  828, 1256,  381,\n",
      "        561,  142,  378, 1122, 1001, 1239,  884,  111, 1036,  295,  960,\n",
      "        892,   33,  102,  995,  293,   37,  150,  815,  596,  664,  132,\n",
      "        455,  316,   99,  549,  356, 1002,  880,  444, 1202,  654, 1162,\n",
      "       1076, 1268,  534,  966,  407, 1030,  569, 1078,  698,  143, 1080,\n",
      "        658, 1116,  839,  964, 1278,   43,  889, 1265,  247,  985, 1023,\n",
      "        999,    5, 1037,  989,  706,  611,  384, 1149,  331, 1042,  517,\n",
      "        336, 1131, 1004,  563,  810,  906,  504,  856,  236,  365,  618,\n",
      "        644,  326,  709,  397,  256,  726,  463,   92,  677,  480,  614,\n",
      "        422,  772,  621,   25,  222,  894,  146,  223,  337,  280,  258,\n",
      "        592,    3,  893,  495,  958,  536,   24,  329,  155,  952,  451,\n",
      "       1048,   36,  934,   57,  997], dtype=int64), Array([1067,  657,  789,  361,  721,  627,  932,   54,  136,  510,  442,\n",
      "       1017,   28,   71,  791,  377,  116,  940,  121,  712,   15,  479,\n",
      "        164,  697,  580,  739,  134,  300,  267,  610,  416, 1170, 1072,\n",
      "        774,  665, 1274, 1223,  735,  775,  418, 1030,  761, 1160, 1220,\n",
      "        203,  367, 1010,  520,  412,  732,   14,  953,  944,  663, 1171,\n",
      "        686, 1040,  961,  956,  171,  353, 1191,  946, 1096,  119,  912,\n",
      "       1219, 1140,  387,  624,  784, 1129,  970,  477, 1118,  558, 1070,\n",
      "        602, 1257,  313,  372,  628,  715,  396, 1024,  568,  129,   46,\n",
      "        220,  655,  439,  743,  841,  756,  718,  572, 1031, 1058, 1120,\n",
      "        147,  294,   45, 1105,  153,  801,  675,  741,   41,  769,  593,\n",
      "        452,  916, 1244,  532, 1206, 1137,   82,   47,  820,  613, 1052,\n",
      "        615, 1009, 1028,  162,  356,  950,  812,  288,  808, 1195,  653,\n",
      "         66,  319,  369, 1029, 1186, 1245,   86,  488,  126,  188,  619,\n",
      "       1155,  832, 1159,  317, 1164,  457,    0,  441,  925, 1013,  553,\n",
      "        805,  589,  355,  933,  573,  176,   87,  175,  173, 1249,  526,\n",
      "        647, 1077,  311, 1046,  835, 1229,  163,  636,  701,  363, 1145,\n",
      "        123, 1126,  890,   93, 1168,  487, 1085,  478, 1166,  512,  505,\n",
      "        917,   62, 1110,  850,  290, 1203,  540,  545,  172,  901,  874,\n",
      "       1158,  435,  364,  766, 1055,  886,   94, 1138, 1020,  724,  708,\n",
      "        421,  411,  206, 1152,  252,  908,  626,  333,  157,  508, 1134,\n",
      "        408,  733,  462, 1239,  800,  599, 1098,  971, 1172,  519,  807,\n",
      "        247,  513,  389, 1263,  320,  752,  543,  650,  905,  645, 1227,\n",
      "        964,  855,   29,  591,  958,  402,  326, 1270, 1236,  230,  472,\n",
      "       1178, 1133,  382,  972,  221,  730,  309,  521, 1063,  539,  223,\n",
      "       1211,  142,  299,  954, 1074,  276,  902,  222,  747,  469,  948,\n",
      "         37,  274,  693,  485, 1049,  861, 1251,  380,  511,  118, 1101,\n",
      "        847,   89, 1084,  344,  109, 1121, 1001,  556,  945, 1141, 1144,\n",
      "        413,  554,  723,  517,  161,  343,  371,   98,  680,  407,  155,\n",
      "       1154, 1161,  906,  640,    1,  991,  231,  669,  386,  937,  257,\n",
      "       1016,  401,  748,  503,  979,  286,  699,  678,  709,  885,  398,\n",
      "        566,  895,  765,  182,  349,  137,  437, 1022,  753,  849,  684,\n",
      "       1180,  445,  354,   78,  154,  376,  232,  815,  331,  446, 1233,\n",
      "       1199,  528,  612, 1076,   26, 1004, 1106,  720,  489,  652, 1113,\n",
      "        632,  185,  391, 1258, 1247, 1027, 1066,  359, 1232,   91,  341,\n",
      "        131,  997,  436, 1261, 1083,  458,  170,  767, 1234,  875,  576,\n",
      "        194,  880,  139,  911,  781,  799,  955, 1175,  383,  959,  673,\n",
      "        330,  424,  915,  744,  255,  120, 1108,  842,  941,  370, 1208,\n",
      "         43,  992,  751,  310, 1033,  179,  502,   52,  759,  393,  226,\n",
      "        308,  777,  981,  304,  105,  641,  698, 1057,  378,  314,  999,\n",
      "         23,   99, 1136,  177,  782, 1279,  592,  598,  198, 1215,  115,\n",
      "        464,  312, 1193,  856,   18,   68, 1188,  658,  324,  689,  268,\n",
      "        995,  486,  110,    9,  289,  982, 1187,  726,   25,  434,  845,\n",
      "        581,  823,  629, 1265,  451,  190,  830,  706,  183,  228,  200,\n",
      "        468,  711, 1131, 1045, 1228,  253,  939,   38, 1071,  879,  551,\n",
      "         31,  474, 1114,   72,  574,  683, 1109,  760,  199,  962,  501,\n",
      "        837, 1218,  969, 1183, 1079,  930,  266,  410,  983, 1278, 1091,\n",
      "        202,  888,  250, 1102, 1238, 1119,  496,  282,  315,  431,   92,\n",
      "        869,  818,  459, 1259,  473,  882,  277, 1272,  579,  802, 1179,\n",
      "       1235, 1212, 1240, 1184,  828,  853,  338,  112,  810,  595,  785,\n",
      "        876,  306,   85, 1262,   20, 1213, 1061, 1273,  368,  422,  235,\n",
      "        809,  827,   42,   81,  323,  225,  166,  607,  197, 1176,   17,\n",
      "       1157,  729,  860,  907, 1276,  374,  509,  682,  755,  562,  770,\n",
      "         83,  564,  773,  727,  934,  634,  482,  664, 1093,  622,  504,\n",
      "        381, 1231,  904,  557,  201,  967,  921,  811,  644, 1075,  516,\n",
      "       1021,  829,  998,  738, 1122,  713,  909,  186,  430,  762, 1068,\n",
      "        365,  210, 1090,  254, 1097,  273, 1169,  456,  490,  656,  691,\n",
      "        994,  666,  530,  822,  455,   90,  414,  318,  260,  746,  740,\n",
      "       1241,  128, 1275, 1268,  379, 1192, 1209,  839,  688, 1181,  169,\n",
      "        481,  968,   64, 1237,  238,  614,  651, 1048,  947, 1202,  631,\n",
      "        604, 1087, 1069,  233, 1103,  525,  865,   84,  722,  405,    5,\n",
      "        248,  866, 1054,  976, 1254, 1089,  763,  132, 1038,  258,  196,\n",
      "        360,  728,  596,  561,  618,  538,  453,  443,   36,  931,  108,\n",
      "        667,  375, 1201,  868, 1037,   21,  211,  385, 1064,  583,  454,\n",
      "        620,  844,   24,  480,  151,  191, 1190,  167, 1032, 1023,  942,\n",
      "        717,  240,   33,  646, 1018, 1086, 1007,  872,  836,  826,  470,\n",
      "        425,  216, 1082,  328, 1255,  867,  878,  929,  271,  102,  633,\n",
      "        778,  565,  754,  256,  332,  776, 1099,  548,   57,  952, 1156,\n",
      "        529,   80,  690,  460, 1111,  321, 1051,  347,  205,  394,  494,\n",
      "        679,   34,  339,  388,  399,  208,  603,   59,  249,  662,  550,\n",
      "        218,  302, 1019,  787,  996,  771,  146,  783,  101,   12,  889,\n",
      "        278,  325,  552,  758,  296,  149,   44,  873, 1092,  846,  160,\n",
      "       1146, 1100,   95,  499,  703,  515,  894,  207,   97, 1002,  350,\n",
      "        957,  871,  292,  913,  215,    8,   11,  219,  103, 1044,  674,\n",
      "        725,  825,   58,  833,  518,  676, 1150, 1230,  834,  295,  986,\n",
      "       1177,  272,  261,  409, 1015], dtype=int64), Array([  86,   71,  164, ..., 1208,  292,  480], dtype=int64), Array([ 45, 267,  82, ..., 780, 975, 635], dtype=int64)], 'curricula_losses': [Array([-37.40061939,  -2.92665984,  -0.89133009, ...,  -1.30468274,\n",
      "       -12.48612879, -10.24925355], dtype=float64), Array([-2.94881493, -2.18823183, -1.55630893, ..., -1.37525962,\n",
      "       -2.32244323, -2.05849835], dtype=float64), Array([-2.62146718, -2.05097176, -2.14788041, ..., -2.09081444,\n",
      "       -2.29472158, -2.45248489], dtype=float64), Array([-2.49859862, -2.17363506, -1.62707287, ..., -1.69864841,\n",
      "       -1.93744972, -2.00188723], dtype=float64), Array([-2.64399209, -1.51627362, -1.68336931, ..., -1.58331358,\n",
      "       -1.86962493, -1.92369859], dtype=float64), Array([-2.59075521, -1.68971815, -1.76594305, ..., -1.70587946,\n",
      "       -1.78319298, -2.00140039], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 4.4651:  10%|█         | 1/10 [00:05<00:45,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3532:  20%|██        | 2/10 [00:10<00:40,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1346:  30%|███       | 3/10 [00:15<00:36,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.9678:  40%|████      | 4/10 [00:20<00:31,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.8447:  50%|█████     | 5/10 [00:26<00:27,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7455:  60%|██████    | 6/10 [00:32<00:22,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6794:  70%|███████   | 7/10 [00:37<00:15,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5375:  80%|████████  | 8/10 [00:37<00:07,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5754:  90%|█████████ | 9/10 [00:38<00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5106: 100%|██████████| 10/10 [00:39<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(4.46511755, dtype=float64), Array(2.35323934, dtype=float64), Array(2.40269622, dtype=float64), Array(2.31726694, dtype=float64), Array(2.21802477, dtype=float64), Array(2.16513881, dtype=float64), Array(2.13459241, dtype=float64), Array(2.14783876, dtype=float64), Array(2.05731897, dtype=float64), Array(2.00028847, dtype=float64), Array(2.02081199, dtype=float64), Array(1.96776432, dtype=float64), Array(2.07603858, dtype=float64), Array(1.90722066, dtype=float64), Array(1.95717144, dtype=float64), Array(1.87353252, dtype=float64), Array(1.941979, dtype=float64), Array(1.91395398, dtype=float64), Array(1.94152502, dtype=float64), Array(1.84470925, dtype=float64), Array(1.9234253, dtype=float64), Array(1.76327231, dtype=float64), Array(1.7966361, dtype=float64), Array(1.78896645, dtype=float64), Array(1.74546087, dtype=float64), Array(1.80271335, dtype=float64), Array(1.86352143, dtype=float64), Array(1.74706682, dtype=float64), Array(1.84207273, dtype=float64), Array(1.82598339, dtype=float64), Array(1.77005728, dtype=float64), Array(1.74014962, dtype=float64), Array(1.76617806, dtype=float64), Array(1.71028554, dtype=float64), Array(1.82860862, dtype=float64), Array(1.73289513, dtype=float64), Array(1.75570773, dtype=float64), Array(1.69902528, dtype=float64), Array(1.78327781, dtype=float64), Array(1.67942963, dtype=float64), Array(1.70372314, dtype=float64), Array(1.69169727, dtype=float64), Array(1.78437741, dtype=float64), Array(1.5374641, dtype=float64), Array(1.64172511, dtype=float64), Array(1.77267588, dtype=float64), Array(1.74808538, dtype=float64), Array(1.79199812, dtype=float64), Array(1.67417201, dtype=float64), Array(1.6765417, dtype=float64), Array(1.63366693, dtype=float64), Array(1.77044699, dtype=float64), Array(1.68898472, dtype=float64), Array(1.71758987, dtype=float64), Array(1.64982424, dtype=float64), Array(1.67053032, dtype=float64), Array(1.63428589, dtype=float64), Array(1.62304677, dtype=float64), Array(1.57539782, dtype=float64), Array(1.63821607, dtype=float64), Array(1.62370455, dtype=float64), Array(1.54607349, dtype=float64), Array(1.6417183, dtype=float64), Array(1.732131, dtype=float64), Array(1.56455516, dtype=float64), Array(1.63940903, dtype=float64), Array(1.51060741, dtype=float64), Array(1.52498508, dtype=float64), Array(1.56373944, dtype=float64), Array(1.51990312, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 360,  930,  476,  214, 1072,  471,  421,  682,  909,  904, 1044,\n",
      "        906,  517,  571, 1078,  972,  337,  248,  691, 1030, 1254,  869,\n",
      "        895, 1069,  181,  960, 1133,  433, 1244,  342,  945,  645,  969,\n",
      "          7,    9,  271,   16,  568,   25,   68,  387,  314,  250,  554,\n",
      "        699,  178,  278,  100,  832,  208,  634,  370,  117,  715,  781,\n",
      "        301,   14, 1214, 1197,  870,  377, 1238,  707,  595,   48, 1109,\n",
      "        221,  412, 1123,  526,  287,  641,  657,  828,  455, 1056,    6,\n",
      "       1263,  642,  467, 1246, 1166,  971, 1047,  262,  582,  965,  788,\n",
      "       1066, 1235,  297,  770,  995,  528,  766,  474,  591,  104, 1224,\n",
      "         43,  631,  845,   66,  496,  776,  661, 1125,  675, 1231,  349,\n",
      "        920, 1081,  274, 1054,  202,  923, 1031, 1236,  432,  551,  462,\n",
      "       1046,  706,  905, 1130, 1026,  985,  872], dtype=int64), Array([ 337,  798,  997,  148,  145,  870, 1010,  241,  788, 1235,  643,\n",
      "        691,  276,  969,  949,  599,  469,  620,  595,    9,  220, 1236,\n",
      "        214,  593,  551, 1169,  202,  127,  906,  432,  930,  184, 1247,\n",
      "        151,  711,  801,  781, 1031, 1097, 1083,  759,  909, 1063,  874,\n",
      "        867,  295,  107,  226,  588,  473, 1167,  780,  330, 1227,  985,\n",
      "        960, 1027,   41,  567,  293,  919,  786, 1123,  648, 1204,  187,\n",
      "       1125,  444,  549,  872,  840,  153,   70, 1210,  892,  890,  436,\n",
      "       1101,  341,  699,  230,   22,  318,  467,    6, 1037,  724, 1254,\n",
      "        257, 1163,  474,  486,  407,  629,  703,  325,  959,  302,  287,\n",
      "        208,  568, 1028,  807, 1065,  698,  433,  816,   50,  815,   79,\n",
      "        805, 1136,   90,  972,  944,  398, 1216,  357,   37, 1076,   20,\n",
      "        886,  479, 1104,   18,  303,   74,  594, 1145, 1212,  272,  260,\n",
      "        752,  742,  882,  630, 1113,  776,  360,  645,  476,  369,   36,\n",
      "        212, 1252, 1257,  814,  117,  345,  687,   68,  146,  445,  304,\n",
      "        262,  657,  359,  684,  156,  564,  869,  723,  583, 1171,  982,\n",
      "        762,  350,  390, 1217,  435,   47, 1001,  419,  941, 1030,  168,\n",
      "        573,  246,  406,   83,  278, 1108,  525,   67,   52,  288,  989,\n",
      "        209, 1003, 1089,  305,  205, 1270,  797,   27,  897,  722,  104,\n",
      "        387, 1118, 1268,  774,  683,   25,  625,  527,  832,  682,  346,\n",
      "          7,  900,   17,  611,  417,  685,  884,  923,  820,  324,  408,\n",
      "        697,  965, 1112,  377,  123,  641,  457,  296, 1116, 1060,  622,\n",
      "       1018,  895,  964,  744, 1056,  191, 1240,  499,  396,  663,  192,\n",
      "       1193, 1190, 1034,  496,  934,  122,  158,  120,  796, 1046,  619,\n",
      "        716,  233, 1054,  557,  131,  800, 1096,  232,  773, 1156,  689,\n",
      "        466, 1242, 1263,  500,  947,  656,  228, 1269, 1093,   53,   13,\n",
      "        945,  364, 1244,  463,  309, 1234, 1007,  811,  661, 1041,  108,\n",
      "       1175,  649,   98, 1177, 1150,  586,  414,  354,  600,  662,  173,\n",
      "       1139,   95,  898, 1129,  929, 1275,  904,  592,  916, 1130,  130,\n",
      "        179, 1079,   16,  763,  253,  558,  135,  860,  349,  741,  421,\n",
      "        881, 1055,  464,  667,  631,  426,  693, 1021,  427,  851,  298,\n",
      "        194,  165, 1127,  822,    5,  511,  301,  715,  142, 1205,  116,\n",
      "       1026,  332,  799,  747,  617,   75,  880, 1103, 1023,  920, 1151,\n",
      "        119,   93,   21,  806,  367,  492], dtype=int64), Array([ 309,  697,  444,  341,  869,  762,  151,  258, 1001, 1123,  295,\n",
      "       1104, 1242,  390,  591,  797,  287,  112, 1235, 1247, 1216,  568,\n",
      "        302,  629, 1167,  203,  959, 1163,  595,  436,  413, 1169, 1230,\n",
      "       1161,  527,  801,  788,  689,  526,  960,  780,  622, 1254,  969,\n",
      "        998,  406, 1246,  874,  832,  820,  313,  815,  685,  987,  122,\n",
      "        657,  407,  230, 1193, 1063,  435,  257,  599,  723, 1046,  949,\n",
      "        135,  906, 1236,  724,  393,   47, 1076,  699,  682, 1028, 1027,\n",
      "        909,  153, 1151,  912,  583,  130,  426,  214, 1204,  107,  565,\n",
      "        643,  219,    5,  474,  990,  432,   18,  113,  228,  799,  934,\n",
      "        781, 1145, 1118, 1054,  433,   50,  232,  593,    9, 1108,  942,\n",
      "        649,  927,  330,  202,  840,  704, 1056,  394,  369,  467,  530,\n",
      "        715, 1041, 1082,  645,   22,  293,  992,  220,  318,  241,  500,\n",
      "        127,  355,  476,  278,  982,  895,  944,  276,  104,  360,  876,\n",
      "       1122,  746,  499,   90,  573,  647, 1136,  146,  798,  776,  923,\n",
      "        920, 1037,  698,  759,  867,  479,  881,  615,  691,  772, 1197,\n",
      "         66,  557,  995,  703,  145,  605,  488, 1074, 1050,  345,  886,\n",
      "        684, 1034,  191,  337, 1042,  548,  376,  812, 1274,  671,  167,\n",
      "        828,  260,  425,   67,  152,  919,  900,   70,  965,  398,  997,\n",
      "        617,  594,  304, 1055, 1269,  320, 1097, 1068, 1200,   27, 1244,\n",
      "       1053, 1040,  567,  870,  957,  899, 1010,  274,  387, 1217,  518,\n",
      "        814,  400,  138,  245,  979, 1186,   13,  506, 1051,  688, 1100,\n",
      "       1205,  212,  147,  872,   20,  742,  271,  687, 1124, 1239,  121,\n",
      "        585,   69,  752,  155,  701, 1020,  296,  620, 1083,    7, 1093,\n",
      "        922,  396,  364, 1212, 1030, 1270,  408, 1164,  226, 1187,  644,\n",
      "       1017,  562,  589,   36,  269,  961,  457,  486, 1255,  705,  989,\n",
      "        150,  839,  835,  972,  734, 1133,  472,  714,  243,  816,   91,\n",
      "        833,  897,  370,  603,  100,  367,  963,  312,  663,  523,  445,\n",
      "        128,  529, 1227,   95,  213, 1177,  358,  747, 1276, 1015,  709,\n",
      "       1125,  148,    8,  117,  281,  473,  108, 1018,  826,  623,  855,\n",
      "        659,  786,  868, 1179,  827,  328, 1078,  917,  572, 1279,   16,\n",
      "       1245,  507,  785,  850,  301,  964,  847,  321,  351,  882,  421,\n",
      "        732, 1262,  890,  496, 1073,  638, 1107,  297,  652,  329,   71,\n",
      "        277,  910,  222,  165,  905,   98,  149,  439,  385,  656,  534,\n",
      "        731, 1023,  325, 1150, 1094,  666, 1175,  736, 1257, 1098, 1231,\n",
      "       1188,  335,  755,  976,   58, 1229,  378, 1180,  229,  765, 1119,\n",
      "        447,  111,  930,  323,  550,  434,  464,  884,  947,  700,  227,\n",
      "       1268,  817,   35,  403, 1263,  208,  551,  487,  361, 1117,  750,\n",
      "       1143,  625,  428,   26,  845,  611, 1026,  233, 1000,  382,  170,\n",
      "       1084,  125,  985, 1178,  764,  996,  175,  187,  779,  249,  377,\n",
      "        637,  120, 1113, 1233,  524,  528,  597,  372,   43, 1206,  856,\n",
      "        774, 1005,  409,  504, 1088, 1162,  159,   93, 1240,   65,  853,\n",
      "         56,  630,  836,  171,   14, 1148,  244,  646,  541,  896,    1,\n",
      "        456, 1033, 1003,  854, 1116,  825,  123,  185, 1214,  770,   68,\n",
      "        726,  744,  892,  999,  414,  531, 1012, 1060, 1183,  604,  412,\n",
      "        458,  748,  156,  454,  136, 1080,  498,  824,  391,  514,  674,\n",
      "        275,  875,  560,  184,  516, 1256,  368,   28,  725, 1172,    0,\n",
      "       1134,  564, 1071,  353,  790,  686, 1221,  290,  422,  766, 1049,\n",
      "       1219,  621,  137, 1131,  946,  168,  419,  431, 1127,  157, 1232,\n",
      "        810,  577,  664,  784,  962,  239, 1139,  846,  520,   12,  861,\n",
      "        103,  679,  702, 1031,  384,  880,  455,  753,  740,  859,  237,\n",
      "        968,   37, 1156, 1032, 1095,  322, 1241, 1210,  450, 1128,   54,\n",
      "        907,  904, 1160,  265,  238,  665,  945,  903,  537,  851,  178,\n",
      "         11,  371,   40, 1189, 1115,  570,  522,  209,  180,  741,  471,\n",
      "       1014,  951,  718,  315, 1106], dtype=int64), Array([ 295, 1247,  433,  145,  643,  759, 1076,  257,  341,  909,  241,\n",
      "        107,  780,  593,  969,  781,  906, 1236, 1010,   22,  337,  445,\n",
      "        620,  982,  798, 1235,  997,  815,  595,  474,  469,  104,  330,\n",
      "       1218,  276, 1145,  153, 1214,  788,  473,  727,  226,  432,  287,\n",
      "        870,  302,  699, 1116,  117, 1204, 1169,   36, 1254,  886,  949,\n",
      "         18, 1123, 1139,  657,  801, 1023,  682,  989,  202,  127,  228,\n",
      "         47,  408,  960, 1130, 1097,  867,  258,  985,  360,  797, 1063,\n",
      "        724,  421,  220,  151,  467,  364, 1163,  902,   50,  972,  444,\n",
      "        944,  715,  148, 1037,  959,  293,  890,  872,  227, 1227,  165,\n",
      "        703,  387,  278,  684,  455,  419,  500,  752,  296,  191,  146,\n",
      "       1136,  551, 1108,  622,  527, 1031,  594,  349,  476,  568,  912,\n",
      "        965,  499,  309,  742,  645, 1027, 1216,  406, 1133, 1056, 1003,\n",
      "        762, 1212, 1001,   67,  230,  301,  318, 1229,  271, 1193,  398,\n",
      "        414, 1046,  687,  874,  599,  567,  108, 1205, 1270,  685, 1028,\n",
      "        200,  816,  370,  412,  250,  923,  260,  776, 1242,  208,   27,\n",
      "        212, 1239,  390,  479,  304,  941, 1234, 1167,  723,  209,  113,\n",
      "       1118,  222, 1104,  919,  814,  121,  998, 1210,  369,  897,  629,\n",
      "       1018, 1078,  214,  934,  486, 1125,    9, 1177,  869,  156, 1054,\n",
      "        591,   95,   98,  714,   90,  407,  892,  820, 1041,  435,  573,\n",
      "        528,  436,  120, 1004,  698,  786, 1240,  630,  856,  203,  649,\n",
      "        691,  829,  900,  554, 1230,   13,  964,  580, 1268,  112,   16,\n",
      "        377,  895, 1083, 1093,  920, 1150, 1026, 1219, 1175, 1231,  822,\n",
      "       1135,  747,  233,  851,  122,  496,  946, 1113,  562,  881,  689,\n",
      "        325, 1208,  799, 1103,  193, 1055,  832,  350,  376, 1120,  393,\n",
      "        187,  396,  722, 1179,  625, 1256,  100,  746, 1159,  583,  184,\n",
      "        663, 1171,  346,  560,  378, 1034, 1158,  135,  438,   68,  232,\n",
      "       1269,  549,   99,    7,    5,  426,  908,  774,   66, 1206,  876,\n",
      "        281,  954, 1246,  347,  428,  777, 1112,  863,  483, 1263,  329,\n",
      "        526, 1040,  320,  939, 1151, 1181,   12,  471, 1100, 1149,  644,\n",
      "        884, 1244,  837,  844,  189,  425,  158,  662, 1186, 1176,  688,\n",
      "        119,  537,  219, 1220,   70, 1065,  297,  812,  245,  123,  564,\n",
      "        277,  168,  345,  280, 1080,  611,  693,  882,  904,  927,  129,\n",
      "        557,  733,  805,  597, 1257,  990,  534,  178,   75,   20,  385,\n",
      "        542,  231,   74, 1042,  516, 1160,  116,  738, 1109,  670,   55,\n",
      "        565, 1153,  773,  898,  335,  348,  751,  835,  655,  648,   35,\n",
      "        394,  809, 1156,   91,  843,   56,  477,  603,  180,  634,  732,\n",
      "        221, 1099,  356,  635, 1024,   10,  481,   71,  289,  144,  559,\n",
      "        569, 1067,  422, 1189,  833,  606,    6, 1261,  945, 1021,  642,\n",
      "        125,  205,  987, 1253,   40,  621,   43,  163,  143, 1190,  767,\n",
      "        930,  973,  905, 1221,  525,  605,  507,  979,  638, 1082,  961,\n",
      "        873, 1170, 1271,  667,  558,  188, 1030, 1011,  817,  459, 1073,\n",
      "       1114,  929,  859, 1092,  901,   77,  958,  705,  285,  578,  743,\n",
      "       1089,  588, 1215, 1005, 1265,  704,  411,  899,  571,  953,  681,\n",
      "        482,  447,   42,  453,  288,  248,  211,   41,  888,  637,   65,\n",
      "       1197,  466, 1057,  917,  828, 1209,  947,  449,  840,  702,  650,\n",
      "        719,  658,  457,   37, 1036,  314, 1059,  586,  862,  748,  988,\n",
      "       1085,  986,  716,  371,  619,   32,  359,  661,   94,  303,  340,\n",
      "       1223,  492,  760,  726, 1207, 1000, 1033,  536, 1068, 1202,  272,\n",
      "        665,  889,  282,  626, 1172,  374,   64,   85,  155, 1191,  744,\n",
      "        338,  137,  115,  779,  866,   59, 1147,  697,  454,  618,    4,\n",
      "         28,  488,  940, 1101, 1278,  491,   17,  522,  737,   96, 1262,\n",
      "        441,  264,  755,  761, 1044,   78, 1060,  307, 1225, 1273, 1079,\n",
      "        256,  170, 1194,  484, 1052,  887,  957,  215,  270,    0,  653,\n",
      "        300, 1233,  903,  853, 1152,  434, 1258,  460,  530,  322, 1201,\n",
      "        950,  602,  880, 1045,  172,  632,  323,   83,  353,  778,   23,\n",
      "        463,  413,  487,  807,  970,  765,  933,  545,  532,  179,  283,\n",
      "        391,  182,  324,  656,  141, 1200,  896,  770,  363,  427, 1274,\n",
      "       1084,  478,  794, 1199,  430,  252,  458,  382,  804, 1025, 1121,\n",
      "        541,  204,  368,  480,  130,  305,   60,  772,  574,   72,  647,\n",
      "        729,  971,  821, 1072,  313,  400,  885,  358,  713,   48,  962,\n",
      "        596,  456,  609,  126, 1074,  262,  641,  362,  617,  319, 1053,\n",
      "       1138,  827,  627,  676, 1164,  511,  624,  440,  251, 1090,   30,\n",
      "        279,  996,  916,   61, 1144, 1006, 1185, 1276,  803,  218,  935,\n",
      "        575,  417, 1115,  639, 1029,  246,  810,  811,  229,  717,  552,\n",
      "       1266,  354,  224, 1088, 1224,   49, 1095,  269,  943,  800,  736,\n",
      "       1129, 1259, 1035, 1155,  683,  718,  711,  501,   73,  399,  589,\n",
      "        263,  190, 1071, 1165, 1022,  384,   97,  367,  695,  666,  462,\n",
      "        992,  795,  409,  498,  690, 1117,  339,  468, 1255,  613,  538,\n",
      "        610,   24,  631, 1134, 1107,  198,  802,  766,  166,  207,  529,\n",
      "         53,  980,  331,  177, 1132, 1217, 1222,  533,  365,  981,  855,\n",
      "        849,  686,  298, 1110,  921,  550,  615, 1275, 1064,  539,   93,\n",
      "       1174,   82,  310,  110,  819,  918, 1249,   87,  332,  956,  101,\n",
      "        771,  793,  175, 1245,  254,  764,  381,  274,  848,  343,  150,\n",
      "        679,  785,  416,  894,  672,  355,   79,  993,   57,  750,  601,\n",
      "        984,  171,   84,  506,  273], dtype=int64), Array([ 444,  960,  341, ...,   33,  109, 1061], dtype=int64), Array([ 337, 1123,  969, ..., 1207,   57,  489], dtype=int64)], 'curricula_losses': [Array([-1.9495023 , -2.43169648, -2.7977408 , ..., -1.42424316,\n",
      "       -2.00237235, -1.45337154], dtype=float64), Array([-2.09889389, -2.1527325 , -2.11361986, ..., -2.08164026,\n",
      "       -2.08426601, -2.0500699 ], dtype=float64), Array([-2.24876905, -2.26007548, -2.03695979, ..., -2.0743005 ,\n",
      "       -2.14389741, -2.31550471], dtype=float64), Array([-1.89240382, -1.50819496, -1.58538427, ..., -1.53997115,\n",
      "       -1.90401503, -1.6836753 ], dtype=float64), Array([-1.87567212, -1.43794584, -1.70334578, ..., -1.64168257,\n",
      "       -1.78249555, -1.7425806 ], dtype=float64), Array([-1.73109517, -1.50517485, -1.49763419, ..., -1.43432821,\n",
      "       -1.6623814 , -1.63020714], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try training it severeal times in a for loop to see if we get oom errors or not. \n",
    "for i in range(3):\n",
    "    train_spl_curriculum(i,i, 128*10, 400,100, 128,  [0.3,0.7], 128, \"./spl_traindata/spl_training_data\"+str(i)+\"/\", \"spl_model_\"+str(i), 0.1, 5)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 3000 len(batches) 100 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_np_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./spl_training_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspl_model_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 539\u001b[0m, in \u001b[0;36mtrain_np_baseline\u001b[0;34m(dataset_key_int, dataloader_key_int, dataset_size, training_step_number, eval_intervals, eval_dataset_size, sampler_ratios, chunk_size, save_path, model_name)\u001b[0m\n\u001b[1;32m    537\u001b[0m     loss_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(loss_array_eval)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m--> 539\u001b[0m     params_new, opt_state, loss_arr \u001b[38;5;241m=\u001b[39m \u001b[43mscan_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Update the training steps\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Since this variable is only used inside the function and never later , it doesnt matter for the training_step_number restriction if it overcounts.  \u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Although it would so pay attention if implementation changes.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_steps, \u001b[38;5;28mlen\u001b[39m(batches))\n",
      "Cell \u001b[0;32mIn[3], line 382\u001b[0m, in \u001b[0;36mtrain_np_baseline.<locals>.scan_train\u001b[0;34m(params, opt_state, key, batches)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscan_train\u001b[39m(params, opt_state, key,  batches):\n\u001b[0;32m--> 382\u001b[0m     last, out \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     params, opt_state, _ \u001b[38;5;241m=\u001b[39m last\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params, opt_state, out\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:287\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll, _split_transpose)\u001b[0m\n\u001b[1;32m    285\u001b[0m   in_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39min_state, \u001b[38;5;241m*\u001b[39min_carry, \u001b[38;5;241m*\u001b[39min_ext]\n\u001b[1;32m    286\u001b[0m   num_carry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(attrs_tracked)\n\u001b[0;32m--> 287\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mscan_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_consts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_carry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_carry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43min_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                  \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m_split_transpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_split_transpose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attrs_tracked:\n\u001b[1;32m    294\u001b[0m   out_state, out \u001b[38;5;241m=\u001b[39m split_list(out, [\u001b[38;5;28mlen\u001b[39m(attrs_tracked)])\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:1262\u001b[0m, in \u001b[0;36mscan_bind\u001b[0;34m(*args, **params)\u001b[0m\n\u001b[1;32m   1260\u001b[0m   _scan_typecheck(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39min_atoms, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m   1261\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_jaxpr(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaxpr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mjaxpr)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAxisPrimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:2788\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2784\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2785\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2786\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2787\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:298\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 298\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    301\u001b[0m   maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    302\u001b[0m       executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m    303\u001b[0m       jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:176\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:2788\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2784\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2785\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2786\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2787\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1488\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1485\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1486\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xla_extension_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m226\u001b[39m:\n\u001b[0;32m-> 1488\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_arg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxla_extension_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m229\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_shard_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1492\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m                       tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1496\u001b[0m                       _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1471\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1471\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1477\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1478\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1479\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1406\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[1;32m   1399\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m _resolve_in_shardings(\n\u001b[1;32m   1400\u001b[0m     args, in_shardings, out_shardings,\n\u001b[1;32m   1401\u001b[0m     resource_env\u001b[38;5;241m.\u001b[39mphysical_mesh \u001b[38;5;28;01mif\u001b[39;00m resource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1403\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2369\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2368\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2369\u001b[0m     executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2373\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2908\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2905\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2908\u001b[0m xla_executable, compile_options \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_replicated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2915\u001b[0m   semantics_in_shardings \u001b[38;5;241m=\u001b[39m SemanticallyEqualShardings(\n\u001b[1;32m   2916\u001b[0m       in_shardings, global_in_avals)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2718\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2713\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2716\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2717\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2718\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:333\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    324\u001b[0m       backend,\n\u001b[1;32m    325\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m       cache_key,\n\u001b[1;32m    331\u001b[0m   )\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:500\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    492\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    493\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    498\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    499\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 500\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    504\u001b[0m   _cache_write(\n\u001b[1;32m    505\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    506\u001b[0m   )\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_np_baseline(0,0, 128*100, 3000, 100, 128, [0.3,0.7], 128, \"./spl_training_data/\", \"spl_model_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['curricula_weights', 'curricula_losses'])\n",
      "[(1280,), (3584,), (5888,), (8192,), (10496,), (12799,)]\n",
      "------\n",
      "[12800, 12800, 12800, 12800, 12800, 12800]\n",
      "300\n",
      "[Array(2.29060209, dtype=float64), Array(2.29909844, dtype=float64), Array(2.28902785, dtype=float64), Array(2.2461967, dtype=float64), Array(2.19320791, dtype=float64), Array(2.13528681, dtype=float64), Array(2.15164188, dtype=float64), Array(2.19272812, dtype=float64), Array(2.10880933, dtype=float64), Array(2.29776451, dtype=float64), Array(2.29199952, dtype=float64), Array(2.28311861, dtype=float64), Array(2.24845846, dtype=float64), Array(2.29688893, dtype=float64), Array(2.25350828, dtype=float64), Array(2.28710009, dtype=float64), Array(2.24978636, dtype=float64), Array(2.25361405, dtype=float64), Array(2.23469844, dtype=float64), Array(2.24080019, dtype=float64), Array(2.28575186, dtype=float64), Array(2.22914399, dtype=float64), Array(2.26851264, dtype=float64), Array(2.23539186, dtype=float64), Array(2.24233039, dtype=float64), Array(2.23173241, dtype=float64), Array(2.25461616, dtype=float64), Array(2.22094342, dtype=float64), Array(2.24437068, dtype=float64), Array(2.24341911, dtype=float64), Array(2.21768619, dtype=float64), Array(2.21001917, dtype=float64), Array(2.23933155, dtype=float64), Array(2.25919972, dtype=float64), Array(2.24442214, dtype=float64), Array(2.20219572, dtype=float64), Array(2.26483053, dtype=float64), Array(2.13469144, dtype=float64), Array(2.10669879, dtype=float64), Array(2.03235848, dtype=float64), Array(2.08161022, dtype=float64), Array(2.11090065, dtype=float64), Array(2.11592455, dtype=float64), Array(2.13844802, dtype=float64), Array(2.07847044, dtype=float64), Array(2.10183222, dtype=float64), Array(2.10618984, dtype=float64), Array(2.1046232, dtype=float64), Array(2.10257236, dtype=float64), Array(2.11595998, dtype=float64), Array(2.14404152, dtype=float64), Array(2.09580445, dtype=float64), Array(2.05738916, dtype=float64), Array(2.07971163, dtype=float64), Array(2.10241271, dtype=float64), Array(2.06892521, dtype=float64), Array(2.06074852, dtype=float64), Array(2.08139448, dtype=float64), Array(2.08032838, dtype=float64), Array(2.05144551, dtype=float64), Array(2.08296614, dtype=float64), Array(2.09155002, dtype=float64), Array(2.10735368, dtype=float64), Array(2.06437102, dtype=float64), Array(2.0930424, dtype=float64), Array(2.07665544, dtype=float64), Array(2.10318903, dtype=float64), Array(2.08846698, dtype=float64), Array(2.06106068, dtype=float64), Array(2.05079564, dtype=float64), Array(2.09168509, dtype=float64), Array(2.1063291, dtype=float64), Array(2.08927674, dtype=float64), Array(2.08868806, dtype=float64), Array(2.09709866, dtype=float64), Array(2.06806364, dtype=float64), Array(2.01059609, dtype=float64), Array(2.06851349, dtype=float64), Array(2.05494319, dtype=float64), Array(2.04905573, dtype=float64), Array(2.03967557, dtype=float64), Array(2.02825941, dtype=float64), Array(2.0730702, dtype=float64), Array(1.96072677, dtype=float64), Array(1.94797566, dtype=float64), Array(1.99059379, dtype=float64), Array(1.9833269, dtype=float64), Array(1.99033859, dtype=float64), Array(1.9647804, dtype=float64), Array(1.89713733, dtype=float64), Array(1.92818422, dtype=float64), Array(1.95521957, dtype=float64), Array(1.98619961, dtype=float64), Array(1.95745883, dtype=float64), Array(1.95202468, dtype=float64), Array(1.92462509, dtype=float64), Array(1.94346176, dtype=float64), Array(1.98592194, dtype=float64), Array(1.97301158, dtype=float64), Array(1.95241728, dtype=float64), Array(1.94170735, dtype=float64), Array(1.8717244, dtype=float64), Array(1.96878897, dtype=float64), Array(1.89978913, dtype=float64), Array(1.95430211, dtype=float64), Array(1.95373231, dtype=float64), Array(2.0042655, dtype=float64), Array(1.9313668, dtype=float64), Array(1.89415105, dtype=float64), Array(1.89889347, dtype=float64), Array(1.93199229, dtype=float64), Array(1.84768222, dtype=float64), Array(1.93508024, dtype=float64), Array(1.94957088, dtype=float64), Array(1.89821444, dtype=float64), Array(1.95069754, dtype=float64), Array(1.92145853, dtype=float64), Array(1.90474263, dtype=float64), Array(1.85181109, dtype=float64), Array(2.07488314, dtype=float64), Array(2.09378316, dtype=float64), Array(1.9574513, dtype=float64), Array(1.87349441, dtype=float64), Array(1.98372486, dtype=float64), Array(1.91359032, dtype=float64), Array(1.91026638, dtype=float64), Array(1.9526963, dtype=float64), Array(1.80979127, dtype=float64), Array(1.94717377, dtype=float64), Array(1.97713118, dtype=float64), Array(1.83361117, dtype=float64), Array(1.9364752, dtype=float64), Array(1.87755207, dtype=float64), Array(1.94166184, dtype=float64), Array(1.87873095, dtype=float64), Array(1.94751788, dtype=float64), Array(1.87670256, dtype=float64), Array(1.93781936, dtype=float64), Array(1.88265613, dtype=float64), Array(1.85152639, dtype=float64), Array(1.82803333, dtype=float64), Array(1.8389523, dtype=float64), Array(1.81769308, dtype=float64), Array(1.932443, dtype=float64), Array(1.83110844, dtype=float64), Array(1.81456356, dtype=float64), Array(1.72802939, dtype=float64), Array(1.64042022, dtype=float64), Array(1.72957522, dtype=float64), Array(1.69450771, dtype=float64), Array(1.64748368, dtype=float64), Array(1.72130233, dtype=float64), Array(1.6504178, dtype=float64), Array(1.57328583, dtype=float64), Array(1.56376786, dtype=float64), Array(1.67983958, dtype=float64), Array(1.6566012, dtype=float64), Array(1.69857782, dtype=float64), Array(1.60280525, dtype=float64), Array(1.62890962, dtype=float64), Array(1.59865464, dtype=float64), Array(1.65601441, dtype=float64), Array(1.59329937, dtype=float64), Array(1.67661622, dtype=float64), Array(1.62537057, dtype=float64), Array(1.59620458, dtype=float64), Array(1.73050503, dtype=float64), Array(1.67344823, dtype=float64), Array(1.66952233, dtype=float64), Array(1.65261222, dtype=float64), Array(1.61270855, dtype=float64), Array(1.55323838, dtype=float64), Array(1.57990572, dtype=float64), Array(1.68063106, dtype=float64), Array(1.66795784, dtype=float64), Array(1.64226389, dtype=float64), Array(1.53597897, dtype=float64), Array(1.67357644, dtype=float64), Array(1.60400297, dtype=float64), Array(1.58913553, dtype=float64), Array(1.64942519, dtype=float64), Array(1.5999489, dtype=float64), Array(1.55094511, dtype=float64), Array(1.6104044, dtype=float64), Array(1.61398145, dtype=float64), Array(1.58182456, dtype=float64), Array(1.56613184, dtype=float64), Array(1.54407437, dtype=float64), Array(1.54348845, dtype=float64), Array(1.6053823, dtype=float64), Array(1.60232578, dtype=float64), Array(1.60447097, dtype=float64), Array(1.63099873, dtype=float64), Array(1.51200843, dtype=float64), Array(1.55767579, dtype=float64), Array(1.63236923, dtype=float64), Array(1.53697567, dtype=float64), Array(1.62301939, dtype=float64), Array(1.66116372, dtype=float64), Array(1.68916948, dtype=float64), Array(1.58256684, dtype=float64), Array(1.61000285, dtype=float64), Array(1.58524841, dtype=float64), Array(1.65638871, dtype=float64), Array(1.57580419, dtype=float64), Array(1.54076794, dtype=float64), Array(1.5593083, dtype=float64), Array(1.58842113, dtype=float64), Array(1.59205568, dtype=float64), Array(1.53831511, dtype=float64), Array(1.59948292, dtype=float64), Array(1.60071739, dtype=float64), Array(1.54854785, dtype=float64), Array(1.58680464, dtype=float64), Array(1.57123426, dtype=float64), Array(1.51930156, dtype=float64), Array(1.65702905, dtype=float64), Array(1.53218915, dtype=float64), Array(1.55952976, dtype=float64), Array(1.4680395, dtype=float64), Array(1.54304002, dtype=float64), Array(1.50492651, dtype=float64), Array(1.73297123, dtype=float64), Array(1.64582723, dtype=float64), Array(1.62675029, dtype=float64), Array(1.54528113, dtype=float64), Array(1.57757912, dtype=float64), Array(1.60194279, dtype=float64), Array(1.63750824, dtype=float64), Array(1.34040948, dtype=float64), Array(1.34995572, dtype=float64), Array(1.36770305, dtype=float64), Array(1.31462825, dtype=float64), Array(1.3944242, dtype=float64), Array(1.39258827, dtype=float64), Array(1.41449893, dtype=float64), Array(1.31667014, dtype=float64), Array(1.33871222, dtype=float64), Array(1.16326812, dtype=float64), Array(1.30723134, dtype=float64), Array(1.17380377, dtype=float64), Array(1.27073999, dtype=float64), Array(1.41585996, dtype=float64), Array(1.39004389, dtype=float64), Array(1.25034816, dtype=float64), Array(1.48571094, dtype=float64), Array(1.25337967, dtype=float64), Array(1.51389815, dtype=float64), Array(1.31913624, dtype=float64), Array(1.36900132, dtype=float64), Array(1.40179671, dtype=float64), Array(1.22863221, dtype=float64), Array(1.28643249, dtype=float64), Array(1.30852448, dtype=float64), Array(1.16324396, dtype=float64), Array(1.39763619, dtype=float64), Array(1.20114618, dtype=float64), Array(1.33403298, dtype=float64), Array(1.40071654, dtype=float64), Array(1.17792272, dtype=float64), Array(1.34506035, dtype=float64), Array(1.35974611, dtype=float64), Array(1.37323046, dtype=float64), Array(1.26642174, dtype=float64), Array(1.30759423, dtype=float64), Array(1.38173646, dtype=float64), Array(1.33372274, dtype=float64), Array(1.27649949, dtype=float64), Array(1.41606192, dtype=float64), Array(1.36684097, dtype=float64), Array(1.34790642, dtype=float64), Array(1.31562008, dtype=float64), Array(1.3105475, dtype=float64), Array(1.37099213, dtype=float64), Array(1.28118355, dtype=float64), Array(1.30200264, dtype=float64), Array(1.18862831, dtype=float64), Array(1.33265071, dtype=float64), Array(1.27718826, dtype=float64), Array(1.42377407, dtype=float64), Array(1.26193465, dtype=float64), Array(1.11402241, dtype=float64), Array(1.22589252, dtype=float64), Array(1.19190196, dtype=float64), Array(1.13528034, dtype=float64), Array(1.38815021, dtype=float64), Array(1.5275268, dtype=float64), Array(1.22719344, dtype=float64), Array(1.29971904, dtype=float64), Array(1.22748061, dtype=float64), Array(1.43401447, dtype=float64), Array(1.41269833, dtype=float64), Array(1.44281058, dtype=float64), Array(1.28340026, dtype=float64), Array(1.29210625, dtype=float64), Array(1.21987801, dtype=float64), Array(1.34247096, dtype=float64), Array(1.36117082, dtype=float64), Array(1.30954897, dtype=float64)]\n",
      "training in task errors\n",
      "{'ece': [Array(-1.70100546, dtype=float64), Array(-1.30177305, dtype=float64)], 'rmse': [Array(1.27534312, dtype=float64), Array(1.14523866, dtype=float64)], 'std_residuals': [Array(0.82427542, dtype=float64), Array(0.97109711, dtype=float64)]}\n",
      "2\n",
      " training outtask errors\n",
      "{'ece': [Array(-2.50619525, dtype=float64), Array(-1.61444061, dtype=float64)], 'rmse': [Array(1.39656962, dtype=float64), Array(1.12156753, dtype=float64)], 'std_residuals': [Array(1.27505485, dtype=float64), Array(1.1054488, dtype=float64)]}\n",
      "2\n",
      "---------------\n",
      "{'params': {'embed_both': {'Dense_0': {'bias': (64,), 'kernel': (128, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'embed_xs': {'Dense_0': {'bias': (64,), 'kernel': (1, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'embed_ys': {'Dense_0': {'bias': (64,), 'kernel': (1, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'likelihood': {'projection': {'layers_0': {'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'module': {'Dense_0': {'bias': (128,), 'kernel': (128, 128)}, 'Dense_1': {'bias': (128,), 'kernel': (128, 128)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'LayerNorm_1': {'bias': (128,), 'scale': (128,)}}}, 'layers_1': {'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'module': {'Dense_0': {'bias': (128,), 'kernel': (128, 128)}, 'Dense_1': {'bias': (128,), 'kernel': (128, 128)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'LayerNorm_1': {'bias': (128,), 'scale': (128,)}}}, 'layers_2': {'bias': (2,), 'kernel': (128, 2)}}}, 'posterior_fun': {'likelihood': {'projection': {'Dense_0': {'bias': (128,), 'kernel': (64, 128)}, 'Dense_1': {'bias': (64,), 'kernel': (128, 64)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}}}}}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets check if the necessary savings happened nicely\n",
    "\n",
    "# First the curriculum weights. \n",
    "\n",
    "with open(\"./spl_training_data/spl_model_0_curricula_logs.pkl\", 'rb') as f:\n",
    "    curricula_logs= pickle.load(f)\n",
    "\n",
    "print(curricula_logs.keys())\n",
    "print([w.shape for w in curricula_logs[\"curricula_weights\"]])\n",
    "print(\"------\")\n",
    "\n",
    "print([len(w) for w in curricula_logs[\"curricula_losses\"]])\n",
    "# Now the training metrics\n",
    "\n",
    "with open(\"./spl_training_data/spl_model_0_training_metrics.pkl\", 'rb') as f:\n",
    "    training_metrics = pickle.load(f)\n",
    "\n",
    "print(len(training_metrics[\"training_loss\"]))\n",
    "print(training_metrics[\"training_loss\"][1:])\n",
    "plt.plot(training_metrics[\"training_loss\"][1:])\n",
    "plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(training_metrics[\"training_loss\"])))\n",
    "plt.grid()\n",
    "\n",
    "# Finally the model params\n",
    "\n",
    "print(\"training in task errors\")\n",
    "print(training_metrics[\"training_intask_errors\"])\n",
    "print(len(training_metrics[\"training_intask_errors\"][\"rmse\"]))\n",
    "\n",
    "print(\" training outtask errors\")\n",
    "print(training_metrics[\"training_outtask_errors\"])\n",
    "print(len(training_metrics[\"training_outtask_errors\"]['ece']))\n",
    "\n",
    "print(\"---------------\")\n",
    "\n",
    "params = load_model_params(\"./spl_training_data/spl_model_0.pkl\")\n",
    "\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical cross entropy accuracy metrics, using the log likelihood of gaussians and a given target point y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_params = load_model_params(\"./spl_training_data/spl_model_0.pkl\")\n",
    "\n",
    "model_rng , key_model = jax.random.split(jax.random.PRNGKey(232))\n",
    "model, init_params = create_model(model_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,) (96,)\n",
      "(96, 1) (96, 1)\n",
      "(64, 1) (64, 1) (32, 1) (32, 1)\n",
      "(96, 1)\n",
      "(96, 1) (96, 1, 1) (96, 1, 1) printing the shapes, they should be just an array of values\n",
      "(128, 96) (128, 96)\n",
      "(128, 64, 1) (128, 64, 1) (128, 32, 1) (128, 32, 1)\n",
      "(96, 1) (96, 1, 1) (96, 1, 1) printing the shapes, they should be just an array of values\n",
      "1.3509923177346979\n",
      "1.2327490860445123\n",
      "0.8290704017381973\n"
     ]
    }
   ],
   "source": [
    "num_context_samples = 64\n",
    "num_target_samples = 32\n",
    "batch_size = 128\n",
    "kl_penalty = 1e-4\n",
    "num_posterior_mc = 1\n",
    "acc_rng = jax.random.PRNGKey(42)\n",
    "\n",
    "acc_rng , dataset_key = jax.random.split(acc_rng)\n",
    "# Lets sample a in trask distribution dataset\n",
    "\n",
    "sampler_clean = partial(\n",
    "    joint, \n",
    "    f2, \n",
    "    partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    ")\n",
    "\n",
    "\n",
    "# lets get 1280 samples from the dataset\n",
    "\n",
    "x ,y= sampler_clean(dataset_key) \n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x, y = x[..., None], y[..., None]\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x_context, x_target = jnp.split(x, indices_or_sections=(num_context_samples, ))\n",
    "y_context, y_target = jnp.split(y, indices_or_sections=(num_context_samples, ))\n",
    "\n",
    "print(x_context.shape, y_context.shape, x_target.shape, y_target.shape)\n",
    "\n",
    "\n",
    "full_x = jnp.concatenate([x_context, x_target])\n",
    "print(full_x.shape)\n",
    "\n",
    "acc = cross_entropy_error(model, loaded_params, x_context, y_context, x_target, y_target, acc_rng, 1)\n",
    "\n",
    "xs , ys = jax.vmap(sampler_clean)(jax.random.split(dataset_key,128))\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "xs, ys = xs[..., None], ys[..., None]\n",
    "\n",
    "x_contexts, x_targets = jnp.split(xs, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "y_contexts, y_targets = jnp.split(ys, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "print(x_contexts.shape, y_contexts.shape, x_targets.shape, y_targets.shape)\n",
    "\n",
    "accs = jax.vmap(partial(cross_entropy_error, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(accs.mean())\n",
    "\n",
    "rmse_accs = jax.vmap(partial(RMSE_means, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(rmse_accs.mean())\n",
    "\n",
    "std_residuals = jax.vmap(partial(STD_residuals, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(std_residuals.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
