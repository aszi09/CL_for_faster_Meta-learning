{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "from aa_train_utils.model_utils import create_model, save_model_params, load_model_params\n",
    "from aa_train_utils.dataset_generation import joint, uniform, f2, RegressionDataset\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "import netket as nk\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock\n",
    "#from dataloader import MixtureDataset\n",
    "\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spl_curriculum(dataset_key_int, dataset_size, training_step_number, sampler_ratios, chunk_size, save_path ,  model_name, start_rate, growth_epochs):\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    num_context_samples = 64\n",
    "    num_target_samples = 32\n",
    "    batch_size = 128\n",
    "    kl_penalty = 1e-4\n",
    "    num_posterior_mc = 1\n",
    "\n",
    "\n",
    "    # First lets create the dataset, \n",
    "    # Lets hardcode it for now, and then we can make it more flexible later on\n",
    "    \n",
    "    sampler_noise = partial(\n",
    "        joint, \n",
    "        WhiteNoise(f2, 0.1), \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    sampler_clean = partial(\n",
    "        joint, \n",
    "        f2, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    samplers = [sampler_noise, sampler_clean]\n",
    "\n",
    "    dataset_key = jax.random.PRNGKey(dataset_key_int)\n",
    "    dataset = RegressionDataset(generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, chunk_size, num_context_samples, dataset_key))\n",
    "\n",
    "    # Lets setup the SPL curriculum\n",
    "\n",
    "    rng , curricula_key = jax.random.split(dataset_key)\n",
    "    spl_curricula = SPL_curriculum(start_rate, growth_epochs , dataset, batch_size, curricula_key)\n",
    "\n",
    "\n",
    "\n",
    "    # Lets initalize the model we are going to train\n",
    "\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    model , params = create_model(key)\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "\n",
    "    for i in (pbar := tqdm.trange(10 ,desc='Optimizing params. ')):\n",
    "        \n",
    "        rng, key = jax.random.split(rng)\n",
    "        \n",
    "        batches = jnp.asarray( jax.tree_util.tree_map(lambda tensor : tensor.numpy(), [batch for batch in spl_curricula.data_curriculum(model, params, i)]))\n",
    "        # params_new, opt_state, loss = step(params, opt_state, key)\n",
    "        params_new, opt_state, loss_arr = scan_train(params, opt_state, key,batches)\n",
    "\n",
    "        losses.extend(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "        \n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "        \n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "\n",
    "    # Lets save what we need to save for the model and training. \n",
    "\n",
    "    ### After training we should save  \n",
    "        # the model parameters with a name that we know how it was trained\n",
    "        # the losses and other relevant information accrued during training\n",
    "        # the curriculum weight log for the dataset\n",
    "\n",
    "    # Saving the model params\n",
    "    # We could also save opt_state here for later training\n",
    "    #Also after trying out whether the training would continue saving and loading the params back in I saw change in printed loss. Not sure why that is the case,\n",
    "    # Even if I restore the opt_state as well. Regardless, the model continue training so saving the params is enough to use the model for evaluation later on. \n",
    "    \n",
    "    save_model_params(best_params,save_path, model_name) \n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_curricula_weights.pkl'), 'wb') as f:\n",
    "        if(len(spl_curricula.weight_log)>0):\n",
    "            pickle.dump(spl_curricula.weight_log, f)\n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_training_metrics.pkl'), 'wb') as f:\n",
    "        pickle.dump(losses, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
