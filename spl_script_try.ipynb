{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "from aa_train_utils.model_utils import create_model, save_model_params, load_model_params\n",
    "from aa_train_utils.dataset_generation import joint, uniform, f6, f5, RegressionDataset , generate_noisy_split_trainingdata\n",
    "from aa_train_utils.spl_curriculum import SPL_curriculum \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "import netket as nk\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock\n",
    "#from dataloader import MixtureDataset\n",
    "\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical cross entropy accuracy metrics, using the log likelihood of gaussians and a given target point y.\n",
    "\n",
    "\n",
    "def cross_entropy_error(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "\n",
    "    # Lets compute the model application, \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "    # Lets compute the log likelihood of the target points given the means and stds\n",
    "    log_probs = -0.5 * ((full_y- y_means)**2 / y_stds**2 + jnp.log(2 * jnp.pi * y_stds**2))\n",
    "    log_likelihood = log_probs.mean()\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def RMSE_means(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "    \n",
    "    return jnp.sqrt(jnp.mean((y_means - full_y)**2))\n",
    "\n",
    "\n",
    "def STD_residuals(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "\n",
    "    return (full_y - y_means) / y_stds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO:\n",
    "\n",
    "    - Add validation set to the training loop and log it (In distribution , out of task distribution validations?)\n",
    "\n",
    "    - Determine how to make sure that SPL curricula and Baseline trains on the same amount of data, as currently SPL doesnt train for same steps with the same epoch number (Maybe introduce a trainined_step_number to cut the training?)\n",
    "\n",
    "    - Create the empirical cross entropy difficulty measure and use it for the validation set as well. \n",
    "\n",
    "    - SPL curriculum call with best_params or just params for the loss calculation based ordering?\n",
    "\n",
    "    - Additionally maybe increase the difficulty of the dataset a bit , for smoother learning curve, more interesting results? \n",
    "        - Could also look into creating a more diverse dataset , the split dataset generator would allow for that easily. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# There can be several scenarios with the periodic eval and the training step restriction,\n",
    "\n",
    "# Firstly , and most importantly, we must not train more than the restriction steps. \n",
    "\n",
    "# Secondly, if the eval happens and flows into and over the restriction number, the restriction will only be enforced in the next epoch. \n",
    "\n",
    "# Thirdly, if the eval is too small of a period , and it fits multuplicatively into the number of batches per epoch, it will only run once and not every period within that epoch. \n",
    "\n",
    "\n",
    "def train_spl_curriculum(dataset_key_int,dataloader_key_int, dataset_size, training_step_number, eval_dataset_size, eval_intervals, sampler_ratios, chunk_size, save_path ,  model_name, start_rate, growth_epochs):\n",
    "    \n",
    "    \"\"\" Training function for the SPL curriculum based Neural Process model training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Lets define the training functions here and not in their own files, because I couldnt make them modular enough.\n",
    "    # (The posterior loss was relying on the global variable model, I tried creating a partial with the params not included to have the scan carry over a new param based partial to the step function but it wasnt working, this works for now)\n",
    "\n",
    "    def posterior_loss(\n",
    "        params: flax.typing.VariableDict,\n",
    "        batch,\n",
    "        key: flax.typing.PRNGKey,\n",
    "    ):\n",
    "        key_data, key_model = jax.random.split(key)\n",
    "        \n",
    "\n",
    "\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        x_test = batch[2]\n",
    "        y_test = batch[3]\n",
    "        # Compute ELBO over batch of datasets\n",
    "        elbos = jax.vmap(\n",
    "        partial(\n",
    "                model.apply,\n",
    "                params,  \n",
    "                beta=kl_penalty,\n",
    "                k=num_posterior_mc,\n",
    "                method=model.elbo\n",
    "        ) \n",
    "        )(\n",
    "            X, y, x_test, y_test, rngs={'default': jax.random.split(key_model, X.shape[0])}\n",
    "        )\n",
    "        \n",
    "        return -elbos.mean()\n",
    "\n",
    "    @jax.jit\n",
    "    def step(\n",
    "        theta: flax.typing.VariableDict, \n",
    "        opt_state: optax.OptState,\n",
    "        current_batch,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "    ) -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "        # Implements a generic SGD Step\n",
    "        \n",
    "        # value, grad = jax.value_and_grad(posterior_loss_filtered, argnums=0)(theta, random_key)\n",
    "        value, grad = jax.value_and_grad(posterior_loss, argnums=0)(theta, current_batch, random_key )\n",
    "        \n",
    "        updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "        \n",
    "        return theta, opt_state, value\n",
    "\n",
    "\n",
    "    def body_batch(carry, batch):\n",
    "        params, opt_state, key = carry\n",
    "        key_carry, key_step = jax.random.split(key)\n",
    "\n",
    "        X, x_test = jnp.split(batch[0], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        y, y_test = jnp.split(batch[1], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        params, opt_state, value = step(params, opt_state, (X,y, x_test,y_test ), key_step )\n",
    "\n",
    "        return (params, opt_state, key_carry ), value\n",
    "\n",
    "    jax.jit\n",
    "    def scan_train(params, opt_state, key,  batches):\n",
    "        \n",
    "        last, out = jax.lax.scan(body_batch, (params, opt_state, key ), batches)\n",
    "\n",
    "        params, opt_state, _ = last\n",
    "        \n",
    "        return params, opt_state, out\n",
    "\n",
    "    torch.manual_seed(dataloader_key_int) # Setting the seed for the dataloader\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    num_context_samples = 64\n",
    "    num_target_samples = 32\n",
    "    batch_size = 128\n",
    "    kl_penalty = 1e-4\n",
    "    num_posterior_mc = 1\n",
    "\n",
    "\n",
    "    # First lets create the dataset, \n",
    "    # Lets hardcode it for now, and then we can make it more flexible later on\n",
    "    \n",
    "    sampler_noise = partial(\n",
    "        joint, \n",
    "        WhiteNoise(f6, 0.1), \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    sampler_clean = partial(\n",
    "        joint, \n",
    "        f6, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    out_task_sampler = partial(\n",
    "        joint, \n",
    "        f5, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    samplers = [sampler_noise, sampler_clean]\n",
    "\n",
    "    dataset_key = jax.random.PRNGKey(dataset_key_int)\n",
    "    dataset = RegressionDataset(generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, chunk_size , dataset_key))\n",
    "\n",
    "    # Lets setup the SPL curriculum\n",
    "\n",
    "    rng , curricula_key = jax.random.split(dataset_key)\n",
    "    spl_curricula = SPL_curriculum(start_rate, growth_epochs , dataset, batch_size, curricula_key)\n",
    "\n",
    "\n",
    "\n",
    "    # Lets initalize the model we are going to train\n",
    "\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    model , params = create_model(key)\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "    in_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the in task errors for the model\n",
    "    out_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the out of task errors for the model\n",
    "    training_steps = 0\n",
    "\n",
    "    for i in (pbar := tqdm.trange(10 ,desc='Optimizing params. ')):\n",
    "        \n",
    "        rng, key = jax.random.split(rng)\n",
    "        _ , eval_epoch_key = jax.random.split(rng)\n",
    "        model_partial_loss_function = partial(model.apply, params, beta=kl_penalty, k=num_posterior_mc, method=model.elbo) \n",
    "        \n",
    "\n",
    "\n",
    "        batches = jnp.asarray( jax.tree_util.tree_map(lambda tensor : tensor.numpy(), [batch for batch in spl_curricula.data_curriculum(model_partial_loss_function, i, num_context_samples)]))\n",
    "        # params_new, opt_state, loss = step(params, opt_state, key)\n",
    "        \n",
    "        # Right now the number of batches might lead us to over train for the training_step_number, or overtrain for the eval_intervals\n",
    "\n",
    "        # First take care of the case where we might be overtraining for the eval_intervals. \n",
    "\n",
    "        batches_until_eval = eval_intervals - (training_steps % eval_intervals)\n",
    "        batches_until_end = training_step_number - training_steps\n",
    "        if batches_until_end < len(batches):\n",
    "            batches = batches[:batches_until_end]\n",
    "\n",
    "        # If the batches until eval is less than the batches until end, we eval, then train until batches_until_end - batches_until_eval\n",
    "        # If the batches until eval is more than the batches until end, we train batches_until_end\n",
    "        # if the batches until eval is == to batches_until_end, we eval and then let the training end.\n",
    "        # If the batches until eval is less than the batches until end , but the batches_until_end - (len(batches) - batches_until_eval) is less than 0, we train the rest of the batches and end the training.\n",
    "        # if the batches_until_end is negative or 0 we break the training loop.  \n",
    "        print(\"batches_until_eval\", batches_until_eval, \"batches_until_end\", batches_until_end, \"len(batches)\", len(batches), \"training_steps\", training_steps )\n",
    "\n",
    "        # Okay so if the eval period can fit inside multiple times into a len(batches) it should be run that many times. \n",
    "        # It can be done so if \n",
    "        # if the len(batches) / eval_intervals is larger than 2 . \n",
    "\n",
    "\n",
    "        if batches_until_eval < len(batches):\n",
    "            # then get the slice to make up the eval_intervals\n",
    "            \n",
    "            trained_steps_within_eval = 0\n",
    "             \n",
    "            batch_slice_pre_eval = eval_intervals - ( training_steps % eval_intervals )\n",
    "            batch_slice = batch_slice_pre_eval \n",
    "            loss_array_eval = []\n",
    "            params_new = params\n",
    "            for i in range(0,1+((len(batches)-batch_slice_pre_eval) // eval_intervals)):\n",
    "                \n",
    "\n",
    "                print(\"current eval loop number\", i , \"currently trained steps within eval\", trained_steps_within_eval , \"current batch slice\", (trained_steps_within_eval, (trained_steps_within_eval+batch_slice)) ) \n",
    "                params_new, opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:(trained_steps_within_eval+batch_slice)])\n",
    "\n",
    "                loss_array_eval.extend(loss_arr)  # dont lose the loss values upon next batch training\n",
    "                trained_steps_within_eval += batch_slice\n",
    "                batch_slice = eval_intervals\n",
    "\n",
    "                eval_epoch_key, eval_inkey_data, eval_outkey_data, eval_model_key = jax.random.split(eval_epoch_key, 4)\n",
    "                intask_x_eval, intask_y_eval = jax.vmap(sampler_clean)(jax.random.split(eval_inkey_data, eval_dataset_size)) \n",
    "                intask_x_eval, intask_y_eval = intask_x_eval[..., None], intask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(intask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(intask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                in_task_errors['ece'].append(ece_errors.mean())\n",
    "                in_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                in_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "                # Now lets do the out of task evaluation (f for now like the original notebook)\n",
    "                outtask_x_eval, outtask_y_eval = jax.vmap(out_task_sampler)(jax.random.split(eval_outkey_data, eval_dataset_size))\n",
    "                outtask_x_eval, outtask_y_eval = outtask_x_eval[..., None], outtask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(outtask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(outtask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                out_task_errors['ece'].append(ece_errors.mean())\n",
    "                out_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                out_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            # Now we can train the rest of the batches\n",
    "            \n",
    "            # with trained_steps_within_eval start slicing, only train if len(batches) - trained_steps_within_eval > 0\n",
    "            if len(batches) - trained_steps_within_eval > 0: \n",
    "                print(\"training on the rest of the remaining batches after eval\", len(batches)-trained_steps_within_eval)\n",
    "                params_new , opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:])\n",
    "                loss_array_eval.extend(loss_arr)\n",
    "            else:\n",
    "                print(\"Eval period was the last period, no more training, eval intervals fit perfectly within this batch.\")\n",
    "\n",
    "            \n",
    "            loss_arr = jnp.asarray(loss_array_eval)\n",
    "        else: \n",
    "            params_new, opt_state, loss_arr = scan_train(params, opt_state, key,batches)\n",
    "        \n",
    "        # Update the training steps\n",
    "        # Since this variable is only used inside the function and never later , it doesnt matter for the training_step_number restriction if it overcounts.  \n",
    "        # Although it would so pay attention if implementation changes.\n",
    "        print(training_steps, len(batches))\n",
    "        training_steps+= len(batches)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        losses.extend(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "        \n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "        \n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "\n",
    "        if(training_steps >= training_step_number):\n",
    "            break\n",
    "    # Lets save what we need to save for the model and training. \n",
    "\n",
    "    ### After training we should save  \n",
    "        # the model parameters with a name that we know how it was trained\n",
    "        # the losses and other relevant information accrued during training\n",
    "        # the curriculum weight log for the dataset\n",
    "\n",
    "    # Saving the model params\n",
    "    # We could also save opt_state here for later training\n",
    "    # Also after trying out whether the training would continue saving and loading the params back in I saw change in printed loss. Not sure why that is the case,\n",
    "    # Even if I restore the opt_state as well. Regardless, the model continued training so saving the params is enough to use the model for evaluation later on. \n",
    "    \n",
    "    print(\"printing losses length\", len(losses))\n",
    "    save_model_params(best_params,save_path, model_name) \n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_curricula_logs.pkl'), 'wb') as f:\n",
    "        if(len(spl_curricula.weight_log)>0):\n",
    "            pickle.dump({\"curricula_weights\": spl_curricula.weight_log , \"curricula_losses\": spl_curricula.epoch_losses_log}, f)\n",
    "    \n",
    "    with open(os.path.join(save_path, model_name + '_training_metrics.pkl'), 'wb') as f:\n",
    "        pickle.dump({\"training_loss\" : losses, \"training_intask_errors\": in_task_errors, \"training_outtask_errors\": out_task_errors }, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lets also define a function for the baseline training, so that we can compare the two models later on.\n",
    "\n",
    "def train_np_baseline(dataset_key_int,dataloader_key_int, dataset_size, training_step_number, eval_intervals, eval_dataset_size, sampler_ratios, chunk_size, save_path ,  model_name):\n",
    "\n",
    "    \"\"\" Training function for the SPL curriculum based Neural Process model training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Lets define the training functions here and not in their own files, because I couldnt make them modular enough.\n",
    "    # (The posterior loss was relying on the global variable model, I tried creating a partial with the params not included to have the scan carry over a new param based partial to the step function but it wasnt working, this works for now)\n",
    "\n",
    "    def posterior_loss(\n",
    "        params: flax.typing.VariableDict,\n",
    "        batch,\n",
    "        key: flax.typing.PRNGKey,\n",
    "    ):\n",
    "        key_data, key_model = jax.random.split(key)\n",
    "\n",
    "\n",
    "\n",
    "        X = batch[0]\n",
    "        y = batch[1]\n",
    "        x_test = batch[2]\n",
    "        y_test = batch[3]\n",
    "        # Compute ELBO over batch of datasets\n",
    "        elbos = jax.vmap(\n",
    "        partial(\n",
    "                model.apply,\n",
    "                params,\n",
    "                beta=kl_penalty,\n",
    "                k=num_posterior_mc,\n",
    "                method=model.elbo\n",
    "        )\n",
    "        )(\n",
    "            X, y, x_test, y_test, rngs={'default': jax.random.split(key_model, X.shape[0])}\n",
    "        )\n",
    "\n",
    "        return -elbos.mean()\n",
    "\n",
    "    @jax.jit\n",
    "    def step(\n",
    "        theta: flax.typing.VariableDict,\n",
    "        opt_state: optax.OptState,\n",
    "        current_batch,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "    ) -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "        # Implements a generic SGD Step\n",
    "\n",
    "        # value, grad = jax.value_and_grad(posterior_loss_filtered, argnums=0)(theta, random_key)\n",
    "        value, grad = jax.value_and_grad(posterior_loss, argnums=0)(theta, current_batch, random_key )\n",
    "\n",
    "        updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "\n",
    "        return theta, opt_state, value\n",
    "\n",
    "\n",
    "    def body_batch(carry, batch):\n",
    "        params, opt_state, key = carry\n",
    "        key_carry, key_step = jax.random.split(key)\n",
    "\n",
    "        X, x_test = jnp.split(batch[0], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        y, y_test = jnp.split(batch[1], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        params, opt_state, value = step(params, opt_state, (X,y, x_test,y_test ), key_step )\n",
    "\n",
    "        return (params, opt_state, key_carry ), value\n",
    "\n",
    "    jax.jit\n",
    "    def scan_train(params, opt_state, key,  batches):\n",
    "\n",
    "        last, out = jax.lax.scan(body_batch, (params, opt_state, key ), batches)\n",
    "\n",
    "        params, opt_state, _ = last\n",
    "\n",
    "        return params, opt_state, out\n",
    "\n",
    "\n",
    "    torch.manual_seed(dataloader_key_int) # Setting the seed for the dataloader\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    num_context_samples = 64\n",
    "    num_target_samples = 32\n",
    "    batch_size = 128\n",
    "    kl_penalty = 1e-4\n",
    "    num_posterior_mc = 1\n",
    "\n",
    "\n",
    "    # First lets create the dataset,\n",
    "    # Lets hardcode it for now, and then we can make it more flexible later on\n",
    "\n",
    "    sampler_noise = partial(\n",
    "        joint,\n",
    "        WhiteNoise(f6, 0.1),\n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "\n",
    "    sampler_clean = partial(\n",
    "        joint,\n",
    "        f6,\n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    out_task_sampler = partial(\n",
    "        joint, \n",
    "        f5, \n",
    "        partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    "    )\n",
    "    samplers = [sampler_noise, sampler_clean]\n",
    "\n",
    "    dataset_key = jax.random.PRNGKey(dataset_key_int)\n",
    "    dataset = RegressionDataset(generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, chunk_size , dataset_key))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Lets setup the SPL curriculum\n",
    "\n",
    "    rng , curricula_key = jax.random.split(dataset_key)\n",
    "\n",
    "\n",
    "    # Lets initalize the model we are going to train\n",
    "\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    model , params = create_model(key)\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "    in_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the in task errors for the model\n",
    "    out_task_errors = {'ece':[], 'rmse':[], 'std_residuals':[]} # We will log the out of task errors for the model\n",
    "    training_steps = 0\n",
    "\n",
    "    eval_intervals = 100\n",
    "    for i in (pbar := tqdm.trange(10 ,desc='Optimizing params. ')):\n",
    "\n",
    "        rng, key = jax.random.split(rng)\n",
    "        _ , eval_epoch_key = jax.random.split(rng)\n",
    "        \n",
    "\n",
    "        batches = jnp.asarray( jax.tree_util.tree_map(lambda tensor : tensor.numpy(), [batch for batch in dataloader]))\n",
    "        # params_new, opt_state, loss = step(params, opt_state, key)\n",
    "\n",
    "        batches_until_eval = eval_intervals - (training_steps % eval_intervals)\n",
    "        batches_until_end = training_step_number - training_steps\n",
    "        if batches_until_end < len(batches):\n",
    "            batches = batches[:batches_until_end]\n",
    "\n",
    "        # If the batches until eval is less than the batches until end, we eval, then train until batches_until_end - batches_until_eval\n",
    "        # If the batches until eval is more than the batches until end, we train batches_until_end\n",
    "        # if the batches until eval is == to batches_until_end, we eval and then let the training end.\n",
    "        # If the batches until eval is less than the batches until end , but the batches_until_end - (len(batches) - batches_until_eval) is less than 0, we train the rest of the batches and end the training.\n",
    "        # if the batches_until_end is negative or 0 we break the training loop.  \n",
    "        print(\"batches_until_eval\", batches_until_eval, \"batches_until_end\", batches_until_end, \"len(batches)\", len(batches), \"training_steps\", training_steps )\n",
    "\n",
    "        # Okay so if the eval period can fit inside multiple times into a len(batches) it should be run that many times. \n",
    "        # It can be done so if \n",
    "        # if the len(batches) / eval_intervals is larger than 2 . \n",
    "\n",
    "\n",
    "        if batches_until_eval < len(batches):\n",
    "            # then get the slice to make up the eval_intervals\n",
    "            \n",
    "            trained_steps_within_eval = 0\n",
    "             \n",
    "            batch_slice_pre_eval = eval_intervals - ( training_steps % eval_intervals )\n",
    "            batch_slice = batch_slice_pre_eval \n",
    "            loss_array_eval = []\n",
    "            params_new = params\n",
    "            for i in range(0,1+((len(batches)-batch_slice_pre_eval) // eval_intervals)):\n",
    "                \n",
    "\n",
    "                print(\"current eval loop number\", i , \"currently trained steps within eval\", trained_steps_within_eval , \"current batch slice\", (trained_steps_within_eval, (trained_steps_within_eval+batch_slice)) ) \n",
    "                params_new, opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:(trained_steps_within_eval+batch_slice)])\n",
    "\n",
    "                loss_array_eval.extend(loss_arr)  # dont lose the loss values upon next batch training\n",
    "                trained_steps_within_eval += batch_slice\n",
    "                batch_slice = eval_intervals\n",
    "\n",
    "                eval_epoch_key, eval_inkey_data, eval_outkey_data, eval_model_key = jax.random.split(eval_epoch_key, 4)\n",
    "                intask_x_eval, intask_y_eval = jax.vmap(sampler_clean)(jax.random.split(eval_inkey_data, eval_dataset_size)) \n",
    "                intask_x_eval, intask_y_eval = intask_x_eval[..., None], intask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(intask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(intask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                in_task_errors['ece'].append(ece_errors.mean())\n",
    "                in_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                in_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "                # Now lets do the out of task evaluation (f for now like the original notebook)\n",
    "                outtask_x_eval, outtask_y_eval = jax.vmap(out_task_sampler)(jax.random.split(eval_outkey_data, eval_dataset_size))\n",
    "                outtask_x_eval, outtask_y_eval = outtask_x_eval[..., None], outtask_y_eval[..., None]\n",
    "\n",
    "                #lets split them into the context and target sets\n",
    "                x_contexts, x_targets = jnp.split(outtask_x_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "                y_contexts, y_targets = jnp.split(outtask_y_eval, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "                ece_errors = jax.vmap(partial(cross_entropy_error, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                rmse_errors= jax.vmap(partial(RMSE_means, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "                std_residuals= jax.vmap(partial(STD_residuals, model, params_new, k=num_posterior_mc), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(eval_model_key, eval_dataset_size))\n",
    "\n",
    "                out_task_errors['ece'].append(ece_errors.mean())\n",
    "                out_task_errors['rmse'].append(rmse_errors.mean())\n",
    "                out_task_errors['std_residuals'].append(std_residuals.mean())\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            # Now we can train the rest of the batches\n",
    "            \n",
    "            # with trained_steps_within_eval start slicing, only train if len(batches) - trained_steps_within_eval > 0\n",
    "            if len(batches) - trained_steps_within_eval > 0: \n",
    "                print(\"training on the rest of the remaining batches after eval\", len(batches)-trained_steps_within_eval)\n",
    "                params_new , opt_state, loss_arr = scan_train(params_new, opt_state, key,batches[trained_steps_within_eval:])\n",
    "                loss_array_eval.extend(loss_arr)\n",
    "            else:\n",
    "                print(\"Eval period was the last period, no more training, eval intervals fit perfectly within this batch.\")\n",
    "\n",
    "            \n",
    "            loss_arr = jnp.asarray(loss_array_eval)\n",
    "        else: \n",
    "            params_new, opt_state, loss_arr = scan_train(params, opt_state, key,batches)\n",
    "        \n",
    "        # Update the training steps\n",
    "        # Since this variable is only used inside the function and never later , it doesnt matter for the training_step_number restriction if it overcounts.  \n",
    "        # Although it would so pay attention if implementation changes.\n",
    "        print(training_steps, len(batches))\n",
    "        training_steps+= len(batches)\n",
    "        losses.extend(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "\n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "\n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "\n",
    "        if(training_steps >= training_step_number):\n",
    "            break\n",
    "    # Lets save what we need to save for the model and training.\n",
    "\n",
    "    ### After training we should save\n",
    "        # the model parameters with a name that we know how it was trained\n",
    "        # the losses and other relevant information accrued during training\n",
    "        # the curriculum weight log for the dataset\n",
    "\n",
    "    # Saving the model params\n",
    "    # We could also save opt_state here for later training\n",
    "    # Also after trying out whether the training would continue saving and loading the params back in I saw change in printed loss. Not sure why that is the case,\n",
    "    # Even if I restore the opt_state as well. Regardless, the model continued training so saving the params is enough to use the model for evaluation later on.\n",
    "\n",
    "    save_model_params(best_params,save_path, model_name)\n",
    "\n",
    "    with open(os.path.join(save_path, model_name + '_training_metrics.pkl'), 'wb') as f:\n",
    "        pickle.dump({\"training_loss\" : losses, \"training_intask_errors\": in_task_errors, \"training_outtask_errors\": out_task_errors }, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 0.1 epoch number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_spl_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./spl_training_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspl_model_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 167\u001b[0m, in \u001b[0;36mtrain_spl_curriculum\u001b[0;34m(dataset_key_int, dataloader_key_int, dataset_size, training_step_number, eval_dataset_size, eval_intervals, sampler_ratios, chunk_size, save_path, model_name, start_rate, growth_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m _ , eval_epoch_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    163\u001b[0m model_partial_loss_function \u001b[38;5;241m=\u001b[39m partial(model\u001b[38;5;241m.\u001b[39mapply, params, beta\u001b[38;5;241m=\u001b[39mkl_penalty, k\u001b[38;5;241m=\u001b[39mnum_posterior_mc, method\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39melbo) \n\u001b[0;32m--> 167\u001b[0m batches \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray( jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m tensor : tensor\u001b[38;5;241m.\u001b[39mnumpy(), [batch \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspl_curricula\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_partial_loss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_context_samples\u001b[49m\u001b[43m)\u001b[49m]))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# params_new, opt_state, loss = step(params, opt_state, key)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Right now the number of batches might lead us to over train for the training_step_number, or overtrain for the eval_intervals\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# First take care of the case where we might be overtraining for the eval_intervals. \u001b[39;00m\n\u001b[1;32m    174\u001b[0m batches_until_eval \u001b[38;5;241m=\u001b[39m eval_intervals \u001b[38;5;241m-\u001b[39m (training_steps \u001b[38;5;241m%\u001b[39m eval_intervals)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/aa_train_utils/spl_curriculum.py:52\u001b[0m, in \u001b[0;36mSPL_curriculum.data_curriculum\u001b[0;34m(self, loss_partial, epoch, num_context_samples)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_data_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_difficulty_ordering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_context_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39margsort(losses)[:curr_data_size]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msorted indices shape\u001b[39m\u001b[38;5;124m'\u001b[39m, sorted_indices\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/aa_train_utils/spl_curriculum.py:76\u001b[0m, in \u001b[0;36mSPL_curriculum.calculate_difficulty_ordering\u001b[0;34m(self, loss_partial, num_context_samples)\u001b[0m\n\u001b[1;32m     74\u001b[0m X, x_test \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msplit(xs, indices_or_sections\u001b[38;5;241m=\u001b[39m(num_context_samples, ), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m y, y_test \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msplit(ys, indices_or_sections\u001b[38;5;241m=\u001b[39m(num_context_samples, ), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mchunked_loss_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/netket/jax/_vmap_chunked.py:41\u001b[0m, in \u001b[0;36m_eval_fun_in_chunks\u001b[0;34m(vmapped_fun, chunk_size, argnums, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m args_chunks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m     _get_chunks(a) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnums \u001b[38;5;28;01melse\u001b[39;00m a \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)\n\u001b[1;32m     37\u001b[0m ]\n\u001b[1;32m     38\u001b[0m args_rest \u001b[38;5;241m=\u001b[39m [_get_rest(a) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnums \u001b[38;5;28;01melse\u001b[39;00m a \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args)]\n\u001b[1;32m     40\u001b[0m y_chunks \u001b[38;5;241m=\u001b[39m _unchunk(\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mscanmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvmapped_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscan_append\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rest \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_chunks\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/netket/jax/_scanmap.py:133\u001b[0m, in \u001b[0;36mscanmap.<locals>.f_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m f \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mwrap_init(fun, kwargs)\n\u001b[1;32m    130\u001b[0m f_partial, dyn_args \u001b[38;5;241m=\u001b[39m argnums_partial(\n\u001b[1;32m    131\u001b[0m     f, argnums, args, require_static_args_hashable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    132\u001b[0m )\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscan_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_partial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdyn_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/netket/jax/_scanmap.py:91\u001b[0m, in \u001b[0;36mscan_append_reduce\u001b[0;34m(f, x, append_cond, op, zero_fun)\u001b[0m\n\u001b[1;32m     88\u001b[0m     y_reduce \u001b[38;5;241m=\u001b[39m op(y_carry, y_op)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, y_reduce), y_append\n\u001b[0;32m---> 91\u001b[0m (_, res_op), res_append \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarry_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# reconstruct the result from the reduced and appended parts in the two trees\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tree_select(res_append, res_op)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:287\u001b[0m, in \u001b[0;36mscan\u001b[0;34m(f, init, xs, length, reverse, unroll, _split_transpose)\u001b[0m\n\u001b[1;32m    285\u001b[0m   in_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39min_state, \u001b[38;5;241m*\u001b[39min_carry, \u001b[38;5;241m*\u001b[39min_ext]\n\u001b[1;32m    286\u001b[0m   num_carry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(attrs_tracked)\n\u001b[0;32m--> 287\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mscan_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_consts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_carry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_carry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43min_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                  \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m_split_transpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_split_transpose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attrs_tracked:\n\u001b[1;32m    294\u001b[0m   out_state, out \u001b[38;5;241m=\u001b[39m split_list(out, [\u001b[38;5;28mlen\u001b[39m(attrs_tracked)])\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:1262\u001b[0m, in \u001b[0;36mscan_bind\u001b[0;34m(*args, **params)\u001b[0m\n\u001b[1;32m   1260\u001b[0m   _scan_typecheck(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39min_atoms, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m   1261\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_jaxpr(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaxpr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mjaxpr)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAxisPrimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:2788\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2784\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2785\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2786\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2787\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:298\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 298\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    301\u001b[0m   maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    302\u001b[0m       executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m    303\u001b[0m       jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:176\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39minit_states, \u001b[38;5;241m*\u001b[39margs_flat]\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:2788\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2784\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2785\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2786\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2787\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1488\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1485\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1486\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xla_extension_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m226\u001b[39m:\n\u001b[0;32m-> 1488\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_argnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_arg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxla_extension_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m229\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_shard_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1492\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_explicit_sharding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m                       tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1496\u001b[0m                       _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1471\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1471\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1477\u001b[0m       compiled, tree_structure(out_flat), args, out_flat, [], jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m   1478\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1479\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/pjit.py:1427\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1421\u001b[0m   distributed_debug_log((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning pjit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md function\u001b[39m\u001b[38;5;124m\"\u001b[39m, name),\n\u001b[1;32m   1422\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_shardings),\n\u001b[1;32m   1423\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_shardings),\n\u001b[1;32m   1424\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract args\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmap\u001b[39m(xla\u001b[38;5;241m.\u001b[39mabstractify, args)),\n\u001b[1;32m   1425\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfingerprint\u001b[39m\u001b[38;5;124m\"\u001b[39m, fingerprint))\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1427\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsafe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, compiled\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFloatingPointError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1429\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_nans\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug_infs\u001b[38;5;241m.\u001b[39mvalue  \u001b[38;5;66;03m# compiled_fun can only raise in this case\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Github/CL_for_faster_Meta-learning/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1213\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1211\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1215\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_spl_curriculum(0,0, 128*100, 300, 100, 128, [0.3,0.7], 128, \"./spl_training_data/\", \"spl_model_0\", 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 7.1476:  10%|█         | 1/10 [00:04<00:44,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3355:  20%|██        | 2/10 [00:09<00:39,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.0826:  30%|███       | 3/10 [00:14<00:34,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.9729:  40%|████      | 4/10 [00:20<00:30,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7804:  50%|█████     | 5/10 [00:25<00:26,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6939:  60%|██████    | 6/10 [00:31<00:21,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5483:  70%|███████   | 7/10 [00:35<00:15,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.4905:  80%|████████  | 8/10 [00:36<00:07,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5315:  90%|█████████ | 9/10 [00:37<00:02,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.3186: 100%|██████████| 10/10 [00:38<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(7.1476481, dtype=float64), Array(2.33553561, dtype=float64), Array(2.35164553, dtype=float64), Array(2.28304682, dtype=float64), Array(2.18469931, dtype=float64), Array(2.08259002, dtype=float64), Array(2.09686869, dtype=float64), Array(2.0965406, dtype=float64), Array(2.06030111, dtype=float64), Array(2.03384646, dtype=float64), Array(1.98928973, dtype=float64), Array(2.01009892, dtype=float64), Array(1.97294493, dtype=float64), Array(1.93222295, dtype=float64), Array(1.95511106, dtype=float64), Array(1.92082819, dtype=float64), Array(1.87994243, dtype=float64), Array(1.97160641, dtype=float64), Array(1.89423986, dtype=float64), Array(1.88922796, dtype=float64), Array(1.78038898, dtype=float64), Array(1.76223514, dtype=float64), Array(1.75034777, dtype=float64), Array(1.7406877, dtype=float64), Array(1.76540039, dtype=float64), Array(1.82032126, dtype=float64), Array(1.72370332, dtype=float64), Array(1.80020471, dtype=float64), Array(1.69390789, dtype=float64), Array(1.72623411, dtype=float64), Array(1.75179461, dtype=float64), Array(1.6200606, dtype=float64), Array(1.68566509, dtype=float64), Array(1.6842936, dtype=float64), Array(1.77545442, dtype=float64), Array(1.64176648, dtype=float64), Array(1.70559326, dtype=float64), Array(1.54833016, dtype=float64), Array(1.73563969, dtype=float64), Array(1.67229073, dtype=float64), Array(1.67198467, dtype=float64), Array(1.59097982, dtype=float64), Array(1.74595717, dtype=float64), Array(1.61302486, dtype=float64), Array(1.70031881, dtype=float64), Array(1.59751633, dtype=float64), Array(1.61919879, dtype=float64), Array(1.49048661, dtype=float64), Array(1.50591696, dtype=float64), Array(1.75732077, dtype=float64), Array(1.65964276, dtype=float64), Array(1.70124198, dtype=float64), Array(1.60438169, dtype=float64), Array(1.55197745, dtype=float64), Array(1.67485363, dtype=float64), Array(1.60775849, dtype=float64), Array(1.56743329, dtype=float64), Array(1.53147323, dtype=float64), Array(1.57408963, dtype=float64), Array(1.55923479, dtype=float64), Array(1.46480728, dtype=float64), Array(1.5656397, dtype=float64), Array(1.63061821, dtype=float64), Array(1.64993646, dtype=float64), Array(1.53245828, dtype=float64), Array(1.55442011, dtype=float64), Array(1.60937642, dtype=float64), Array(1.48813517, dtype=float64), Array(1.31863036, dtype=float64), Array(1.48985407, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 281, 1140,  572, 1169,  134,  137,  972,  305,  149,  184,  425,\n",
      "        728,  772,  584,  671,  309,  685,  558,  326,  770,  418,  940,\n",
      "        252,  820,  850, 1099,  712,  751,  557,  873,  428,   72, 1231,\n",
      "        681,  789,  795,    2, 1233,  124, 1203, 1178,  930,  877,  509,\n",
      "        511,  706,   11,  441,  406,  321,  357,   92,  911,   34,  328,\n",
      "         46,  945,   43,  383,  294,  340,  876,  668,  455,  198, 1006,\n",
      "        941,  868,  931, 1138,   28,  551,  371, 1171,  969,  583,  238,\n",
      "         44,   50,  280,  875,  986, 1227,  248,  366,  206, 1166,  358,\n",
      "        575,  141,  957,  607,  150,   63,  836,   36,  522,  392, 1068,\n",
      "        115,  701, 1062,  250,  355, 1243, 1198,  510,  102, 1260, 1238,\n",
      "       1043, 1228,  320,  414,  791, 1070,  104, 1086,   30, 1029, 1269,\n",
      "        423,  741,  415,  816, 1270,    9,  528], dtype=int64), Array([ 371,  104,  986,  248, 1233,   72,    2,  320,  733,  258,  957,\n",
      "        399,  706,  983,  252, 1108, 1171,  534, 1088, 1181,  285, 1265,\n",
      "         11,  149,  508,  366,  607, 1074,  363,  786,  875,  461,  491,\n",
      "        716,  857,  394,  839,  861, 1043, 1260, 1121,  187, 1029,   56,\n",
      "        751, 1051,  584,  633, 1250,  338,  551,  671,  583,    6,  973,\n",
      "        842,   23,  455,  689,  876,  572,  186,  791, 1151,  417,  580,\n",
      "       1198,  837, 1023,  546,   46,  574, 1195,  549,  354,   65,  198,\n",
      "        763,  476,  206,  136,  952,  835,  283,   79, 1071,  260,  418,\n",
      "       1228,  930,  150,   49,  111,  962,  184, 1223,  383,  923,  714,\n",
      "        355,    8, 1077, 1072,  402,  974,  741,  859, 1018,  557,  511,\n",
      "        423,  329,  464,  785,  984, 1000, 1206,  918, 1053,  703,   91,\n",
      "         48, 1187,  199, 1061,  877,  359,  302, 1022,  564,  870,  441,\n",
      "        344,  369,  501,  284, 1021,  495,  594, 1263, 1210, 1234, 1057,\n",
      "       1203,  217,  840,  236, 1002,  116, 1014,  220,  240,  947,  137,\n",
      "        907,  954,  655,  514, 1045,  309,  178,  712,  977,  575,  105,\n",
      "        686, 1052,  195,  664, 1227,  373, 1066,  940, 1128, 1274,  675,\n",
      "        946,  851,  849,  860,  305,   89,  286,  425,  261,  130, 1271,\n",
      "        668,   36, 1067, 1040,   84,  908,  532,  386,  159,  710,  442,\n",
      "       1004, 1229, 1277,  231,  836,  301,  937, 1168,  224, 1099,  330,\n",
      "        368,  737,  880,  882,  398,  874,  440,  624,  428,  707,  804,\n",
      "        685, 1068,  850,  673,  126,  128, 1231,  602,  124,  787, 1257,\n",
      "       1202,  357, 1169, 1118, 1226,  163,  719, 1097,  663,   37, 1221,\n",
      "        902,  579, 1177,   25,  894, 1028,  478,  935, 1178,   28,  872,\n",
      "        695,  265,  789,  795,  100,  729,  776,  331,  404,  204, 1247,\n",
      "        281,  911,  467,  144,  739,  629,  933,   19,   68,  180,  683,\n",
      "        976,  323,  994,  235,  219,  274,  753,  537,    9,  506,  280,\n",
      "        722,  895,  509,  388,  775,  743,  941,  674,   47,   24,  680,\n",
      "       1064, 1246,  705,  121,  293,  635,  718,  975,  277,   55,  972,\n",
      "       1024,  970,  713,  161, 1182,   43,  697,  306,  479, 1138,  603,\n",
      "       1154, 1173,  405, 1140,   57, 1191,  797, 1069,  666, 1208,  250,\n",
      "        647,  196,  649, 1063,  101,  545,  420,  326, 1180,  400,  834,\n",
      "        135,  980,  819,  244,  312,  630,  803,  345, 1073, 1086,  110,\n",
      "       1003, 1163, 1127, 1212, 1184,  573], dtype=int64), Array([ 607,  716,  248,  186,  741,  116,  557,  511, 1053,  302,  918,\n",
      "        923,  574, 1210, 1067,    8,  149,  952,  383,   65, 1198, 1115,\n",
      "         49,  703,  508, 1250, 1002,  973,  363,  369,  655,  714,  178,\n",
      "       1271,  399,  105,  859,  417, 1187,  689, 1263,   72, 1086,  274,\n",
      "        791, 1040, 1121,   56,  633,  198,  546, 1211, 1018,  786,  712,\n",
      "         25, 1072,  670,  882,  440,  584,  849, 1029,  371,  338,  908,\n",
      "        737, 1260,  231,  283, 1000,   79, 1173,  743,  418,  763,  861,\n",
      "        388, 1068,  842, 1233,  104, 1206,  334,  284,  308,  394,  671,\n",
      "        329, 1043, 1223,  940,  722, 1228,  206, 1052, 1195, 1096, 1108,\n",
      "         23, 1051,  491,  305, 1221,  204,  479,  344,  136, 1097, 1057,\n",
      "       1021,  876,  240,  285, 1088,  795, 1061,  187,  346, 1099, 1231,\n",
      "        729,  603,  296,  685,  751, 1063,  706,  130,  357,   33,  935,\n",
      "        957,  753,  461,  626, 1171,  349, 1184,  355,   11,  978, 1234,\n",
      "         31,  840,   91,  583,  402, 1238,  509, 1227,  837, 1178,  733,\n",
      "        426,  260,  301,  905,  551,  144,  441,  188, 1022,  549,  710,\n",
      "        159,  384,  870,   57,  947,  552,  531,  880,  664, 1074,  515,\n",
      "        883,  851,  219,  834, 1257, 1203,  564,  309,  629,  594,  252,\n",
      "       1266,  313,  630,  972,  971,  580,   74,  241,  673,  423,  856,\n",
      "        455,  470,  983,   37,  902, 1158,  663, 1216,  442,   24,  611,\n",
      "        323, 1160,  835, 1189,   48,  954,  877,   89,   58,  896,  265,\n",
      "        425,  135,  537, 1064,  967,  674,  941,  875,   41,  617,  974,\n",
      "         75,  545,  184, 1176,  739,  679,  984, 1183,  141,  428,  137,\n",
      "        124, 1192,  197,  374,  857, 1138,  192,  911,  320, 1151,  111,\n",
      "        644,  100,  638,  767,  150,  199,  600,  731, 1180, 1001, 1217,\n",
      "         98,  366,  133,  506, 1062,  801,  277,  362,  930,    7,  912,\n",
      "        825, 1168, 1014,  565,   10,  245,  213,  413,  960,  294,  497,\n",
      "       1066,  889, 1128,  450,  375,  326, 1079, 1190,  534,  180,  828,\n",
      "        575, 1264,  533, 1076,  719, 1166,  328,  585,  286,  796,  937,\n",
      "        699,  250, 1261,  878,  787,  799,  258,  788, 1265,  500,   36,\n",
      "       1279,  210,  963,  121,  140, 1100,  412,  339,  695,   34, 1222,\n",
      "       1077,  587,  263,  403,  330, 1194,  933, 1019,  749, 1028,  156,\n",
      "         96,  293,  151, 1254,  525,  747,  359,  214, 1070,  635,  806,\n",
      "        862,  303,  271,  126,  661,  492,  734,  992,  406, 1025,  914,\n",
      "       1032,  884, 1050, 1071,   86,  619, 1272,  161,   66, 1146,  745,\n",
      "       1181,  808,  602,   52,  514, 1202,  651,  289,  965,  839,  848,\n",
      "       1220,  987,  648,  899, 1046,  292,  464,  532,  304, 1130, 1075,\n",
      "        377,    0,  217,  639, 1274, 1049,   43,  732,  142,   51,  207,\n",
      "        327,   62,  538,  608,  173, 1208, 1150,  354,  478,  174, 1207,\n",
      "       1004,  704, 1123, 1118,  169,  113, 1179,  891,  776,  894,  986,\n",
      "        230, 1201,  676,  872,  833,  475,  779,  820,  427, 1030, 1101,\n",
      "        802,  681,  713,   97,  555, 1012,   94, 1085,  314,  264,  904,\n",
      "        312,  641, 1182,  690,  132, 1163, 1137,  386,  297,   71,  886,\n",
      "         61,  183,  755,  809,    3,  228,  798,  850,  962,  613,   22,\n",
      "        804, 1126, 1136,  408,  873, 1015,  975,  697,  147, 1059, 1156,\n",
      "       1142,  797, 1154,   95,  395,  773,  945,  421, 1095,  401,  816,\n",
      "        202, 1199, 1252,  195,  895,  936,  887, 1268, 1169, 1277,  841,\n",
      "        419, 1232,  830,  298,   84, 1193, 1278,  968,  907,  281,  969,\n",
      "        430,  495,  762,  588, 1196,  754,  416,  792,   13,  507,  496,\n",
      "       1117,  483,  391, 1246,  643,   77,  209,  335, 1157, 1215,  606,\n",
      "       1073,  853, 1235, 1035,  387,  503,  453,   90,  123, 1212,   26,\n",
      "       1007, 1253,  698,  393,  683,  780,   39,  114,  805,  193, 1058,\n",
      "        364,  586,  270, 1170,  486,  827,  288,  759,  476,  628,  829,\n",
      "       1045,  148,  976, 1209, 1197,  247, 1275,  373,  215,  266,  916,\n",
      "       1240,  980,  997, 1033, 1205], dtype=int64), Array([1202,  791,  252,   49,  441,   72, 1187,  371,    2,  402,  857,\n",
      "        305,   79,  363,  186,  455,  137,  464,  338,  786,  206,  366,\n",
      "       1000,  689, 1223, 1088, 1051,  575,    8,  633,  258,  546,  837,\n",
      "        557,  706,  184,  248, 1171,  859,  584,  751,  954,  461,  714,\n",
      "       1043, 1228, 1029, 1234,   56,  344,  136, 1018, 1233,  369, 1071,\n",
      "        467,  716,  564,  882, 1121,  532, 1260,  947,  986,   11,  204,\n",
      "        417,  199, 1198, 1206,  973,  508,  983,  185,  877,  309,  104,\n",
      "         65,   48,  285,  386,  198,  149,  583,  952,  354,  580,  655,\n",
      "        607,  302,  930,  870, 1072,  231, 1108,  795,  671,  418,  861,\n",
      "       1074, 1250,  394,  511,  110,  383,  284,  840,   46,  399,  918,\n",
      "       1178,  935, 1023,  664,  697,  478,  111,   23,  329,  851, 1169,\n",
      "        283,  880,  763,  355,  663,  495,  703, 1195,  876,  479,  974,\n",
      "        122,  260,   84,  875,  277,  440, 1210,  105,  957,  839,  413,\n",
      "        941,  685, 1061,   89, 1066, 1138,  911, 1265,  835,   36,  312,\n",
      "        984,  603,   91, 1257,  187,  574,  602,  842, 1067,  978, 1021,\n",
      "        578,  719,  357, 1203,  733,  908,  549, 1115,   25,  552, 1271,\n",
      "        178,  240,  134,  124,  729,  722,  753,  301,  128,  359,  116,\n",
      "        787,  739, 1022,  281, 1196, 1014,  224,  428,  710, 1057,  195,\n",
      "       1277, 1180,  509,  972,  940,   24,  326,  133,  159, 1099, 1128,\n",
      "        594,  491,  713, 1052, 1097,  537, 1235, 1025, 1053, 1028,  587,\n",
      "        614,  130,  962,  849, 1163,  365, 1050,  686,  712,  764,  551,\n",
      "        442,  320,   53, 1073,  741,  423,  197,  845, 1002, 1177, 1040,\n",
      "        630,  514,  899, 1068, 1238,  150,  923,  674,  737,  553, 1154,\n",
      "        458,  624,  217,  425,  834, 1212, 1274,   28,   86,  820, 1231,\n",
      "        946,  745,  476,  668,  373,  860,  219, 1077,  743,  683, 1173,\n",
      "       1263,  349,  779,  736,  323, 1227,  558,  265, 1004,  100, 1175,\n",
      "        173,  789,  905,  179,  647, 1181,  747,   57,  345, 1151,   37,\n",
      "        902,  501,  772,  612,   92,  673,  629,  313,  291,  975,  776,\n",
      "        196,  388,  933,  640,  681,  306,  328,  707, 1140, 1134,  327,\n",
      "       1208,  296,  688,  126,  749,  850, 1082,   47,  181,   77,  380,\n",
      "       1063,  987,  250, 1246, 1211,  311,  384, 1030,  700, 1086,   30,\n",
      "        994, 1199,  421,  499,  161,  135, 1254,  907,    6, 1229,  316,\n",
      "        280,  545,  660,  498,  489, 1011,  275,   34,  483,  535,  752,\n",
      "        572, 1226, 1272,  246,  466,  634,  237, 1279,    0,  836,  232,\n",
      "        294,  855, 1098, 1221,  141,  274,  404,  506,  585,  611,  748,\n",
      "        379,  230,  806,  695,  408,  635,  477,   52,  802,  995, 1033,\n",
      "        170,  672,  406,  270,  809,  271,  503,  894,  628, 1214,  656,\n",
      "       1149,  610,  808, 1215,  534,  372,  374,  214, 1113,  690,  937,\n",
      "        788, 1062,   66,  348, 1166,  830,  831,  797,  331, 1130,  210,\n",
      "        912,  261,  398,  682,  262,   94, 1137,  342, 1158,  308,  203,\n",
      "        945,  213,   63,  120,  426,  450,  106,  785, 1003,  286,  163,\n",
      "       1100,  445, 1159,  675,  427,   96,  330,   75,  669,  236, 1222,\n",
      "        732,  742,  701,  515,  678,  799,  631,  727,  115,  419, 1266,\n",
      "        816,  891,  896, 1218, 1275,  579, 1207,  756,  694,  140,  336,\n",
      "        677,    9,   16,  475,  895, 1008, 1168,   74,  971, 1024,  965,\n",
      "        724,  616,  847,  448, 1019,  900,  435,  778,   43,  268, 1192,\n",
      "        144,  734,  436, 1101,  735,  804, 1217,  626,  220,  387,  885,\n",
      "        961, 1240, 1065, 1118, 1190,  873,  676,  525, 1046, 1042,  223,\n",
      "        530,  539,  523,  188,  174,   95,  518,  922,  256, 1197,  889,\n",
      "        192,  272,  463,  169,  758,  550, 1232,  801,  481, 1035, 1095,\n",
      "        556, 1157,  803, 1132,  447,  340, 1045,  754,  416,  862,  917,\n",
      "        726,  335, 1079,  818,  924,  798,  290, 1143,  244,  949,  193,\n",
      "        303,  661,  318,  129,  932,  177,  639,  864,   55, 1216,  717,\n",
      "        670,  456, 1261,   39,  276, 1081, 1012,   80,  339,  102,  593,\n",
      "        773,  625,  951,  667,  609,  868,   18,  176, 1005,  936,  746,\n",
      "       1027,  469,  617, 1236,  300,  160,  565,  967,  777,   99,  993,\n",
      "        871,  194,  968,  465,  606,  235, 1155,  381,  540, 1239,  521,\n",
      "        618,  942,  598,  702,   67, 1174,  295,  770,  832,   69,  487,\n",
      "       1070,   90,  953,   76,   20,  430,  201, 1006, 1064,   73,  619,\n",
      "        409,  577, 1010,   33, 1039,  107,  229,    5,  520, 1114, 1044,\n",
      "       1103, 1273,   50, 1244,  805,  337,  325,  382,  969,  146,  517,\n",
      "        766,   51,  958,  377,   54,  691,  666,  771,  502,    4,  604,\n",
      "       1224,  212,  662,  699,  680,  708, 1047,  887,  650, 1153,   41,\n",
      "       1076,  636,  744,  825,  249,  542,  155,  297, 1084,  812,  955,\n",
      "         13, 1093,   61,   31,    1,  925,   19,  529,  108,  315,    7,\n",
      "        914,   83,  589,  504,  460, 1204,  621,  172,  627, 1205,  643,\n",
      "        921, 1055,  869,  238, 1165,  705,  360, 1249,  704,  814,    3,\n",
      "        601,  903,  234,  926,  257,  127,  872,  913,  883, 1170, 1139,\n",
      "        109,  322, 1083,  783,  171, 1102,  642,  715, 1142,  856,  118,\n",
      "        720,  142,  595, 1016,  620,  443,  990,  874,  513,  659,  852,\n",
      "        757,   78, 1220,  470,  638,  590,  343,  225,  944,  527,  482,\n",
      "        519, 1090,  282, 1243,  368,  826,  396,  226,   22,  321,  405,\n",
      "        920,  121, 1182, 1156,  310,   85, 1146, 1270,  910,  298, 1241,\n",
      "        901,   17,  811,   98,  496, 1150, 1269,  897,  264,  216,   38,\n",
      "        762,  927,  139,  571,   27], dtype=int64), Array([ 918,  371,  607, ..., 1251,  784,   14], dtype=int64), Array([861, 791, 918, ..., 970, 866, 245], dtype=int64)], 'curricula_losses': [Array([ -5.38987106,  -2.94617798, -18.10935558, ...,  -2.25079339,\n",
      "        -1.69209681,  -2.34305441], dtype=float64), Array([-2.06629521, -2.10011473, -2.9687758 , ..., -2.31396661,\n",
      "       -1.73724974, -2.15671581], dtype=float64), Array([-2.26129482, -1.95812545, -2.19121241, ..., -2.22145158,\n",
      "       -2.21779706, -2.29954356], dtype=float64), Array([-2.03047332, -1.71054489, -3.43493375, ..., -2.35399024,\n",
      "       -1.64849493, -2.03472525], dtype=float64), Array([-1.98982298, -1.90491019, -2.06342793, ..., -2.02266812,\n",
      "       -1.65731696, -1.75585831], dtype=float64), Array([-1.98422424, -1.76240061, -2.36730867, ..., -1.98298545,\n",
      "       -1.52062393, -1.64540718], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 43.9267:  10%|█         | 1/10 [00:05<00:46,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n",
      "1 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3328:  20%|██        | 2/10 [00:10<00:40,  5.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1716:  30%|███       | 3/10 [00:15<00:35,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.0261:  40%|████      | 4/10 [00:20<00:31,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.8847:  50%|█████     | 5/10 [00:26<00:26,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7222:  60%|██████    | 6/10 [00:32<00:21,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5986:  70%|███████   | 7/10 [00:36<00:15,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6276:  80%|████████  | 8/10 [00:37<00:07,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5606:  90%|█████████ | 9/10 [00:38<00:02,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5358: 100%|██████████| 10/10 [00:39<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(43.92671504, dtype=float64), Array(3.21197485, dtype=float64), Array(2.33278906, dtype=float64), Array(2.30164024, dtype=float64), Array(2.25242969, dtype=float64), Array(2.17163677, dtype=float64), Array(2.19064591, dtype=float64), Array(2.10653289, dtype=float64), Array(2.15299826, dtype=float64), Array(2.06303583, dtype=float64), Array(2.02612052, dtype=float64), Array(2.06965327, dtype=float64), Array(2.05155638, dtype=float64), Array(1.92429648, dtype=float64), Array(2.00917265, dtype=float64), Array(1.88470089, dtype=float64), Array(1.92459251, dtype=float64), Array(1.98697541, dtype=float64), Array(1.90410637, dtype=float64), Array(1.95357093, dtype=float64), Array(1.92323463, dtype=float64), Array(1.83792049, dtype=float64), Array(1.87664576, dtype=float64), Array(1.83417827, dtype=float64), Array(1.82337746, dtype=float64), Array(1.83150837, dtype=float64), Array(1.72215115, dtype=float64), Array(1.82796389, dtype=float64), Array(1.81553478, dtype=float64), Array(1.73938135, dtype=float64), Array(1.81679469, dtype=float64), Array(1.78553251, dtype=float64), Array(1.80730219, dtype=float64), Array(1.75225759, dtype=float64), Array(1.70127653, dtype=float64), Array(1.59860924, dtype=float64), Array(1.68554348, dtype=float64), Array(1.75222787, dtype=float64), Array(1.89135645, dtype=float64), Array(1.72505939, dtype=float64), Array(1.71821412, dtype=float64), Array(1.78142873, dtype=float64), Array(1.64924104, dtype=float64), Array(1.62764021, dtype=float64), Array(1.74417552, dtype=float64), Array(1.72942225, dtype=float64), Array(1.65379459, dtype=float64), Array(1.76424813, dtype=float64), Array(1.70222458, dtype=float64), Array(1.72337913, dtype=float64), Array(1.66643199, dtype=float64), Array(1.66760917, dtype=float64), Array(1.72653228, dtype=float64), Array(1.72982824, dtype=float64), Array(1.67138854, dtype=float64), Array(1.67235486, dtype=float64), Array(1.64257353, dtype=float64), Array(1.56059215, dtype=float64), Array(1.64446367, dtype=float64), Array(1.63097598, dtype=float64), Array(1.70658941, dtype=float64), Array(1.53577278, dtype=float64), Array(1.68028235, dtype=float64), Array(1.5564923, dtype=float64), Array(1.67317365, dtype=float64), Array(1.61325945, dtype=float64), Array(1.65740626, dtype=float64), Array(1.66942587, dtype=float64), Array(1.60222527, dtype=float64), Array(1.56375706, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 267, 1017,  171, 1084,  166,  253,  126, 1193,  932,  142,  568,\n",
      "        789,  153, 1211, 1101,  457, 1175,  591, 1159, 1063, 1165,   71,\n",
      "        796,  508,   68,  353,   15,  458,  964,  832,  818, 1058,  628,\n",
      "       1126,  721,  286,  954,  581, 1237,  228, 1076,  355,  488,  678,\n",
      "        377,  530,  765,  513,  860, 1208, 1170,  164,  769,  439, 1259,\n",
      "        503, 1077,  886,  319, 1172,  921,  615, 1160, 1072, 1010,  999,\n",
      "         27,   98, 1120,  981,   83,  620,  388,  339, 1016,  573, 1004,\n",
      "       1029,  472,  916,  398,  894,  558,  109,  723,  991,  870,  170,\n",
      "        372,  441,  603, 1019,  971,  175, 1195,  421,  356,  785,  774,\n",
      "        743,  217,  868,  882,  405,  969,   29,  241,  925,  527,  304,\n",
      "        807,  756,  481,  464, 1081,  344,  154, 1002, 1134,  544,  979,\n",
      "         54, 1168, 1106, 1070, 1118,  594, 1023], dtype=int64), Array([1191,  539,  912,  319,  761, 1126,  732, 1067, 1049,  532,  712,\n",
      "         54,  421, 1195, 1017,  627,  789,  827, 1240, 1145, 1223,  657,\n",
      "        937,  161,  402,  610,  266,  784,  886,  153,  508,  479,  617,\n",
      "         15,  520, 1070,  377,  519,  655,  439,   71,  393, 1118,  396,\n",
      "        636,  232,  416,  173, 1193,   45, 1166,  839,  512, 1208,  162,\n",
      "        739,  812, 1039,  946,  274, 1096,  785,  206, 1159,  123,  126,\n",
      "        221,  289,  625,  442,  953,  171, 1160,  478,  916, 1077,  808,\n",
      "         82,  718, 1155,  268,  756,  553, 1010, 1227, 1206,  699, 1229,\n",
      "       1220,  419,  700,  647,  353,  572,  290,  481,  580, 1127, 1040,\n",
      "        359,   28,   46,  971,  372,  164, 1052, 1116, 1031, 1158,  355,\n",
      "        956,  412,  294,  823,  723, 1134,  457,  176, 1170,  488,  378,\n",
      "       1188,  874,  961,   89,  721,  735, 1244,  367,  944,  573, 1084,\n",
      "        856,  203, 1072,  933,  801,  554,  904,  513, 1029, 1202,  890,\n",
      "        528,  970,  296,  505, 1129, 1251,  510, 1203, 1120,   29,  364,\n",
      "         14, 1152,   31,  743,    0,  131,  155, 1058,  568,  615,  850,\n",
      "        613,  589,   41,  954,  775,  720,  628, 1211,  558, 1013,   66,\n",
      "        666,  800,  503,  485,  847,  157,  753,  533,  320,  741,   86,\n",
      "        240,  701,  249,  805,  300, 1178, 1233,  958,  889, 1046,  401,\n",
      "        121,  356,  267,  607,  930,  619, 1004,   47, 1066,  932, 1137,\n",
      "        223, 1215,  494,  612,   87,  663,  154,  147,  678,  626,  645,\n",
      "        602,  733,  487,  411,  347,  136, 1009,  841,  593,  730,  271,\n",
      "        832, 1106, 1140,  333,  960,  624, 1141, 1028,  540, 1164,  774,\n",
      "        665,  387,  651,  653,  231,  331,  906,  940,   94,  452,  328,\n",
      "       1024,  950,  389,  490,  545,  119,  820,  959,  361,  386, 1005,\n",
      "        116,  722, 1110,  901,  282,  925, 1168,  837,  441,  893,  129,\n",
      "        313,  208,  693,  908,  236,   93,  245, 1022, 1085,  530,   20,\n",
      "        972,  363,   26,  543, 1263, 1083,  175,  193,  905,  822,  265,\n",
      "        252,  371,  861,  835,  109, 1001, 1199, 1257, 1103,  309,   59,\n",
      "        380,  317, 1210,  369,  992,  142,   24,  948,  418,   81, 1064,\n",
      "        769,   18,  748,  632,  591,  708, 1048,  211,  458, 1075, 1165,\n",
      "       1278, 1274,  477,  166,  964,  408,  599, 1186, 1071,  312,  941,\n",
      "        464,  830, 1016,  382,  957,  276,  462,  247,  344,  670, 1138,\n",
      "        286,  684, 1219,  262,  776,  885], dtype=int64), Array([ 147,  267,   86,  961,  396,  789, 1049,  602,  970,   93,   71,\n",
      "        721,  808,  121,  558, 1118,  164,  162,  333,    0,  171,  801,\n",
      "         82,  807,   45, 1031,  540,  412,  791, 1160,  344,  593,  353,\n",
      "        627, 1096,  944,  372,  657, 1068,  136,  478, 1219,  680,   64,\n",
      "        157,  956, 1058,  175,  886,  289,  309,  833,  134,  510,  574,\n",
      "       1052,  732,  520,  545,  847,  653,  361, 1017, 1129, 1274,   94,\n",
      "        479,   54,  628, 1170, 1195, 1084,  203,  735,  610,  619,  194,\n",
      "        591, 1067,  367,  932,  874,  741, 1028,  323,  645,  173,  580,\n",
      "        532, 1168, 1229,  133,  496,  418, 1213, 1178,  207, 1166, 1013,\n",
      "         15,  701, 1029,   98,  441, 1040,  730,  842,  505,  421,  953,\n",
      "        805,  756,  126,  849,  312,  163, 1223,  400,  553, 1070,  775,\n",
      "        901,  531,  765,   46, 1262, 1220, 1244,   14,  172, 1247,  284,\n",
      "        369,  543,   74,  363,  916, 1258,  712,  800, 1015, 1206,  624,\n",
      "        929, 1181,  166,  525,   47,  380, 1236,  521,  108,  547,  462,\n",
      "       1159,   65,  477, 1193,  766, 1126,  154,  784,  900,  140,  946,\n",
      "        387,  841,  416, 1279,  718, 1134,  595,  452, 1140, 1055,   41,\n",
      "       1212,  472,  131,  439, 1191, 1233,  761, 1263,  488,  486,  579,\n",
      "        774, 1137,  743,   28, 1156,  116,  885,   89,  221,  835,  512,\n",
      "         87,  382,   16,  145,  319, 1186,  912,  485,  276,  436,  355,\n",
      "        636, 1046, 1240,  908,  640,  442, 1151,  832,  720,  686,  702,\n",
      "        820,  320,  503,  457, 1016,  129, 1010,   68,  176,  411,  647,\n",
      "        797, 1144,  550,  959,  530,  708,  431,  803,  665, 1110,  750,\n",
      "        812, 1024, 1074,  357,  113,  589, 1152,  655,  300,  855,  311,\n",
      "        199,  717,  515, 1180,  792,  972,  875,  254, 1257, 1020, 1065,\n",
      "       1098,  632, 1203,  948,  443,  282,  663,  308, 1164,   49,  641,\n",
      "        977,  197,  188,  925, 1145, 1066,  272,  268,  895,  568,  689,\n",
      "       1081,  185,  405,  670,   75,  406,  119,  573,  270,  112, 1119,\n",
      "       1173, 1069, 1249,  693, 1138,  742,  315, 1157,  324,  769,  891,\n",
      "        377,  508, 1241,  685, 1083, 1182,  123,  233, 1057,   62,  955,\n",
      "        578,  588, 1215,  408, 1273, 1155,  814, 1050,   55,  752,  981,\n",
      "        313, 1120,  716, 1097,  318,  979,  971,  118,  275,  230,   78,\n",
      "       1139,  930,  903,  626,  837,  734,  159,  358,  738,  630,    4,\n",
      "        933,  402,   56, 1163,  793,  226,  599,  212,  446,  376,  328,\n",
      "        678,  770,  368,  225,  722,  238,  872,  317, 1092,  210,  861,\n",
      "        666,  314,  684, 1216, 1082,  206, 1072,  179,  498,  252,  787,\n",
      "       1060,   29, 1142,  711,  101,  870,  151,  990, 1234, 1077, 1136,\n",
      "        905,  556,   67,  539,  826,  650,  879,   18, 1132,  931,  117,\n",
      "       1075,  301,  918,   66,  945,  869,  795,  757, 1088,  978,   39,\n",
      "        719,  890,  191,  345, 1172, 1005,  733,  214,  338,  965,  850,\n",
      "         80,  165, 1211, 1246, 1269,  354,  753,  364,  940,  283,  904,\n",
      "        115,  852,  697,  919,  389, 1101, 1264, 1209, 1158,  603,  866,\n",
      "        980, 1245,  998,  984,  153,  935,  231,  699,   22, 1102, 1063,\n",
      "         50,  383,  744, 1266, 1184, 1032,  851,  739,  828, 1256,  381,\n",
      "        561,  142,  378, 1122, 1001, 1239,  884,  111, 1036,  295,  960,\n",
      "        892,   33,  102,  995,  293,   37,  150,  815,  596,  664,  132,\n",
      "        455,  316,   99,  549,  356, 1002,  880,  444, 1202,  654, 1162,\n",
      "       1076, 1268,  534,  966,  407, 1030,  569, 1078,  698,  143, 1080,\n",
      "        658, 1116,  839,  964, 1278,   43,  889, 1265,  247,  985, 1023,\n",
      "        999,    5, 1037,  989,  706,  611,  384, 1149,  331, 1042,  517,\n",
      "        336, 1131, 1004,  563,  810,  906,  504,  856,  236,  365,  618,\n",
      "        644,  326,  709,  397,  256,  726,  463,   92,  677,  480,  614,\n",
      "        422,  772,  621,   25,  222,  894,  146,  223,  337,  280,  258,\n",
      "        592,    3,  893,  495,  958,  536,   24,  329,  155,  952,  451,\n",
      "       1048,   36,  934,   57,  997], dtype=int64), Array([1067,  657,  789,  361,  721,  627,  932,   54,  136,  510,  442,\n",
      "       1017,   28,   71,  791,  377,  116,  940,  121,  712,   15,  479,\n",
      "        164,  697,  580,  739,  134,  300,  267,  610,  416, 1170, 1072,\n",
      "        774,  665, 1274, 1223,  735,  775,  418, 1030,  761, 1160, 1220,\n",
      "        203,  367, 1010,  520,  412,  732,   14,  953,  944,  663, 1171,\n",
      "        686, 1040,  961,  956,  171,  353, 1191,  946, 1096,  119,  912,\n",
      "       1219, 1140,  387,  624,  784, 1129,  970,  477, 1118,  558, 1070,\n",
      "        602, 1257,  313,  372,  628,  715,  396, 1024,  568,  129,   46,\n",
      "        220,  655,  439,  743,  841,  756,  718,  572, 1031, 1058, 1120,\n",
      "        147,  294,   45, 1105,  153,  801,  675,  741,   41,  769,  593,\n",
      "        452,  916, 1244,  532, 1206, 1137,   82,   47,  820,  613, 1052,\n",
      "        615, 1009, 1028,  162,  356,  950,  812,  288,  808, 1195,  653,\n",
      "         66,  319,  369, 1029, 1186, 1245,   86,  488,  126,  188,  619,\n",
      "       1155,  832, 1159,  317, 1164,  457,    0,  441,  925, 1013,  553,\n",
      "        805,  589,  355,  933,  573,  176,   87,  175,  173, 1249,  526,\n",
      "        647, 1077,  311, 1046,  835, 1229,  163,  636,  701,  363, 1145,\n",
      "        123, 1126,  890,   93, 1168,  487, 1085,  478, 1166,  512,  505,\n",
      "        917,   62, 1110,  850,  290, 1203,  540,  545,  172,  901,  874,\n",
      "       1158,  435,  364,  766, 1055,  886,   94, 1138, 1020,  724,  708,\n",
      "        421,  411,  206, 1152,  252,  908,  626,  333,  157,  508, 1134,\n",
      "        408,  733,  462, 1239,  800,  599, 1098,  971, 1172,  519,  807,\n",
      "        247,  513,  389, 1263,  320,  752,  543,  650,  905,  645, 1227,\n",
      "        964,  855,   29,  591,  958,  402,  326, 1270, 1236,  230,  472,\n",
      "       1178, 1133,  382,  972,  221,  730,  309,  521, 1063,  539,  223,\n",
      "       1211,  142,  299,  954, 1074,  276,  902,  222,  747,  469,  948,\n",
      "         37,  274,  693,  485, 1049,  861, 1251,  380,  511,  118, 1101,\n",
      "        847,   89, 1084,  344,  109, 1121, 1001,  556,  945, 1141, 1144,\n",
      "        413,  554,  723,  517,  161,  343,  371,   98,  680,  407,  155,\n",
      "       1154, 1161,  906,  640,    1,  991,  231,  669,  386,  937,  257,\n",
      "       1016,  401,  748,  503,  979,  286,  699,  678,  709,  885,  398,\n",
      "        566,  895,  765,  182,  349,  137,  437, 1022,  753,  849,  684,\n",
      "       1180,  445,  354,   78,  154,  376,  232,  815,  331,  446, 1233,\n",
      "       1199,  528,  612, 1076,   26, 1004, 1106,  720,  489,  652, 1113,\n",
      "        632,  185,  391, 1258, 1247, 1027, 1066,  359, 1232,   91,  341,\n",
      "        131,  997,  436, 1261, 1083,  458,  170,  767, 1234,  875,  576,\n",
      "        194,  880,  139,  911,  781,  799,  955, 1175,  383,  959,  673,\n",
      "        330,  424,  915,  744,  255,  120, 1108,  842,  941,  370, 1208,\n",
      "         43,  992,  751,  310, 1033,  179,  502,   52,  759,  393,  226,\n",
      "        308,  777,  981,  304,  105,  641,  698, 1057,  378,  314,  999,\n",
      "         23,   99, 1136,  177,  782, 1279,  592,  598,  198, 1215,  115,\n",
      "        464,  312, 1193,  856,   18,   68, 1188,  658,  324,  689,  268,\n",
      "        995,  486,  110,    9,  289,  982, 1187,  726,   25,  434,  845,\n",
      "        581,  823,  629, 1265,  451,  190,  830,  706,  183,  228,  200,\n",
      "        468,  711, 1131, 1045, 1228,  253,  939,   38, 1071,  879,  551,\n",
      "         31,  474, 1114,   72,  574,  683, 1109,  760,  199,  962,  501,\n",
      "        837, 1218,  969, 1183, 1079,  930,  266,  410,  983, 1278, 1091,\n",
      "        202,  888,  250, 1102, 1238, 1119,  496,  282,  315,  431,   92,\n",
      "        869,  818,  459, 1259,  473,  882,  277, 1272,  579,  802, 1179,\n",
      "       1235, 1212, 1240, 1184,  828,  853,  338,  112,  810,  595,  785,\n",
      "        876,  306,   85, 1262,   20, 1213, 1061, 1273,  368,  422,  235,\n",
      "        809,  827,   42,   81,  323,  225,  166,  607,  197, 1176,   17,\n",
      "       1157,  729,  860,  907, 1276,  374,  509,  682,  755,  562,  770,\n",
      "         83,  564,  773,  727,  934,  634,  482,  664, 1093,  622,  504,\n",
      "        381, 1231,  904,  557,  201,  967,  921,  811,  644, 1075,  516,\n",
      "       1021,  829,  998,  738, 1122,  713,  909,  186,  430,  762, 1068,\n",
      "        365,  210, 1090,  254, 1097,  273, 1169,  456,  490,  656,  691,\n",
      "        994,  666,  530,  822,  455,   90,  414,  318,  260,  746,  740,\n",
      "       1241,  128, 1275, 1268,  379, 1192, 1209,  839,  688, 1181,  169,\n",
      "        481,  968,   64, 1237,  238,  614,  651, 1048,  947, 1202,  631,\n",
      "        604, 1087, 1069,  233, 1103,  525,  865,   84,  722,  405,    5,\n",
      "        248,  866, 1054,  976, 1254, 1089,  763,  132, 1038,  258,  196,\n",
      "        360,  728,  596,  561,  618,  538,  453,  443,   36,  931,  108,\n",
      "        667,  375, 1201,  868, 1037,   21,  211,  385, 1064,  583,  454,\n",
      "        620,  844,   24,  480,  151,  191, 1190,  167, 1032, 1023,  942,\n",
      "        717,  240,   33,  646, 1018, 1086, 1007,  872,  836,  826,  470,\n",
      "        425,  216, 1082,  328, 1255,  867,  878,  929,  271,  102,  633,\n",
      "        778,  565,  754,  256,  332,  776, 1099,  548,   57,  952, 1156,\n",
      "        529,   80,  690,  460, 1111,  321, 1051,  347,  205,  394,  494,\n",
      "        679,   34,  339,  388,  399,  208,  603,   59,  249,  662,  550,\n",
      "        218,  302, 1019,  787,  996,  771,  146,  783,  101,   12,  889,\n",
      "        278,  325,  552,  758,  296,  149,   44,  873, 1092,  846,  160,\n",
      "       1146, 1100,   95,  499,  703,  515,  894,  207,   97, 1002,  350,\n",
      "        957,  871,  292,  913,  215,    8,   11,  219,  103, 1044,  674,\n",
      "        725,  825,   58,  833,  518,  676, 1150, 1230,  834,  295,  986,\n",
      "       1177,  272,  261,  409, 1015], dtype=int64), Array([  86,   71,  164, ..., 1208,  292,  480], dtype=int64), Array([ 45, 267,  82, ..., 780, 975, 635], dtype=int64)], 'curricula_losses': [Array([-37.40061939,  -2.92665984,  -0.89133009, ...,  -1.30468274,\n",
      "       -12.48612879, -10.24925355], dtype=float64), Array([-2.94881493, -2.18823183, -1.55630893, ..., -1.37525962,\n",
      "       -2.32244323, -2.05849835], dtype=float64), Array([-2.62146718, -2.05097176, -2.14788041, ..., -2.09081444,\n",
      "       -2.29472158, -2.45248489], dtype=float64), Array([-2.49859862, -2.17363506, -1.62707287, ..., -1.69864841,\n",
      "       -1.93744972, -2.00188723], dtype=float64), Array([-2.64399209, -1.51627362, -1.68336931, ..., -1.58331358,\n",
      "       -1.86962493, -1.92369859], dtype=float64), Array([-2.59075521, -1.68971815, -1.76594305, ..., -1.70587946,\n",
      "       -1.78319298, -2.00140039], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 128 curr_data_rate 0.1 epoch number 0\n",
      "sorted indices shape (128,)\n",
      "batches_until_eval 128 batches_until_end 400 len(batches) 1 training_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 4.4651:  10%|█         | 1/10 [00:05<00:45,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "curr_data_size 358 curr_data_rate 0.28 epoch number 1\n",
      "sorted indices shape (358,)\n",
      "batches_until_eval 127 batches_until_end 399 len(batches) 2 training_steps 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.3532:  20%|██        | 2/10 [00:10<00:40,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "curr_data_size 588 curr_data_rate 0.45999999999999996 epoch number 2\n",
      "sorted indices shape (588,)\n",
      "batches_until_eval 125 batches_until_end 397 len(batches) 4 training_steps 3\n",
      "3 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 2.1346:  30%|███       | 3/10 [00:15<00:36,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 819 curr_data_rate 0.64 epoch number 3\n",
      "sorted indices shape (819,)\n",
      "batches_until_eval 121 batches_until_end 393 len(batches) 6 training_steps 7\n",
      "7 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.9678:  40%|████      | 4/10 [00:20<00:31,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1049 curr_data_rate 0.82 epoch number 4\n",
      "sorted indices shape (1049,)\n",
      "batches_until_eval 115 batches_until_end 387 len(batches) 8 training_steps 13\n",
      "13 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.8447:  50%|█████     | 5/10 [00:26<00:27,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1279 curr_data_rate 0.9999999999999999 epoch number 5\n",
      "sorted indices shape (1279,)\n",
      "batches_until_eval 107 batches_until_end 379 len(batches) 9 training_steps 21\n",
      "21 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.7455:  60%|██████    | 6/10 [00:32<00:22,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 6\n",
      "batches_until_eval 98 batches_until_end 370 len(batches) 10 training_steps 30\n",
      "30 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.6794:  70%|███████   | 7/10 [00:37<00:15,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 7\n",
      "batches_until_eval 88 batches_until_end 360 len(batches) 10 training_steps 40\n",
      "40 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5375:  80%|████████  | 8/10 [00:37<00:07,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 8\n",
      "batches_until_eval 78 batches_until_end 350 len(batches) 10 training_steps 50\n",
      "50 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5754:  90%|█████████ | 9/10 [00:38<00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_data_size 1280 curr_data_rate 1.0 epoch number 9\n",
      "batches_until_eval 68 batches_until_end 340 len(batches) 10 training_steps 60\n",
      "60 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.5106: 100%|██████████| 10/10 [00:39<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing losses length 70\n",
      "{'training_loss': [Array(4.46511755, dtype=float64), Array(2.35323934, dtype=float64), Array(2.40269622, dtype=float64), Array(2.31726694, dtype=float64), Array(2.21802477, dtype=float64), Array(2.16513881, dtype=float64), Array(2.13459241, dtype=float64), Array(2.14783876, dtype=float64), Array(2.05731897, dtype=float64), Array(2.00028847, dtype=float64), Array(2.02081199, dtype=float64), Array(1.96776432, dtype=float64), Array(2.07603858, dtype=float64), Array(1.90722066, dtype=float64), Array(1.95717144, dtype=float64), Array(1.87353252, dtype=float64), Array(1.941979, dtype=float64), Array(1.91395398, dtype=float64), Array(1.94152502, dtype=float64), Array(1.84470925, dtype=float64), Array(1.9234253, dtype=float64), Array(1.76327231, dtype=float64), Array(1.7966361, dtype=float64), Array(1.78896645, dtype=float64), Array(1.74546087, dtype=float64), Array(1.80271335, dtype=float64), Array(1.86352143, dtype=float64), Array(1.74706682, dtype=float64), Array(1.84207273, dtype=float64), Array(1.82598339, dtype=float64), Array(1.77005728, dtype=float64), Array(1.74014962, dtype=float64), Array(1.76617806, dtype=float64), Array(1.71028554, dtype=float64), Array(1.82860862, dtype=float64), Array(1.73289513, dtype=float64), Array(1.75570773, dtype=float64), Array(1.69902528, dtype=float64), Array(1.78327781, dtype=float64), Array(1.67942963, dtype=float64), Array(1.70372314, dtype=float64), Array(1.69169727, dtype=float64), Array(1.78437741, dtype=float64), Array(1.5374641, dtype=float64), Array(1.64172511, dtype=float64), Array(1.77267588, dtype=float64), Array(1.74808538, dtype=float64), Array(1.79199812, dtype=float64), Array(1.67417201, dtype=float64), Array(1.6765417, dtype=float64), Array(1.63366693, dtype=float64), Array(1.77044699, dtype=float64), Array(1.68898472, dtype=float64), Array(1.71758987, dtype=float64), Array(1.64982424, dtype=float64), Array(1.67053032, dtype=float64), Array(1.63428589, dtype=float64), Array(1.62304677, dtype=float64), Array(1.57539782, dtype=float64), Array(1.63821607, dtype=float64), Array(1.62370455, dtype=float64), Array(1.54607349, dtype=float64), Array(1.6417183, dtype=float64), Array(1.732131, dtype=float64), Array(1.56455516, dtype=float64), Array(1.63940903, dtype=float64), Array(1.51060741, dtype=float64), Array(1.52498508, dtype=float64), Array(1.56373944, dtype=float64), Array(1.51990312, dtype=float64)], 'training_intask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}, 'training_outtask_errors': {'ece': [], 'rmse': [], 'std_residuals': []}}\n",
      "{'curricula_weights': [Array([ 360,  930,  476,  214, 1072,  471,  421,  682,  909,  904, 1044,\n",
      "        906,  517,  571, 1078,  972,  337,  248,  691, 1030, 1254,  869,\n",
      "        895, 1069,  181,  960, 1133,  433, 1244,  342,  945,  645,  969,\n",
      "          7,    9,  271,   16,  568,   25,   68,  387,  314,  250,  554,\n",
      "        699,  178,  278,  100,  832,  208,  634,  370,  117,  715,  781,\n",
      "        301,   14, 1214, 1197,  870,  377, 1238,  707,  595,   48, 1109,\n",
      "        221,  412, 1123,  526,  287,  641,  657,  828,  455, 1056,    6,\n",
      "       1263,  642,  467, 1246, 1166,  971, 1047,  262,  582,  965,  788,\n",
      "       1066, 1235,  297,  770,  995,  528,  766,  474,  591,  104, 1224,\n",
      "         43,  631,  845,   66,  496,  776,  661, 1125,  675, 1231,  349,\n",
      "        920, 1081,  274, 1054,  202,  923, 1031, 1236,  432,  551,  462,\n",
      "       1046,  706,  905, 1130, 1026,  985,  872], dtype=int64), Array([ 337,  798,  997,  148,  145,  870, 1010,  241,  788, 1235,  643,\n",
      "        691,  276,  969,  949,  599,  469,  620,  595,    9,  220, 1236,\n",
      "        214,  593,  551, 1169,  202,  127,  906,  432,  930,  184, 1247,\n",
      "        151,  711,  801,  781, 1031, 1097, 1083,  759,  909, 1063,  874,\n",
      "        867,  295,  107,  226,  588,  473, 1167,  780,  330, 1227,  985,\n",
      "        960, 1027,   41,  567,  293,  919,  786, 1123,  648, 1204,  187,\n",
      "       1125,  444,  549,  872,  840,  153,   70, 1210,  892,  890,  436,\n",
      "       1101,  341,  699,  230,   22,  318,  467,    6, 1037,  724, 1254,\n",
      "        257, 1163,  474,  486,  407,  629,  703,  325,  959,  302,  287,\n",
      "        208,  568, 1028,  807, 1065,  698,  433,  816,   50,  815,   79,\n",
      "        805, 1136,   90,  972,  944,  398, 1216,  357,   37, 1076,   20,\n",
      "        886,  479, 1104,   18,  303,   74,  594, 1145, 1212,  272,  260,\n",
      "        752,  742,  882,  630, 1113,  776,  360,  645,  476,  369,   36,\n",
      "        212, 1252, 1257,  814,  117,  345,  687,   68,  146,  445,  304,\n",
      "        262,  657,  359,  684,  156,  564,  869,  723,  583, 1171,  982,\n",
      "        762,  350,  390, 1217,  435,   47, 1001,  419,  941, 1030,  168,\n",
      "        573,  246,  406,   83,  278, 1108,  525,   67,   52,  288,  989,\n",
      "        209, 1003, 1089,  305,  205, 1270,  797,   27,  897,  722,  104,\n",
      "        387, 1118, 1268,  774,  683,   25,  625,  527,  832,  682,  346,\n",
      "          7,  900,   17,  611,  417,  685,  884,  923,  820,  324,  408,\n",
      "        697,  965, 1112,  377,  123,  641,  457,  296, 1116, 1060,  622,\n",
      "       1018,  895,  964,  744, 1056,  191, 1240,  499,  396,  663,  192,\n",
      "       1193, 1190, 1034,  496,  934,  122,  158,  120,  796, 1046,  619,\n",
      "        716,  233, 1054,  557,  131,  800, 1096,  232,  773, 1156,  689,\n",
      "        466, 1242, 1263,  500,  947,  656,  228, 1269, 1093,   53,   13,\n",
      "        945,  364, 1244,  463,  309, 1234, 1007,  811,  661, 1041,  108,\n",
      "       1175,  649,   98, 1177, 1150,  586,  414,  354,  600,  662,  173,\n",
      "       1139,   95,  898, 1129,  929, 1275,  904,  592,  916, 1130,  130,\n",
      "        179, 1079,   16,  763,  253,  558,  135,  860,  349,  741,  421,\n",
      "        881, 1055,  464,  667,  631,  426,  693, 1021,  427,  851,  298,\n",
      "        194,  165, 1127,  822,    5,  511,  301,  715,  142, 1205,  116,\n",
      "       1026,  332,  799,  747,  617,   75,  880, 1103, 1023,  920, 1151,\n",
      "        119,   93,   21,  806,  367,  492], dtype=int64), Array([ 309,  697,  444,  341,  869,  762,  151,  258, 1001, 1123,  295,\n",
      "       1104, 1242,  390,  591,  797,  287,  112, 1235, 1247, 1216,  568,\n",
      "        302,  629, 1167,  203,  959, 1163,  595,  436,  413, 1169, 1230,\n",
      "       1161,  527,  801,  788,  689,  526,  960,  780,  622, 1254,  969,\n",
      "        998,  406, 1246,  874,  832,  820,  313,  815,  685,  987,  122,\n",
      "        657,  407,  230, 1193, 1063,  435,  257,  599,  723, 1046,  949,\n",
      "        135,  906, 1236,  724,  393,   47, 1076,  699,  682, 1028, 1027,\n",
      "        909,  153, 1151,  912,  583,  130,  426,  214, 1204,  107,  565,\n",
      "        643,  219,    5,  474,  990,  432,   18,  113,  228,  799,  934,\n",
      "        781, 1145, 1118, 1054,  433,   50,  232,  593,    9, 1108,  942,\n",
      "        649,  927,  330,  202,  840,  704, 1056,  394,  369,  467,  530,\n",
      "        715, 1041, 1082,  645,   22,  293,  992,  220,  318,  241,  500,\n",
      "        127,  355,  476,  278,  982,  895,  944,  276,  104,  360,  876,\n",
      "       1122,  746,  499,   90,  573,  647, 1136,  146,  798,  776,  923,\n",
      "        920, 1037,  698,  759,  867,  479,  881,  615,  691,  772, 1197,\n",
      "         66,  557,  995,  703,  145,  605,  488, 1074, 1050,  345,  886,\n",
      "        684, 1034,  191,  337, 1042,  548,  376,  812, 1274,  671,  167,\n",
      "        828,  260,  425,   67,  152,  919,  900,   70,  965,  398,  997,\n",
      "        617,  594,  304, 1055, 1269,  320, 1097, 1068, 1200,   27, 1244,\n",
      "       1053, 1040,  567,  870,  957,  899, 1010,  274,  387, 1217,  518,\n",
      "        814,  400,  138,  245,  979, 1186,   13,  506, 1051,  688, 1100,\n",
      "       1205,  212,  147,  872,   20,  742,  271,  687, 1124, 1239,  121,\n",
      "        585,   69,  752,  155,  701, 1020,  296,  620, 1083,    7, 1093,\n",
      "        922,  396,  364, 1212, 1030, 1270,  408, 1164,  226, 1187,  644,\n",
      "       1017,  562,  589,   36,  269,  961,  457,  486, 1255,  705,  989,\n",
      "        150,  839,  835,  972,  734, 1133,  472,  714,  243,  816,   91,\n",
      "        833,  897,  370,  603,  100,  367,  963,  312,  663,  523,  445,\n",
      "        128,  529, 1227,   95,  213, 1177,  358,  747, 1276, 1015,  709,\n",
      "       1125,  148,    8,  117,  281,  473,  108, 1018,  826,  623,  855,\n",
      "        659,  786,  868, 1179,  827,  328, 1078,  917,  572, 1279,   16,\n",
      "       1245,  507,  785,  850,  301,  964,  847,  321,  351,  882,  421,\n",
      "        732, 1262,  890,  496, 1073,  638, 1107,  297,  652,  329,   71,\n",
      "        277,  910,  222,  165,  905,   98,  149,  439,  385,  656,  534,\n",
      "        731, 1023,  325, 1150, 1094,  666, 1175,  736, 1257, 1098, 1231,\n",
      "       1188,  335,  755,  976,   58, 1229,  378, 1180,  229,  765, 1119,\n",
      "        447,  111,  930,  323,  550,  434,  464,  884,  947,  700,  227,\n",
      "       1268,  817,   35,  403, 1263,  208,  551,  487,  361, 1117,  750,\n",
      "       1143,  625,  428,   26,  845,  611, 1026,  233, 1000,  382,  170,\n",
      "       1084,  125,  985, 1178,  764,  996,  175,  187,  779,  249,  377,\n",
      "        637,  120, 1113, 1233,  524,  528,  597,  372,   43, 1206,  856,\n",
      "        774, 1005,  409,  504, 1088, 1162,  159,   93, 1240,   65,  853,\n",
      "         56,  630,  836,  171,   14, 1148,  244,  646,  541,  896,    1,\n",
      "        456, 1033, 1003,  854, 1116,  825,  123,  185, 1214,  770,   68,\n",
      "        726,  744,  892,  999,  414,  531, 1012, 1060, 1183,  604,  412,\n",
      "        458,  748,  156,  454,  136, 1080,  498,  824,  391,  514,  674,\n",
      "        275,  875,  560,  184,  516, 1256,  368,   28,  725, 1172,    0,\n",
      "       1134,  564, 1071,  353,  790,  686, 1221,  290,  422,  766, 1049,\n",
      "       1219,  621,  137, 1131,  946,  168,  419,  431, 1127,  157, 1232,\n",
      "        810,  577,  664,  784,  962,  239, 1139,  846,  520,   12,  861,\n",
      "        103,  679,  702, 1031,  384,  880,  455,  753,  740,  859,  237,\n",
      "        968,   37, 1156, 1032, 1095,  322, 1241, 1210,  450, 1128,   54,\n",
      "        907,  904, 1160,  265,  238,  665,  945,  903,  537,  851,  178,\n",
      "         11,  371,   40, 1189, 1115,  570,  522,  209,  180,  741,  471,\n",
      "       1014,  951,  718,  315, 1106], dtype=int64), Array([ 295, 1247,  433,  145,  643,  759, 1076,  257,  341,  909,  241,\n",
      "        107,  780,  593,  969,  781,  906, 1236, 1010,   22,  337,  445,\n",
      "        620,  982,  798, 1235,  997,  815,  595,  474,  469,  104,  330,\n",
      "       1218,  276, 1145,  153, 1214,  788,  473,  727,  226,  432,  287,\n",
      "        870,  302,  699, 1116,  117, 1204, 1169,   36, 1254,  886,  949,\n",
      "         18, 1123, 1139,  657,  801, 1023,  682,  989,  202,  127,  228,\n",
      "         47,  408,  960, 1130, 1097,  867,  258,  985,  360,  797, 1063,\n",
      "        724,  421,  220,  151,  467,  364, 1163,  902,   50,  972,  444,\n",
      "        944,  715,  148, 1037,  959,  293,  890,  872,  227, 1227,  165,\n",
      "        703,  387,  278,  684,  455,  419,  500,  752,  296,  191,  146,\n",
      "       1136,  551, 1108,  622,  527, 1031,  594,  349,  476,  568,  912,\n",
      "        965,  499,  309,  742,  645, 1027, 1216,  406, 1133, 1056, 1003,\n",
      "        762, 1212, 1001,   67,  230,  301,  318, 1229,  271, 1193,  398,\n",
      "        414, 1046,  687,  874,  599,  567,  108, 1205, 1270,  685, 1028,\n",
      "        200,  816,  370,  412,  250,  923,  260,  776, 1242,  208,   27,\n",
      "        212, 1239,  390,  479,  304,  941, 1234, 1167,  723,  209,  113,\n",
      "       1118,  222, 1104,  919,  814,  121,  998, 1210,  369,  897,  629,\n",
      "       1018, 1078,  214,  934,  486, 1125,    9, 1177,  869,  156, 1054,\n",
      "        591,   95,   98,  714,   90,  407,  892,  820, 1041,  435,  573,\n",
      "        528,  436,  120, 1004,  698,  786, 1240,  630,  856,  203,  649,\n",
      "        691,  829,  900,  554, 1230,   13,  964,  580, 1268,  112,   16,\n",
      "        377,  895, 1083, 1093,  920, 1150, 1026, 1219, 1175, 1231,  822,\n",
      "       1135,  747,  233,  851,  122,  496,  946, 1113,  562,  881,  689,\n",
      "        325, 1208,  799, 1103,  193, 1055,  832,  350,  376, 1120,  393,\n",
      "        187,  396,  722, 1179,  625, 1256,  100,  746, 1159,  583,  184,\n",
      "        663, 1171,  346,  560,  378, 1034, 1158,  135,  438,   68,  232,\n",
      "       1269,  549,   99,    7,    5,  426,  908,  774,   66, 1206,  876,\n",
      "        281,  954, 1246,  347,  428,  777, 1112,  863,  483, 1263,  329,\n",
      "        526, 1040,  320,  939, 1151, 1181,   12,  471, 1100, 1149,  644,\n",
      "        884, 1244,  837,  844,  189,  425,  158,  662, 1186, 1176,  688,\n",
      "        119,  537,  219, 1220,   70, 1065,  297,  812,  245,  123,  564,\n",
      "        277,  168,  345,  280, 1080,  611,  693,  882,  904,  927,  129,\n",
      "        557,  733,  805,  597, 1257,  990,  534,  178,   75,   20,  385,\n",
      "        542,  231,   74, 1042,  516, 1160,  116,  738, 1109,  670,   55,\n",
      "        565, 1153,  773,  898,  335,  348,  751,  835,  655,  648,   35,\n",
      "        394,  809, 1156,   91,  843,   56,  477,  603,  180,  634,  732,\n",
      "        221, 1099,  356,  635, 1024,   10,  481,   71,  289,  144,  559,\n",
      "        569, 1067,  422, 1189,  833,  606,    6, 1261,  945, 1021,  642,\n",
      "        125,  205,  987, 1253,   40,  621,   43,  163,  143, 1190,  767,\n",
      "        930,  973,  905, 1221,  525,  605,  507,  979,  638, 1082,  961,\n",
      "        873, 1170, 1271,  667,  558,  188, 1030, 1011,  817,  459, 1073,\n",
      "       1114,  929,  859, 1092,  901,   77,  958,  705,  285,  578,  743,\n",
      "       1089,  588, 1215, 1005, 1265,  704,  411,  899,  571,  953,  681,\n",
      "        482,  447,   42,  453,  288,  248,  211,   41,  888,  637,   65,\n",
      "       1197,  466, 1057,  917,  828, 1209,  947,  449,  840,  702,  650,\n",
      "        719,  658,  457,   37, 1036,  314, 1059,  586,  862,  748,  988,\n",
      "       1085,  986,  716,  371,  619,   32,  359,  661,   94,  303,  340,\n",
      "       1223,  492,  760,  726, 1207, 1000, 1033,  536, 1068, 1202,  272,\n",
      "        665,  889,  282,  626, 1172,  374,   64,   85,  155, 1191,  744,\n",
      "        338,  137,  115,  779,  866,   59, 1147,  697,  454,  618,    4,\n",
      "         28,  488,  940, 1101, 1278,  491,   17,  522,  737,   96, 1262,\n",
      "        441,  264,  755,  761, 1044,   78, 1060,  307, 1225, 1273, 1079,\n",
      "        256,  170, 1194,  484, 1052,  887,  957,  215,  270,    0,  653,\n",
      "        300, 1233,  903,  853, 1152,  434, 1258,  460,  530,  322, 1201,\n",
      "        950,  602,  880, 1045,  172,  632,  323,   83,  353,  778,   23,\n",
      "        463,  413,  487,  807,  970,  765,  933,  545,  532,  179,  283,\n",
      "        391,  182,  324,  656,  141, 1200,  896,  770,  363,  427, 1274,\n",
      "       1084,  478,  794, 1199,  430,  252,  458,  382,  804, 1025, 1121,\n",
      "        541,  204,  368,  480,  130,  305,   60,  772,  574,   72,  647,\n",
      "        729,  971,  821, 1072,  313,  400,  885,  358,  713,   48,  962,\n",
      "        596,  456,  609,  126, 1074,  262,  641,  362,  617,  319, 1053,\n",
      "       1138,  827,  627,  676, 1164,  511,  624,  440,  251, 1090,   30,\n",
      "        279,  996,  916,   61, 1144, 1006, 1185, 1276,  803,  218,  935,\n",
      "        575,  417, 1115,  639, 1029,  246,  810,  811,  229,  717,  552,\n",
      "       1266,  354,  224, 1088, 1224,   49, 1095,  269,  943,  800,  736,\n",
      "       1129, 1259, 1035, 1155,  683,  718,  711,  501,   73,  399,  589,\n",
      "        263,  190, 1071, 1165, 1022,  384,   97,  367,  695,  666,  462,\n",
      "        992,  795,  409,  498,  690, 1117,  339,  468, 1255,  613,  538,\n",
      "        610,   24,  631, 1134, 1107,  198,  802,  766,  166,  207,  529,\n",
      "         53,  980,  331,  177, 1132, 1217, 1222,  533,  365,  981,  855,\n",
      "        849,  686,  298, 1110,  921,  550,  615, 1275, 1064,  539,   93,\n",
      "       1174,   82,  310,  110,  819,  918, 1249,   87,  332,  956,  101,\n",
      "        771,  793,  175, 1245,  254,  764,  381,  274,  848,  343,  150,\n",
      "        679,  785,  416,  894,  672,  355,   79,  993,   57,  750,  601,\n",
      "        984,  171,   84,  506,  273], dtype=int64), Array([ 444,  960,  341, ...,   33,  109, 1061], dtype=int64), Array([ 337, 1123,  969, ..., 1207,   57,  489], dtype=int64)], 'curricula_losses': [Array([-1.9495023 , -2.43169648, -2.7977408 , ..., -1.42424316,\n",
      "       -2.00237235, -1.45337154], dtype=float64), Array([-2.09889389, -2.1527325 , -2.11361986, ..., -2.08164026,\n",
      "       -2.08426601, -2.0500699 ], dtype=float64), Array([-2.24876905, -2.26007548, -2.03695979, ..., -2.0743005 ,\n",
      "       -2.14389741, -2.31550471], dtype=float64), Array([-1.89240382, -1.50819496, -1.58538427, ..., -1.53997115,\n",
      "       -1.90401503, -1.6836753 ], dtype=float64), Array([-1.87567212, -1.43794584, -1.70334578, ..., -1.64168257,\n",
      "       -1.78249555, -1.7425806 ], dtype=float64), Array([-1.73109517, -1.50517485, -1.49763419, ..., -1.43432821,\n",
      "       -1.6623814 , -1.63020714], dtype=float64)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try training it severeal times in a for loop to see if we get oom errors or not. \n",
    "for i in range(3):\n",
    "    train_spl_curriculum(i,i, 128*10, 400,100, 128,  [0.3,0.7], 128, \"./spl_training_data\"+str(i)+\"/\", \"spl_model_\"+str(i), 0.1, 5)\n",
    "\n",
    "    #Lets load in the stuff for this run so we can see things are saved well. \n",
    "    with open(os.path.join(\"./spl_training_data\"+str(i)+\"/\", \"spl_model_\"+str(i) + '_training_metrics.pkl'), 'rb') as f:\n",
    "        training_metrics = pickle.load(f)\n",
    "        print(training_metrics)\n",
    "    with open(os.path.join(\"./spl_training_data\"+str(i)+\"/\", \"spl_model_\"+str(i) + '_curricula_logs.pkl'), 'rb') as f:\n",
    "        curricula_logs = pickle.load(f)\n",
    "        print(curricula_logs)\n",
    "    # load in params as well\n",
    "    params = load_model_params(\"./spl_training_data\"+str(i)+\"/spl_model_\"+str(i)+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. :   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 3000 len(batches) 100 training_steps 0\n",
      "0 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.2852:  10%|█         | 1/10 [00:14<02:07, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2900 len(batches) 100 training_steps 100\n",
      "100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.0813:  20%|██        | 2/10 [00:24<01:34, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2800 len(batches) 100 training_steps 200\n",
      "200 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.0792:  30%|███       | 3/10 [00:34<01:17, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2700 len(batches) 100 training_steps 300\n",
      "300 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 1.0698:  40%|████      | 4/10 [00:44<01:04, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2600 len(batches) 100 training_steps 400\n",
      "400 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.9680:  50%|█████     | 5/10 [00:54<00:51, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2500 len(batches) 100 training_steps 500\n",
      "500 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.7255:  60%|██████    | 6/10 [01:03<00:40, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2400 len(batches) 100 training_steps 600\n",
      "600 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.5070:  70%|███████   | 7/10 [01:13<00:29,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2300 len(batches) 100 training_steps 700\n",
      "700 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.2609:  80%|████████  | 8/10 [01:22<00:19,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2200 len(batches) 100 training_steps 800\n",
      "800 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.1924:  90%|█████████ | 9/10 [01:32<00:09,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches_until_eval 100 batches_until_end 2100 len(batches) 100 training_steps 900\n",
      "900 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.1430: 100%|██████████| 10/10 [01:42<00:00, 10.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_np_baseline(0,0, 128*100, 3000, 100, 128, [0.3,0.7], 128, \"./spl_training_data/\", \"spl_model_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['curricula_weights', 'curricula_losses'])\n",
      "[(1280,), (3584,), (5888,), (8192,), (10496,), (12799,)]\n",
      "------\n",
      "[12800, 12800, 12800, 12800, 12800, 12800]\n",
      "320\n",
      "[Array(2.3447213, dtype=float64), Array(2.31487937, dtype=float64), Array(2.30283761, dtype=float64), Array(2.24685976, dtype=float64), Array(2.19020954, dtype=float64), Array(2.14441941, dtype=float64), Array(2.10791754, dtype=float64), Array(2.15310162, dtype=float64), Array(2.1535682, dtype=float64), Array(2.28266524, dtype=float64), Array(2.27615545, dtype=float64), Array(2.29696677, dtype=float64), Array(2.2665223, dtype=float64), Array(2.29267859, dtype=float64), Array(2.24308459, dtype=float64), Array(2.27502594, dtype=float64), Array(2.25849615, dtype=float64), Array(2.23216931, dtype=float64), Array(2.21687149, dtype=float64), Array(2.23931491, dtype=float64), Array(2.28659302, dtype=float64), Array(2.22571072, dtype=float64), Array(2.26891836, dtype=float64), Array(2.21893093, dtype=float64), Array(2.23590097, dtype=float64), Array(2.24298059, dtype=float64), Array(2.2580844, dtype=float64), Array(2.20351747, dtype=float64), Array(2.23240756, dtype=float64), Array(2.21057683, dtype=float64), Array(2.22225358, dtype=float64), Array(2.220409, dtype=float64), Array(2.2342186, dtype=float64), Array(2.25152712, dtype=float64), Array(2.24077456, dtype=float64), Array(2.20816479, dtype=float64), Array(2.23994791, dtype=float64), Array(2.14404733, dtype=float64), Array(2.11668064, dtype=float64), Array(2.05589935, dtype=float64), Array(2.08293365, dtype=float64), Array(2.10290842, dtype=float64), Array(2.10574802, dtype=float64), Array(2.13917174, dtype=float64), Array(2.08678171, dtype=float64), Array(2.09644784, dtype=float64), Array(2.08684562, dtype=float64), Array(2.09909574, dtype=float64), Array(2.09648749, dtype=float64), Array(2.1107797, dtype=float64), Array(2.1333755, dtype=float64), Array(2.08558951, dtype=float64), Array(2.05856845, dtype=float64), Array(2.06061979, dtype=float64), Array(2.08978188, dtype=float64), Array(2.06411957, dtype=float64), Array(2.07784182, dtype=float64), Array(2.08928628, dtype=float64), Array(2.07731518, dtype=float64), Array(2.03941346, dtype=float64), Array(2.08840164, dtype=float64), Array(2.0832694, dtype=float64), Array(2.08970274, dtype=float64), Array(2.05661375, dtype=float64), Array(2.08388661, dtype=float64), Array(2.0840871, dtype=float64), Array(2.09591914, dtype=float64), Array(2.0760566, dtype=float64), Array(2.05530133, dtype=float64), Array(2.04450466, dtype=float64), Array(2.08622158, dtype=float64), Array(2.11758886, dtype=float64), Array(2.07290651, dtype=float64), Array(2.07707678, dtype=float64), Array(2.08665543, dtype=float64), Array(2.06464319, dtype=float64), Array(1.99132189, dtype=float64), Array(2.06265709, dtype=float64), Array(2.04677592, dtype=float64), Array(2.04730385, dtype=float64), Array(2.02291671, dtype=float64), Array(2.03042181, dtype=float64), Array(2.06926161, dtype=float64), Array(1.9437819, dtype=float64), Array(1.93333045, dtype=float64), Array(1.98476673, dtype=float64), Array(1.97244661, dtype=float64), Array(1.98409297, dtype=float64), Array(1.92840584, dtype=float64), Array(1.89312762, dtype=float64), Array(1.93338654, dtype=float64), Array(1.93738058, dtype=float64), Array(1.96894885, dtype=float64), Array(1.98847421, dtype=float64), Array(1.94958598, dtype=float64), Array(1.93179911, dtype=float64), Array(1.93245261, dtype=float64), Array(1.96858924, dtype=float64), Array(1.9955494, dtype=float64), Array(1.93088357, dtype=float64), Array(1.94274379, dtype=float64), Array(1.8909293, dtype=float64), Array(1.97221767, dtype=float64), Array(1.90672151, dtype=float64), Array(1.95546761, dtype=float64), Array(1.94344279, dtype=float64), Array(1.99796136, dtype=float64), Array(1.95020406, dtype=float64), Array(1.91432358, dtype=float64), Array(1.9461877, dtype=float64), Array(1.95420342, dtype=float64), Array(1.88141869, dtype=float64), Array(1.92453059, dtype=float64), Array(1.97319558, dtype=float64), Array(1.9293042, dtype=float64), Array(1.96764451, dtype=float64), Array(1.90225191, dtype=float64), Array(1.92768588, dtype=float64), Array(1.87881507, dtype=float64), Array(1.97343194, dtype=float64), Array(1.96115606, dtype=float64), Array(1.90136862, dtype=float64), Array(1.86913644, dtype=float64), Array(1.98144999, dtype=float64), Array(1.87887468, dtype=float64), Array(1.87752101, dtype=float64), Array(1.92683326, dtype=float64), Array(1.81032673, dtype=float64), Array(1.9341887, dtype=float64), Array(1.86903853, dtype=float64), Array(1.83713636, dtype=float64), Array(1.87593541, dtype=float64), Array(1.83906994, dtype=float64), Array(1.88568312, dtype=float64), Array(1.80982252, dtype=float64), Array(1.90391652, dtype=float64), Array(1.7838672, dtype=float64), Array(1.83681837, dtype=float64), Array(1.83465514, dtype=float64), Array(1.78442, dtype=float64), Array(1.79689252, dtype=float64), Array(1.83826121, dtype=float64), Array(1.74942237, dtype=float64), Array(1.85830428, dtype=float64), Array(1.7354959, dtype=float64), Array(1.91956479, dtype=float64), Array(1.89240068, dtype=float64), Array(1.64648056, dtype=float64), Array(1.73268573, dtype=float64), Array(1.69970688, dtype=float64), Array(1.62581827, dtype=float64), Array(1.69960236, dtype=float64), Array(1.62607966, dtype=float64), Array(1.5506902, dtype=float64), Array(1.51429172, dtype=float64), Array(1.60278037, dtype=float64), Array(1.56482673, dtype=float64), Array(1.63183203, dtype=float64), Array(1.54486939, dtype=float64), Array(1.54554339, dtype=float64), Array(1.53402107, dtype=float64), Array(1.57294089, dtype=float64), Array(1.47471068, dtype=float64), Array(1.57122712, dtype=float64), Array(1.54864844, dtype=float64), Array(1.48660084, dtype=float64), Array(1.58076267, dtype=float64), Array(1.568664, dtype=float64), Array(1.59867594, dtype=float64), Array(1.62148333, dtype=float64), Array(1.5129462, dtype=float64), Array(1.53250773, dtype=float64), Array(1.51755206, dtype=float64), Array(1.54596084, dtype=float64), Array(1.54796624, dtype=float64), Array(1.60144289, dtype=float64), Array(1.44052431, dtype=float64), Array(1.58478169, dtype=float64), Array(1.56080864, dtype=float64), Array(1.45345851, dtype=float64), Array(1.55604001, dtype=float64), Array(1.5057235, dtype=float64), Array(1.41842023, dtype=float64), Array(1.64383374, dtype=float64), Array(1.50623782, dtype=float64), Array(1.52561265, dtype=float64), Array(1.48646406, dtype=float64), Array(1.45528013, dtype=float64), Array(1.5709477, dtype=float64), Array(1.53991342, dtype=float64), Array(1.56256598, dtype=float64), Array(1.47568938, dtype=float64), Array(1.53480835, dtype=float64), Array(1.44209808, dtype=float64), Array(1.42353034, dtype=float64), Array(1.52755763, dtype=float64), Array(1.38191796, dtype=float64), Array(1.50940062, dtype=float64), Array(1.45603425, dtype=float64), Array(1.62994289, dtype=float64), Array(1.52248994, dtype=float64), Array(1.46144349, dtype=float64), Array(1.52572012, dtype=float64), Array(1.61915955, dtype=float64), Array(1.45868322, dtype=float64), Array(1.29706797, dtype=float64), Array(1.40058093, dtype=float64), Array(1.45010058, dtype=float64), Array(1.44490703, dtype=float64), Array(1.41002165, dtype=float64), Array(1.48638614, dtype=float64), Array(1.44148326, dtype=float64), Array(1.37536326, dtype=float64), Array(1.35081033, dtype=float64), Array(1.43964011, dtype=float64), Array(1.35768538, dtype=float64), Array(1.34318747, dtype=float64), Array(1.29758838, dtype=float64), Array(1.40784455, dtype=float64), Array(1.30881568, dtype=float64), Array(1.39263661, dtype=float64), Array(1.38554123, dtype=float64), Array(1.47048014, dtype=float64), Array(1.37323617, dtype=float64), Array(1.40419747, dtype=float64), Array(1.36046302, dtype=float64), Array(1.40994834, dtype=float64), Array(1.40472132, dtype=float64), Array(1.40449236, dtype=float64), Array(1.20691661, dtype=float64), Array(1.18865392, dtype=float64), Array(1.24560052, dtype=float64), Array(1.17014901, dtype=float64), Array(1.20062113, dtype=float64), Array(1.22385301, dtype=float64), Array(1.24706286, dtype=float64), Array(1.08736615, dtype=float64), Array(1.17315103, dtype=float64), Array(0.96325974, dtype=float64), Array(1.07364242, dtype=float64), Array(0.97438351, dtype=float64), Array(1.01148757, dtype=float64), Array(1.13680741, dtype=float64), Array(1.12858629, dtype=float64), Array(0.95794621, dtype=float64), Array(1.16068788, dtype=float64), Array(0.99828705, dtype=float64), Array(1.27491182, dtype=float64), Array(1.03451443, dtype=float64), Array(1.08498307, dtype=float64), Array(1.11320319, dtype=float64), Array(0.98509131, dtype=float64), Array(1.08603216, dtype=float64), Array(1.01914705, dtype=float64), Array(0.90694696, dtype=float64), Array(1.09177514, dtype=float64), Array(0.90823882, dtype=float64), Array(1.04172015, dtype=float64), Array(1.09622351, dtype=float64), Array(0.89123892, dtype=float64), Array(1.02850814, dtype=float64), Array(0.99787106, dtype=float64), Array(1.01572216, dtype=float64), Array(0.96015789, dtype=float64), Array(0.90575248, dtype=float64), Array(1.02435101, dtype=float64), Array(1.01787387, dtype=float64), Array(0.90782078, dtype=float64), Array(1.02849304, dtype=float64), Array(0.95640333, dtype=float64), Array(0.95913171, dtype=float64), Array(0.95407148, dtype=float64), Array(0.98283019, dtype=float64), Array(0.93059919, dtype=float64), Array(0.90808163, dtype=float64), Array(0.98260366, dtype=float64), Array(0.87783696, dtype=float64), Array(0.96480075, dtype=float64), Array(0.93221094, dtype=float64), Array(1.06875389, dtype=float64), Array(0.91155581, dtype=float64), Array(0.77922413, dtype=float64), Array(0.80931128, dtype=float64), Array(0.87901972, dtype=float64), Array(0.84349623, dtype=float64), Array(0.96968138, dtype=float64), Array(1.13472487, dtype=float64), Array(0.80519018, dtype=float64), Array(0.77822963, dtype=float64), Array(0.79541399, dtype=float64), Array(0.97947014, dtype=float64), Array(0.93864565, dtype=float64), Array(1.01317469, dtype=float64), Array(0.83811, dtype=float64), Array(0.83098975, dtype=float64), Array(0.78090366, dtype=float64), Array(0.86755059, dtype=float64), Array(0.83823607, dtype=float64), Array(0.89838694, dtype=float64), Array(0.90935586, dtype=float64), Array(0.77929832, dtype=float64), Array(0.76232465, dtype=float64), Array(0.8021556, dtype=float64), Array(0.68687331, dtype=float64), Array(0.7679315, dtype=float64), Array(0.87752429, dtype=float64), Array(0.91307668, dtype=float64), Array(0.653357, dtype=float64), Array(0.87469133, dtype=float64), Array(0.89799395, dtype=float64), Array(0.84262211, dtype=float64), Array(0.88350816, dtype=float64), Array(0.76414567, dtype=float64), Array(0.67967491, dtype=float64), Array(0.72880882, dtype=float64), Array(0.73074379, dtype=float64), Array(0.77131715, dtype=float64), Array(0.79923322, dtype=float64), Array(0.70747935, dtype=float64)]\n",
      "training in task errors\n",
      "{'ece': [], 'rmse': [], 'std_residuals': []}\n",
      "0\n",
      " training outtask errors\n",
      "{'ece': [], 'rmse': [], 'std_residuals': []}\n",
      "0\n",
      "---------------\n",
      "{'params': {'embed_both': {'Dense_0': {'bias': (64,), 'kernel': (128, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'embed_xs': {'Dense_0': {'bias': (64,), 'kernel': (1, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'embed_ys': {'Dense_0': {'bias': (64,), 'kernel': (1, 64)}, 'Dense_1': {'bias': (64,), 'kernel': (64, 64)}, 'LayerNorm_0': {'bias': (64,), 'scale': (64,)}, 'LayerNorm_1': {'bias': (64,), 'scale': (64,)}}, 'likelihood': {'projection': {'layers_0': {'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'module': {'Dense_0': {'bias': (128,), 'kernel': (128, 128)}, 'Dense_1': {'bias': (128,), 'kernel': (128, 128)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'LayerNorm_1': {'bias': (128,), 'scale': (128,)}}}, 'layers_1': {'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'module': {'Dense_0': {'bias': (128,), 'kernel': (128, 128)}, 'Dense_1': {'bias': (128,), 'kernel': (128, 128)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}, 'LayerNorm_1': {'bias': (128,), 'scale': (128,)}}}, 'layers_2': {'bias': (2,), 'kernel': (128, 2)}}}, 'posterior_fun': {'likelihood': {'projection': {'Dense_0': {'bias': (128,), 'kernel': (64, 128)}, 'Dense_1': {'bias': (64,), 'kernel': (128, 64)}, 'LayerNorm_0': {'bias': (128,), 'scale': (128,)}}}}}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets check if the necessary savings happened nicely\n",
    "\n",
    "# First the curriculum weights. \n",
    "\n",
    "with open(\"./spl_training_data/spl_model_0_curricula_logs.pkl\", 'rb') as f:\n",
    "    curricula_logs= pickle.load(f)\n",
    "\n",
    "print(curricula_logs.keys())\n",
    "print([w.shape for w in curricula_logs[\"curricula_weights\"]])\n",
    "print(\"------\")\n",
    "\n",
    "print([len(w) for w in curricula_logs[\"curricula_losses\"]])\n",
    "# Now the training metrics\n",
    "\n",
    "with open(\"./spl_training_data/spl_model_0_training_metrics.pkl\", 'rb') as f:\n",
    "    training_metrics = pickle.load(f)\n",
    "\n",
    "print(len(training_metrics[\"training_loss\"]))\n",
    "print(training_metrics[\"training_loss\"][1:])\n",
    "plt.plot(training_metrics[\"training_loss\"][1:])\n",
    "plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(training_metrics[\"training_loss\"])))\n",
    "plt.grid()\n",
    "\n",
    "# Finally the model params\n",
    "\n",
    "print(\"training in task errors\")\n",
    "print(training_metrics[\"training_intask_errors\"])\n",
    "print(len(training_metrics[\"training_intask_errors\"][\"rmse\"]))\n",
    "\n",
    "print(\" training outtask errors\")\n",
    "print(training_metrics[\"training_outtask_errors\"])\n",
    "print(len(training_metrics[\"training_outtask_errors\"]['ece']))\n",
    "\n",
    "print(\"---------------\")\n",
    "\n",
    "params = load_model_params(\"./spl_training_data/spl_model_0.pkl\")\n",
    "\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical cross entropy accuracy metrics, using the log likelihood of gaussians and a given target point y.\n",
    "\n",
    "\n",
    "def cross_entropy_error(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "\n",
    "    # Lets compute the model application, \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "    print(y_means.shape, y_stds.shape, y_target.shape, full_y.shape)  \n",
    "    # Lets compute the log likelihood of the target points given the means and stds\n",
    "    log_probs = -0.5 * ((full_y- y_means)**2 / y_stds**2 + jnp.log(2 * jnp.pi * y_stds**2))\n",
    "    log_likelihood = log_probs.mean()\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def RMSE_means(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "    \n",
    "    return jnp.sqrt(jnp.mean((y_means - full_y)**2))\n",
    "\n",
    "\n",
    "def STD_residuals(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    \n",
    "    full_x = jnp.concatenate([x_context, x_target])\n",
    "    y_means, y_stds = model.apply(params, x_context, y_context, full_x,k=1, rngs={'default': rng}) \n",
    "    \n",
    "    full_y = jnp.concatenate([y_context, y_target])\n",
    "\n",
    "    return (full_y - y_means) / y_stds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_params = load_model_params(\"./spl_training_data/spl_model_0.pkl\")\n",
    "\n",
    "model_rng , key_model = jax.random.split(jax.random.PRNGKey(232))\n",
    "model, init_params = create_model(model_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,) (96,)\n",
      "(96, 1) (96, 1)\n",
      "(64, 1) (64, 1) (32, 1) (32, 1)\n",
      "(96, 1)\n",
      "(96, 1, 1) (96, 1, 1) (32, 1) (96, 1)\n",
      "(128, 96) (128, 96)\n",
      "(128, 64, 1) (128, 64, 1) (128, 32, 1) (128, 32, 1)\n",
      "(96, 1, 1) (96, 1, 1) (32, 1) (96, 1)\n",
      "-0.7066740187777362\n",
      "0.6611184896832716\n",
      "0.03279887746899047\n"
     ]
    }
   ],
   "source": [
    "num_context_samples = 64\n",
    "num_target_samples = 32\n",
    "batch_size = 128\n",
    "kl_penalty = 1e-4\n",
    "num_posterior_mc = 1\n",
    "acc_rng = jax.random.PRNGKey(42)\n",
    "\n",
    "acc_rng , dataset_key = jax.random.split(acc_rng)\n",
    "# Lets sample a in trask distribution dataset\n",
    "\n",
    "sampler_clean = partial(\n",
    "    joint, \n",
    "    f2, \n",
    "    partial(uniform, n=num_target_samples + num_context_samples, bounds=(-1, 1))\n",
    ")\n",
    "\n",
    "\n",
    "# lets get 1280 samples from the dataset\n",
    "\n",
    "x ,y= sampler_clean(dataset_key) \n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x, y = x[..., None], y[..., None]\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x_context, x_target = jnp.split(x, indices_or_sections=(num_context_samples, ))\n",
    "y_context, y_target = jnp.split(y, indices_or_sections=(num_context_samples, ))\n",
    "\n",
    "print(x_context.shape, y_context.shape, x_target.shape, y_target.shape)\n",
    "\n",
    "\n",
    "full_x = jnp.concatenate([x_context, x_target])\n",
    "print(full_x.shape)\n",
    "\n",
    "acc = cross_entropy_error(model, loaded_params, x_context, y_context, x_target, y_target, acc_rng, 1)\n",
    "\n",
    "xs , ys = jax.vmap(sampler_clean)(jax.random.split(dataset_key,128))\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "xs, ys = xs[..., None], ys[..., None]\n",
    "\n",
    "x_contexts, x_targets = jnp.split(xs, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "y_contexts, y_targets = jnp.split(ys, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "\n",
    "print(x_contexts.shape, y_contexts.shape, x_targets.shape, y_targets.shape)\n",
    "\n",
    "accs = jax.vmap(partial(cross_entropy_error, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(accs.mean())\n",
    "\n",
    "rmse_accs = jax.vmap(partial(RMSE_means, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(rmse_accs.mean())\n",
    "\n",
    "std_residuals = jax.vmap(partial(STD_residuals, model, loaded_params, k=1), in_axes=(0,0,0,0,0))(x_contexts, y_contexts, x_targets, y_targets, jax.random.split(acc_rng, 128))\n",
    "\n",
    "print(std_residuals.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
