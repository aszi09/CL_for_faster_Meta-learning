{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:14.056640400Z",
     "start_time": "2024-05-22T15:10:13.992538300Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c27f30fa848373",
   "metadata": {
    "collapsed": false,
    "id": "a4c27f30fa848373"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d71e1dd4366ec57",
   "metadata": {
    "id": "1d71e1dd4366ec57",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:14.089669500Z",
     "start_time": "2024-05-22T15:10:14.061697400Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "context_size=64\n",
    "target_size=32\n",
    "num_epochs=6000\n",
    "kl_penalty=1e-4\n",
    "num_posterior_mc=1\n",
    "rng = jax.random.key(0)\n",
    "test_resolution=512\n",
    "dataset_size=num_epochs*batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20600448391ca4",
   "metadata": {
    "collapsed": false,
    "id": "3f20600448391ca4"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1404b71a83bf0c50",
   "metadata": {
    "id": "1404b71a83bf0c50",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:14.120144400Z",
     "start_time": "2024-05-22T15:10:14.092660Z"
    }
   },
   "outputs": [],
   "source": [
    "class MixtureDataset(Dataset):\n",
    "    def __init__(self, dataset_size, key, num_context_samples, num_target_samples, sampler):\n",
    "        self.key = key\n",
    "        self.dataset_size = dataset_size\n",
    "        self.num_context_samples = num_context_samples\n",
    "        self.num_target_samples = num_target_samples\n",
    "        self.sampler = sampler\n",
    "        self.context_xs, self.target_xs, self.context_ys, self.target_ys = self._get_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.context_xs[idx], self.context_ys[idx], self.target_xs[idx], self.target_ys[idx]\n",
    "\n",
    "    def _get_data(self):\n",
    "        key_data, self.key = jax.random.split(self.key)\n",
    "        xs, ys = jax.vmap(self.sampler)(jax.random.split(key_data, num=self.dataset_size))\n",
    "        xs, ys = xs[..., None], ys[..., None]\n",
    "        # Split into context- and target-points.\n",
    "        X, x_test = jnp.split(xs, indices_or_sections=(context_size, ), axis=1)\n",
    "        y, y_test = jnp.split(ys, indices_or_sections=(context_size, ), axis=1)\n",
    "        return X, x_test, y, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba3ed0fd18146f",
   "metadata": {
    "collapsed": false,
    "id": "c3ba3ed0fd18146f"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8faefa241eb549e0",
   "metadata": {
    "id": "8faefa241eb549e0",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:30.595671700Z",
     "start_time": "2024-05-22T15:10:14.130498300Z"
    }
   },
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory allocating 294915840 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mXlaRuntimeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 56\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(xs_context), torch\u001B[38;5;241m.\u001B[39mtensor(ys_context), torch\u001B[38;5;241m.\u001B[39mtensor(xs_target), torch\u001B[38;5;241m.\u001B[39mtensor(ys_target)\n\u001B[0;32m     55\u001B[0m rng, key \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(rng)\n\u001B[1;32m---> 56\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mMixtureDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_context_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_target_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_sampler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39mnumpy_collate)\n\u001B[0;32m     58\u001B[0m data_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(dataloader)\n",
      "Cell \u001B[1;32mIn[30], line 8\u001B[0m, in \u001B[0;36mMixtureDataset.__init__\u001B[1;34m(self, dataset_size, key, num_context_samples, num_target_samples, sampler)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_target_samples \u001B[38;5;241m=\u001B[39m num_target_samples\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampler \u001B[38;5;241m=\u001B[39m sampler\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_xs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_xs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_ys, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_ys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[30], line 18\u001B[0m, in \u001B[0;36mMixtureDataset._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     17\u001B[0m     key_data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey)\n\u001B[1;32m---> 18\u001B[0m     xs, ys \u001B[38;5;241m=\u001B[39m \u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampler\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     xs, ys \u001B[38;5;241m=\u001B[39m xs[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m], ys[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# Split into context- and target-points.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping hidden 3 frame]\u001B[0m\n",
      "Cell \u001B[1;32mIn[31], line 21\u001B[0m, in \u001B[0;36mjoint\u001B[1;34m(module, data_sampler, key, return_params)\u001B[0m\n\u001B[0;32m     18\u001B[0m key_param, key_rng, key_data \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(key, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m     20\u001B[0m params \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39minit({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: key_param, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m: key_rng}, jnp\u001B[38;5;241m.\u001B[39mzeros(()))\n\u001B[1;32m---> 21\u001B[0m xs, ys \u001B[38;5;241m=\u001B[39m \u001B[43mdata_sampler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_params:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m xs, ys, params\n",
      "Cell \u001B[1;32mIn[31], line 39\u001B[0m, in \u001B[0;36muniform\u001B[1;34m(module, params, key, n, bounds)\u001B[0m\n\u001B[0;32m     37\u001B[0m key_xs, key_ys \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(key)\n\u001B[0;32m     38\u001B[0m xs \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(key_xs, (n,)) \u001B[38;5;241m*\u001B[39m (bounds[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m bounds[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m+\u001B[39m bounds[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 39\u001B[0m ys \u001B[38;5;241m=\u001B[39m \u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrngs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdefault\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey_ys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m xs, ys\n",
      "    \u001B[1;31m[... skipping hidden 9 frame]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL_for_faster_Meta-learning\\functions.py:13\u001B[0m, in \u001B[0;36mWhiteNoise.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;129m@nn\u001B[39m\u001B[38;5;241m.\u001B[39mcompact\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: jax\u001B[38;5;241m.\u001B[39mtyping\u001B[38;5;241m.\u001B[39mArrayLike) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m jax\u001B[38;5;241m.\u001B[39mArray:\n\u001B[1;32m---> 13\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_rng()\n\u001B[0;32m     15\u001B[0m     e \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(key, jnp\u001B[38;5;241m.\u001B[39mshape(out))\n",
      "    \u001B[1;31m[... skipping hidden 2 frame]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL_for_faster_Meta-learning\\functions.py:99\u001B[0m, in \u001B[0;36mFourier.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     93\u001B[0m a \u001B[38;5;241m=\u001B[39m a \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(a) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     95\u001B[0m waves \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mcos(\n\u001B[0;32m     96\u001B[0m     (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn) \u001B[38;5;241m*\u001B[39m x \u001B[38;5;241m-\u001B[39m phase) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod\n\u001B[0;32m     97\u001B[0m )\n\u001B[1;32m---> 99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m a[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mwaves\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL_for_faster_Meta-learning\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\reductions.py:226\u001B[0m, in \u001B[0;36msum\u001B[1;34m(a, axis, dtype, out, keepdims, initial, where, promote_integers)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@implements\u001B[39m(np\u001B[38;5;241m.\u001B[39msum, skip_params\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout\u001B[39m\u001B[38;5;124m'\u001B[39m], extra_params\u001B[38;5;241m=\u001B[39m_PROMOTE_INTEGERS_DOC)\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msum\u001B[39m(a: ArrayLike, axis: Axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, dtype: DTypeLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    224\u001B[0m         out: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, keepdims: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, initial: ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    225\u001B[0m         where: ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, promote_integers: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Array:\n\u001B[1;32m--> 226\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_reduce_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_ensure_optional_axes\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mpromote_integers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpromote_integers\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[1;31m[... skipping hidden 18 frame]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CL_for_faster_Meta-learning\\venv\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:1213\u001B[0m, in \u001B[0;36mExecuteReplicated.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1211\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001B[0;32m   1212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1213\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxla_executable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_sharded\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_bufs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dispatch\u001B[38;5;241m.\u001B[39mneeds_check_special():\n\u001B[0;32m   1215\u001B[0m   out_arrays \u001B[38;5;241m=\u001B[39m results\u001B[38;5;241m.\u001B[39mdisassemble_into_single_device_arrays()\n",
      "\u001B[1;31mXlaRuntimeError\u001B[0m: RESOURCE_EXHAUSTED: Out of memory allocating 294915840 bytes."
     ]
    }
   ],
   "source": [
    "f1 = Fourier(n=4, amplitude=.5, period=1.0)\n",
    "f2 = Fourier(n=2, amplitude=.5, period=1.0)\n",
    "f3 = Fourier(n=6, amplitude=.5, period=2.0)\n",
    "f4 = Fourier(n=3, amplitude=1.0, period=2.0)\n",
    "\n",
    "m = Mixture([Shift(f1, y_shift=-2), Shift(f2, y_shift=0.0), Shift(f3, y_shift=2)])\n",
    "nm = Mixture([WhiteNoise(m.branches[0], 0.05), WhiteNoise(m.branches[1], 0.2), WhiteNoise(m.branches[2], 0.1)])\n",
    "def joint(\n",
    "    module: nn.Module,\n",
    "    data_sampler: Callable[\n",
    "        [nn.Module, flax.typing.VariableDict, flax.typing.PRNGKey],\n",
    "        tuple[jax.Array, jax.Array]\n",
    "    ],\n",
    "    key: flax.typing.PRNGKey,\n",
    "    return_params: bool = False\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "    # Samples from p(Z, X, Y)\n",
    "    key_param, key_rng, key_data = jax.random.split(key, 3)\n",
    "\n",
    "    params = module.init({'params': key_param, 'default': key_rng}, jnp.zeros(()))\n",
    "    xs, ys = data_sampler(module, params, key_data)\n",
    "\n",
    "    if return_params:\n",
    "        return xs, ys, params\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def uniform(\n",
    "    module: nn.Module,\n",
    "    params: flax.typing.VariableDict,\n",
    "    key: flax.typing.PRNGKey,\n",
    "    n: int,\n",
    "    bounds: tuple[float, float]\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "\n",
    "    # Samples from p(X, Y | Z) = p(Y | Z, X)p(X)\n",
    "    key_xs, key_ys = jax.random.split(key)\n",
    "    xs = jax.random.uniform(key_xs, (n,)) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "    ys = jax.vmap(module.apply, in_axes=(None, 0))(params, xs, rngs={'default': jax.random.split(key_ys, n)})\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "data_sampler = partial(\n",
    "    joint,\n",
    "    WhiteNoise(f2, 0.1),\n",
    "    partial(uniform, n=context_size + target_size, bounds=(-1, 1))\n",
    ")\n",
    "def numpy_collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    xs_context = np.array(transposed_data[0])\n",
    "    ys_context = np.array(transposed_data[1])\n",
    "    xs_target = np.array(transposed_data[2])\n",
    "    ys_target = np.array(transposed_data[3])\n",
    "    return torch.tensor(xs_context), torch.tensor(ys_context), torch.tensor(xs_target), torch.tensor(ys_target)\n",
    "rng, key = jax.random.split(rng)\n",
    "dataset = MixtureDataset(dataset_size=(batch_size * num_epochs + 10), key=key, num_context_samples=context_size, num_target_samples=target_size, sampler=data_sampler)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qXi4rAB93PVc",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:30.609796700Z",
     "start_time": "2024-05-22T15:10:30.605387300Z"
    }
   },
   "id": "qXi4rAB93PVc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb1ad321c4d3494",
   "metadata": {
    "collapsed": false,
    "id": "8fb1ad321c4d3494"
   },
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hzWixCaS8j3t",
   "metadata": {
    "id": "hzWixCaS8j3t",
    "ExecuteTime": {
     "end_time": "2024-05-22T15:10:30.683941800Z",
     "start_time": "2024-05-22T15:10:30.610862900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def f(\n",
    "        key: flax.typing.PRNGKey,\n",
    "        x: jax.Array,\n",
    "        noise_scale: float = 0.2,\n",
    "        mixture_prob: float = 0.5,\n",
    "        corrupt: bool = True\n",
    "):\n",
    "    key_noise, key_mixture = jax.random.split(key)\n",
    "\n",
    "    noise = jax.random.normal(key, x.shape) * noise_scale\n",
    "    choice = jax.random.bernoulli(key_mixture, mixture_prob, x.shape)\n",
    "\n",
    "    # return choice * (jnp.sin(2 * jnp.pi * x / 2)) + (1 - choice) * (jnp.cos(2 * jnp.pi * 2 * x)) + corrupt * noise\n",
    "    return choice * (-2 - jnp.cos(2 * jnp.pi * x)) + (1 - choice) * (2 + jnp.cos(2 * jnp.pi * x)) + corrupt * noise\n",
    "\n",
    "\n",
    "def initialize_np(rng, dataset_size, test_resolution=500):\n",
    "    rng, key_data, key_test, key_x = jax.random.split(rng, 4)\n",
    "\n",
    "    keys_data = jax.random.split(key_data, (dataset_size,))\n",
    "    keys_test = jax.random.split(key_test, (test_resolution,))\n",
    "\n",
    "    xs = jax.random.uniform(key_x, (dataset_size,)) * 2 - 1\n",
    "    ys = jax.vmap(f)(keys_data, xs)\n",
    "    embedding_xs = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "    embedding_ys = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "    embedding_both = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "\n",
    "    projection_posterior = NonLinearMVN(\n",
    "        MLP([128, 64], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True))\n",
    "\n",
    "    # output_model = nn.Sequential([\n",
    "    #     ResBlock(\n",
    "    #         MLP([128, 128], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    #     ),\n",
    "    #     ResBlock(\n",
    "    #         MLP([128, 128], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    #     ),\n",
    "    #     nn.Dense(2)\n",
    "    # ])\n",
    "    output_model = MLP([128, 128, 2], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True)\n",
    "    projection_outputs = NonLinearMVN(output_model)\n",
    "\n",
    "    posterior_aggregator = MeanAggregator(projection_posterior)\n",
    "\n",
    "    model = MixtureNeuralProcess(\n",
    "        embedding_xs, embedding_ys, embedding_both,\n",
    "        posterior_aggregator,\n",
    "        projection_outputs\n",
    "    )\n",
    "\n",
    "    rng, key1, key2 = jax.random.split(rng, 3)\n",
    "    params = model.init({'params': key1, 'default': key2}, xs[:, None], ys[:, None], xs[:3, None])\n",
    "    return model, params\n",
    "\n",
    "\n",
    "def batch_to_screenernet_input(xs, ys):\n",
    "    xs = xs[:, :, 0]\n",
    "    ys = ys[:, :, 0]\n",
    "    return jnp.concatenate((xs, ys), axis=1)\n",
    "\n",
    "\n",
    "def initialize_optimizer(params):\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "    return optimizer, opt_state\n",
    "\n",
    "def screenernet_loss(screenernet, screenernet_input, apply_fn, losses):\n",
    "    \"\"\"\n",
    "    Computes the objective loss of ScreenerNet.\n",
    "    \"\"\"\n",
    "    weights = apply_fn(screenernet, screenernet_input).flatten()\n",
    "\n",
    "    def body_fun(i, loss_sn):\n",
    "        loss = losses[i]\n",
    "        weight = weights[i]\n",
    "        regularization_term = (1 - weight) * (1 - weight) * loss + weight * weight * jnp.maximum(1 - loss, 0)\n",
    "        return loss_sn + regularization_term\n",
    "\n",
    "    loss_screenernet = 0.0\n",
    "    loss_screenernet = jax.lax.fori_loop(0, len(losses), body_fun, loss_screenernet)\n",
    "    loss_screenernet = loss_screenernet * (1 / len(losses))\n",
    "    return loss_screenernet\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 9, 10))\n",
    "def np_losses_batch(apply_fn, elbo_fn, f_size, np_params, xs_context, ys_context, xs_target, ys_target,\n",
    "                    key, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the un-weighted ELBOs for all tasks in a batch.\n",
    "    \"\"\"\n",
    "    # Compute ELBO over batch of datasets\n",
    "    elbos = jax.vmap(partial(\n",
    "        apply_fn,\n",
    "        np_params,\n",
    "        beta=kl_penalty, k=num_posterior_mc,\n",
    "        method=elbo_fn\n",
    "    ))(\n",
    "        xs_context, ys_context, xs_target, ys_target, rngs={'default': jax.random.split(key, f_size)}\n",
    "    )\n",
    "    return elbos\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 10, 11))\n",
    "def np_weighted_loss(apply_fn, elbo_fn, f_size, np_params, weights, xs_context, ys_context, xs_target,\n",
    "                     ys_target, key, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the weighted loss for a batch of tasks.\n",
    "    \"\"\"\n",
    "    elbos = np_losses_batch(apply_fn, elbo_fn, f_size, np_params, xs_context, ys_context,\n",
    "                            xs_target, ys_target, key, kl_penalty, num_posterior_mc)\n",
    "    return -jnp.multiply(elbos, weights).mean()\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 11, 12, 13))\n",
    "def update_np(\n",
    "        apply_fn,\n",
    "        elbo_fn,\n",
    "        f_size,\n",
    "        theta: flax.typing.VariableDict,\n",
    "        opt_state: optax.OptState,\n",
    "        weights,\n",
    "        xs_context,\n",
    "        ys_context,\n",
    "        xs_target,\n",
    "        ys_target,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "        optimizer,\n",
    "        kl_penalty,\n",
    "        num_posterior_mc\n",
    ") -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "    # Implements a generic SGD Step\n",
    "\n",
    "    value, grad = (jax.value_and_grad(np_weighted_loss, argnums=3)\n",
    "                   (apply_fn, elbo_fn, f_size, theta, weights, xs_context, ys_context, xs_target, ys_target,\n",
    "                    random_key, kl_penalty, num_posterior_mc))\n",
    "\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "    theta = optax.apply_updates(theta, updates)\n",
    "\n",
    "    return theta, opt_state, value\n",
    "\n",
    "\n",
    "def update_screenernet(tx, screenernet_opt, screenernet_input, apply_fn, screenernet, losses):\n",
    "    \"\"\"\n",
    "    Performs one gradient step on the ScreenerNet.\n",
    "    \"\"\"\n",
    "    loss_grad_fn = jax.value_and_grad(screenernet_loss, argnums=0)\n",
    "    loss_val, grads = loss_grad_fn(screenernet, screenernet_input, apply_fn, losses)\n",
    "    updates, opt_state = tx.update(grads, screenernet_opt)\n",
    "    screenernet = optax.apply_updates(screenernet, updates)\n",
    "    return loss_val, screenernet\n",
    "\n",
    "\n",
    "def normalize_array(x, m, M):\n",
    "    return (x - m) / (M - m)\n",
    "\n",
    "\n",
    "def train(dataloader, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Performs training of the NP and ScreenerNet.\n",
    "    \"\"\"\n",
    "    key, rng = jax.random.split(rng)\n",
    "    np_model, np_params = initialize_np(key, dataset_size)\n",
    "    key, rng = jax.random.split(rng)\n",
    "    sn_model = MLP([2 * context_size, 128, 128, 1], activation=jax.nn.sigmoid, activate_final=True, use_layernorm=True)\n",
    "    dummy = jax.random.normal(key, (2 * context_size,))\n",
    "    screenernet_params = sn_model.init(key, dummy)\n",
    "    optimizer, opt_state = initialize_optimizer(np_params)\n",
    "    tx = optax.adam(learning_rate=1e-3)\n",
    "    sn_opt_state = tx.init(screenernet_params)\n",
    "    data_iter = iter(dataloader)\n",
    "    best, best_params = jnp.inf, np_params\n",
    "    np_losses = list()\n",
    "    for _ in (pbar := tqdm.trange(num_epochs, desc='Optimizing params. ')):\n",
    "        batch = next(data_iter)\n",
    "        batch = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch)\n",
    "        xs_context, ys_context, xs_target, ys_target = batch\n",
    "        screenernet_input = batch_to_screenernet_input(xs_context, ys_context)\n",
    "        key, rng = jax.random.split(rng)\n",
    "        losses = np_losses_batch(np_model.apply, np_model.elbo, 2 * context_size, np_params,\n",
    "                                         xs_context, ys_context, xs_target, ys_target,\n",
    "                                         key, kl_penalty=kl_penalty, num_posterior_mc=num_posterior_mc) \n",
    "        # TODO: what should be their range and how should they be interpreted?\n",
    "        loss_np = -losses.mean()\n",
    "        losses = (losses - jnp.min(losses, axis=None)) / (jnp.max(losses, axis=None) - jnp.min(losses, axis=None)) # TODO: better normalization\n",
    "        weights = sn_model.apply(screenernet_params, screenernet_input).flatten()\n",
    "        sum_weights = jnp.sum(weights, axis=None)\n",
    "        if sum_weights != 0:\n",
    "          weights = (1 / sum_weights) * weights\n",
    "        rng, key = jax.random.split(rng)\n",
    "        np_params, opt_state, loss_np_weighted = update_np(np_model.apply, np_model.elbo, 2 * context_size, np_params, opt_state,\n",
    "                                                  weights, xs_context, ys_context, xs_target, ys_target, key, optimizer,\n",
    "                                                  kl_penalty, num_posterior_mc)\n",
    "        loss_sn, screenernet_params = update_screenernet(tx, sn_opt_state, screenernet_input,\n",
    "                                                         sn_model.apply, screenernet_params, losses)\n",
    "        np_losses.append(loss_np)\n",
    "        if loss_np < best:\n",
    "            best = loss_np\n",
    "            best_params = np_params\n",
    "        pbar.set_description(f'Optimizing params. Losses: {loss_sn:.4f} {loss_np:.4f}')\n",
    "    return np_model, best_params, np_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa9e43b6e81c85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "d0aa9e43b6e81c85",
    "outputId": "8c8880fb-329a-4e55-99bd-f9e27bd665c5",
    "ExecuteTime": {
     "start_time": "2024-05-22T15:10:30.615780800Z"
    }
   },
   "outputs": [],
   "source": [
    "model, params, losses=train(dataloader, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67626662884793",
   "metadata": {
    "id": "7b67626662884793",
    "ExecuteTime": {
     "start_time": "2024-05-22T15:10:30.619845500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test predictions on functions from the training-distribution\n",
    "def f(\n",
    "    key: flax.typing.PRNGKey,\n",
    "    x: jax.Array,\n",
    "    noise_scale: float = 0.2,\n",
    "    mixture_prob: float = 0.5,\n",
    "    corrupt: bool = True\n",
    "):\n",
    "    key_noise, key_mixture = jax.random.split(key)\n",
    "\n",
    "    noise = jax.random.normal(key, x.shape) * noise_scale\n",
    "\n",
    "    # return choice * (jnp.sin(2 * jnp.pi * x / 2)) + (1 - choice) * (jnp.cos(2 * jnp.pi * 2 * x)) + corrupt * noise\n",
    "    return(-2-jnp.cos(2 * jnp.pi * x)) + corrupt * noise\n",
    "key = jax.random.key(42)\n",
    "key1, key2, key = jax.random.split(key, 3)\n",
    "keys_test = jax.random.split(key1, (test_resolution,))\n",
    "x_test = jnp.linspace(-1, 1, test_resolution)\n",
    "y_test = jax.vmap(partial(f, corrupt=False))(keys_test, x_test)\n",
    "x_train, y_train = data_sampler(key2)\n",
    "x_train, y_train = x_train[..., None], y_train[..., None]\n",
    "\n",
    "# Split into context- and target-points.\n",
    "X, x_predict_train = jnp.split(x_train, indices_or_sections=(context_size, ))\n",
    "y, y_predict_train = jnp.split(y_train, indices_or_sections=(context_size, ))\n",
    "# Compute ELBO over batch of datasets\n",
    "# means, stds = model.apply(\n",
    "#     params, \n",
    "#     X, y, x_test[..., None],\n",
    "#     k=10,\n",
    "#     rngs={'default': jax.random.key(0)}\n",
    "# )\n",
    "means, stds = model.apply(\n",
    "    params, \n",
    "    X, y, x_test[..., None],\n",
    "    k=1,\n",
    "    rngs={'default': key}\n",
    ")\n",
    "print('prediction-shape', means.shape, stds.shape)  # dim: (len(x_test), k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8973e513093a0c4",
   "metadata": {
    "id": "b8973e513093a0c4",
    "ExecuteTime": {
     "start_time": "2024-05-22T15:10:30.621840Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(losses)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8711dd89e14a9",
   "metadata": {
    "id": "67d8711dd89e14a9",
    "ExecuteTime": {
     "start_time": "2024-05-22T15:10:30.624885300Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(means.shape[1]):\n",
    "    plt.plot(x_test, means[:, i], color='black', alpha=0.2)  # Mixture-components\n",
    "    plt.fill_between(\n",
    "        x_test,\n",
    "        means[:, i, 0] + stds[:, i, 0],\n",
    "        means[:, i, 0] - stds[:, i, 0],\n",
    "        color='blue', alpha=0.2\n",
    "    )\n",
    "\n",
    "plt.scatter(X, y, color='green', label='context')\n",
    "plt.scatter(x_predict_train, y_predict_train, color='red', label='target')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-22T15:10:30.626877600Z"
    }
   },
   "id": "25d252c61957a47f"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
