{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.374505500Z",
     "start_time": "2024-06-04T17:59:53.274344900Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "from flax.training import checkpoints, train_state\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c27f30fa848373",
   "metadata": {
    "collapsed": false,
    "id": "a4c27f30fa848373"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1d71e1dd4366ec57",
   "metadata": {
    "id": "1d71e1dd4366ec57",
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.422977400Z",
     "start_time": "2024-06-04T17:59:53.380489700Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "context_size=64\n",
    "target_size=32\n",
    "num_epochs=100\n",
    "kl_penalty=1e-4\n",
    "num_posterior_mc=1\n",
    "rng = jax.random.key(0)\n",
    "test_resolution=512\n",
    "dataset_size=128*100\n",
    "FOURIER = 0\n",
    "POLYN = 1\n",
    "SLOPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20600448391ca4",
   "metadata": {
    "collapsed": false,
    "id": "3f20600448391ca4"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1404b71a83bf0c50",
   "metadata": {
    "id": "1404b71a83bf0c50",
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.470940500Z",
     "start_time": "2024-06-04T17:59:53.416986300Z"
    }
   },
   "outputs": [],
   "source": [
    "class MixtureDataset(Dataset):\n",
    "    def __init__(self, dataset_size, key, num_context_samples, num_target_samples, sampler):\n",
    "        self.key = key\n",
    "        self.dataset_size = dataset_size\n",
    "        self.num_context_samples = num_context_samples\n",
    "        self.num_target_samples = num_target_samples\n",
    "        self.sampler = sampler\n",
    "        self.context_xs, self.target_xs, self.context_ys, self.target_ys = self._get_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.context_xs[idx], self.context_ys[idx], self.target_xs[idx], self.target_ys[idx]\n",
    "\n",
    "    def _get_data(self):\n",
    "        key_data, self.key = jax.random.split(self.key)\n",
    "        xs, ys = jax.vmap(self.sampler)(jax.random.split(key_data, num=self.dataset_size))\n",
    "        xs, ys = xs[..., None], ys[..., None]\n",
    "        # Split into context- and target-points.\n",
    "        X, x_test = jnp.split(xs, indices_or_sections=(context_size, ), axis=1)\n",
    "        y, y_test = jnp.split(ys, indices_or_sections=(context_size, ), axis=1)\n",
    "        return X, x_test, y, y_test\n",
    "    \n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, dataset, context_size, dataset_size):\n",
    "        self.context_size = context_size\n",
    "        self.dataset_size = dataset_size\n",
    "        self.context_xs, self.target_xs, self.context_ys, self.target_ys, self.distribs, self.noises = self._get_data(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.context_xs[idx], self.context_ys[idx], self.target_xs[idx], self.target_ys[idx], self.distribs[idx], self.noises[idx], idx\n",
    "\n",
    "    def _get_data(self, dataset):\n",
    "      xs_ys, distribs, noises = dataset\n",
    "      xs, ys = xs_ys\n",
    "      context_xs, target_xs = jnp.split(xs, indices_or_sections=(self.context_size, ), axis=1)\n",
    "      context_ys, target_ys = jnp.split(ys, indices_or_sections=(self.context_size, ), axis=1)\n",
    "      return context_xs, target_xs, context_ys, target_ys, distribs, noises"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Active Bias\n",
    "## Variance utility functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334558a637ef98fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def update_vars(sum_xs, sum_xs_squared, inds, all_errs, all_vars, new_val, sample_ind):\n",
    "#     sum_x = sum_xs[sample_ind]\n",
    "#     sum_xs_squared = sum_xs_squared[sample_ind]\n",
    "#     ind = inds[sample_ind]\n",
    "#     errs = all_errs[sample_ind]\n",
    "#     sum_x_new, sum_x_squared_new, new_var, new_errs, new_ind = add_err(sum_x, sum_xs_squared, new_val, errs, ind, len(errs))\n",
    "#     sum_xs_new = sum_x_new.at[sample_ind].set(sum_x_new)\n",
    "#     sum_xs_squared_new = sum_xs_squared.at[sample_ind].set(sum_x_squared_new)\n",
    "#     all_errs_new = all_errs.at[sample_ind].set(new_errs)\n",
    "#     all_vars_new = all_vars.at[sample_ind].set(new_var)\n",
    "#     new_inds = inds.at[sample_ind].set(new_ind)\n",
    "#     return all_vars_new, sum_xs_new, sum_xs_squared_new, all_errs_new, new_inds\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def add_err(sum_x, sum_x_squared, new_x, errs, ind, window_size):\n",
    "#     new_ind = (ind + 1) % window_size\n",
    "#     old_x = errs[new_ind]\n",
    "#     sum_x_new = sum_x + new_x - old_x\n",
    "#     sum_x_squared_new = sum_x_squared + new_x**new_x - old_x**old_x\n",
    "# \n",
    "#     mean_new = sum_x_new / window_size\n",
    "#     new_var = sum_x_squared_new / window_size - mean_new * mean_new\n",
    "#     new_errs = errs.at[new_ind].set(new_x)\n",
    "#     return sum_x_new, sum_x_squared_new, new_var, new_errs, new_ind\n",
    "@jax.jit\n",
    "def get_prob_score(variance, count):\n",
    "    return jax.lax.cond(count >= 2, lambda _: (jnp.sqrt(variance + (variance**2 / (count - 1)))), lambda _ : 0.0, 0)\n",
    "    \n",
    "    \n",
    "@jax.jit\n",
    "def get_prob_scores(vars, counts, eps=0.05):\n",
    "    probs = jax.vmap(get_prob_score)(vars, counts)\n",
    "    return probs + eps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.499024Z",
     "start_time": "2024-06-04T17:59:53.467285300Z"
    }
   },
   "id": "6fc1f24dc2833a77",
   "execution_count": 225
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def add_err(x, count, m, s):\n",
    "    new_count = count + 1\n",
    "    new_m = jax.lax.cond((count > 1), lambda _: (m + (x - m) / count), lambda _: x, 0)\n",
    "    # new_m = (m + (x - m) / count) if count > 1 else x\n",
    "    new_s = jax.lax.cond((count > 1), lambda _: (s + (x - m) * (x - new_m)), lambda _ : 0.0, 0)\n",
    "    # new_s = (s + (x - m) * (x - new_m)) if count > 1 else 0.0\n",
    "    variance = jax.lax.cond((count > 1), lambda _ :(new_s / (count - 1)), lambda _ : 0.0, 0)\n",
    "    # variance = new_s / (count - 1) if count > 1 else 0.0\n",
    "    return variance, new_count, new_m, new_s\n",
    "\n",
    "@partial(jax.jit, static_argnums=(5,))\n",
    "def update_vars(counts, ms, ss, vars, new_val, ind):\n",
    "    count = counts[ind]\n",
    "    m = ms[ind]\n",
    "    s = ss[ind]\n",
    "    variance, new_count, new_m, new_s = add_err(new_val, count, m, s)\n",
    "    new_ms = ms.at[ind].set(new_m)\n",
    "    new_ss = ss.at[ind].set(new_s)\n",
    "    new_counts = counts.at[ind].set(new_count)\n",
    "    new_vars = vars.at[ind].set(variance)\n",
    "    return new_vars, new_ms, new_ss, new_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.636010200Z",
     "start_time": "2024-06-04T17:59:53.503916900Z"
    }
   },
   "id": "52640690e8a2670b",
   "execution_count": 226
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b6ea602fac29aa7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ProbabilitySampler(Sampler):\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs / np.sum(probs)  # Ensure probabilities sum to 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Sample indices according to the probabilities\n",
    "        p = self.probs\n",
    "        p = np.asarray(p).astype('float64')\n",
    "        if p.sum() != 0:\n",
    "            p = p * (1. / p.sum())\n",
    "        indices = np.random.choice(len(self.probs), size=len(self.probs), replace=True, p=p)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.probs)\n",
    "\n",
    "    def update_probs(self, new_probs):\n",
    "        self.probs = new_probs / np.sum(new_probs)  # Update and normalize probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T17:59:53.648431100Z",
     "start_time": "2024-06-04T17:59:53.596286700Z"
    }
   },
   "id": "fa8d7690ce7f5fee",
   "execution_count": 227
  },
  {
   "cell_type": "markdown",
   "id": "c3ba3ed0fd18146f",
   "metadata": {
    "collapsed": false,
    "id": "c3ba3ed0fd18146f"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8faefa241eb549e0",
   "metadata": {
    "id": "8faefa241eb549e0",
    "ExecuteTime": {
     "end_time": "2024-06-04T18:00:08.171084800Z",
     "start_time": "2024-06-04T17:59:53.624899700Z"
    }
   },
   "outputs": [],
   "source": [
    "f1 = Fourier(n=4, amplitude=.5, period=1.0)\n",
    "f2 = Fourier(n=2, amplitude=.5, period=1.0)\n",
    "f3 = Fourier(n=6, amplitude=.5, period=2.0)\n",
    "f4 = Fourier(n=3, amplitude=1.0, period=2.0)\n",
    "f5 = Slope()\n",
    "f6 = Polynomial(order=2, clip_bounds=(-1,1))\n",
    "\n",
    "\n",
    "m = Mixture([Shift(f1, y_shift=-2), Shift(f2, y_shift=0.0), Shift(f3, y_shift=2), Shift(f5, y_shift=0.5), Shift(f6, y_shift=1.5)])\n",
    "nm = Mixture([WhiteNoise(m.branches[0], 0.05), WhiteNoise(m.branches[1], 0.2), WhiteNoise(m.branches[2], 0.1), WhiteNoise(m.branches[3], 0.1), WhiteNoise(m.branches[4], 0.05)])\n",
    "def joint(\n",
    "    module: nn.Module,\n",
    "    data_sampler: Callable[\n",
    "        [nn.Module, flax.typing.VariableDict, flax.typing.PRNGKey],\n",
    "        tuple[jax.Array, jax.Array]\n",
    "    ],\n",
    "    key: flax.typing.PRNGKey,\n",
    "    return_params: bool = False\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "    # Samples from p(Z, X, Y)\n",
    "    key_param, key_rng, key_data = jax.random.split(key, 3)\n",
    "\n",
    "    params = module.init({'params': key_param, 'default': key_rng}, jnp.zeros(()))\n",
    "    xs, ys = data_sampler(module, params, key_data)\n",
    "\n",
    "    if return_params:\n",
    "        return xs, ys, params\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def uniform(\n",
    "    module: nn.Module,\n",
    "    params: flax.typing.VariableDict,\n",
    "    key: flax.typing.PRNGKey,\n",
    "    n: int,\n",
    "    bounds: tuple[float, float]\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "\n",
    "    # Samples from p(X, Y | Z) = p(Y | Z, X)p(X)\n",
    "    key_xs, key_ys = jax.random.split(key)\n",
    "    xs = jax.random.uniform(key_xs, (n,)) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "    ys = jax.vmap(module.apply, in_axes=(None, 0))(params, xs, rngs={'default': jax.random.split(key_ys, n)})\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1))\n",
    "def gen_sampler_datapoint(key, sampler):\n",
    "    x, y = sampler(key)\n",
    "    x, y = x[..., None], y[..., None]\n",
    "    return x, y \n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def generate_dataset(rng, num_batches, sampler):\n",
    "    keys = jax.random.split(rng, num_batches)\n",
    "    batched_generate = jax.vmap(partial(gen_sampler_datapoint, sampler=sampler))\n",
    "    x, y = batched_generate(keys)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_noisy_split_trainingdata(samplers, sampler_ratios, dataset_size, rng):\n",
    "    \"\"\" \n",
    "    Generate a dataset with a split of different samplers and ratios\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(samplers) == len(sampler_ratios), \"The number of samplers and ratios must be the same\"\n",
    "    assert sum(sampler_ratios) == 1.0, \"The sum of the ratios must be 1.0\"\n",
    "    keys = jax.random.split(rng, len(samplers))\n",
    "    datasets = []\n",
    "    distribs = []\n",
    "    noises = []\n",
    "    for (sampler_prop, ratio, key) in zip(samplers, sampler_ratios, keys):\n",
    "        sampler, distrib, noise = sampler_prop[\"sampler\"], sampler_prop[\"distribution\"], sampler_prop[\"noise\"]\n",
    "        dataset = generate_dataset(key, int(dataset_size*ratio), sampler)\n",
    "        datasets.append(np.asarray(dataset))\n",
    "        distribs.append(jnp.repeat(distrib, int(dataset_size*ratio)))\n",
    "        noises.append(jnp.repeat(noise, int(dataset_size*ratio)))\n",
    "    x_datasets, y_datasets = zip(*datasets)\n",
    "    return  np.asarray((jnp.concatenate(x_datasets), jnp.concatenate(y_datasets))), jnp.concatenate(distribs), jnp.concatenate(noises)\n",
    "\n",
    "data_sampler = partial(\n",
    "    joint,\n",
    "    Shift(f6, y_shift=1.5),\n",
    "    partial(uniform, n=context_size + target_size, bounds=(-1, 1))\n",
    ")\n",
    "data_sampler1 = partial(\n",
    "    joint,\n",
    "    WhiteNoise(f2, 0.1),\n",
    "    partial(uniform, n=context_size + target_size, bounds=(-1, 1))\n",
    ")\n",
    "data_sampler_props_1 = {\n",
    "    \"distribution\": FOURIER,\n",
    "    \"noise\": 0.1,\n",
    "    \"sampler\": data_sampler1\n",
    "}\n",
    "data_sampler2 = partial(\n",
    "    joint,\n",
    "    WhiteNoise(f5, 0.15),\n",
    "    partial(uniform, n=context_size + target_size, bounds=(-1, 1))\n",
    ")\n",
    "data_sampler_props_2 = {\n",
    "    \"distribution\": SLOPE,\n",
    "    \"noise\": 0.15,\n",
    "    \"sampler\": data_sampler1\n",
    "}\n",
    "data_sampler3 = partial(\n",
    "    joint,\n",
    "    Shift(f6, y_shift=1.5),\n",
    "    partial(uniform, n=context_size + target_size, bounds=(-1, 1))\n",
    ")\n",
    "data_sampler_props_3 = {\n",
    "    \"distribution\": POLYN,\n",
    "    \"noise\": 0.0,\n",
    "    \"sampler\": data_sampler3\n",
    "}\n",
    "def numpy_collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    xs_context = np.array(transposed_data[0])\n",
    "    ys_context = np.array(transposed_data[1])\n",
    "    xs_target = np.array(transposed_data[2])\n",
    "    ys_target = np.array(transposed_data[3])\n",
    "    distrib = np.array(transposed_data[4])\n",
    "    noise = np.array(transposed_data[5])\n",
    "    idx = np.array(transposed_data[6])\n",
    "    return torch.tensor(xs_context), torch.tensor(ys_context), torch.tensor(xs_target), torch.tensor(ys_target), torch.tensor(distrib), torch.tensor(noise), torch.tensor(idx)\n",
    "rng, key_test, key_train = jax.random.split(rng, 3)\n",
    "# dataset_train = MixtureDataset(dataset_size=dataset_size, key=key_train, num_context_samples=context_size, num_target_samples=target_size, sampler=data_sampler)\n",
    "# dataset_test = MixtureDataset(dataset_size=batch_size*22, key=key_test, num_context_samples=context_size, num_target_samples=target_size, sampler=data_sampler)\n",
    "\n",
    "dataset_train = SimpleDataset(generate_noisy_split_trainingdata([data_sampler_props_1, data_sampler_props_2, data_sampler_props_3], [0.5, 0.3, 0.2], dataset_size, key_train), context_size=context_size, dataset_size=dataset_size)\n",
    "dataset_test = SimpleDataset(generate_noisy_split_trainingdata([data_sampler_props_1, data_sampler_props_2, data_sampler_props_3], [0.5, 0.3, 0.2], batch_size*22, key_test), context_size=context_size, dataset_size=batch_size*22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1ad321c4d3494",
   "metadata": {
    "collapsed": false,
    "id": "8fb1ad321c4d3494"
   },
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "hzWixCaS8j3t",
   "metadata": {
    "id": "hzWixCaS8j3t",
    "ExecuteTime": {
     "end_time": "2024-06-04T18:00:11.557762700Z",
     "start_time": "2024-06-04T18:00:08.173079500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def f(\n",
    "        key: flax.typing.PRNGKey,\n",
    "        x: jax.Array,\n",
    "        noise_scale: float = 0.2,\n",
    "        mixture_prob: float = 0.5,\n",
    "        corrupt: bool = True\n",
    "):\n",
    "    key_noise, key_mixture = jax.random.split(key)\n",
    "\n",
    "    noise = jax.random.normal(key, x.shape) * noise_scale\n",
    "    choice = jax.random.bernoulli(key_mixture, mixture_prob, x.shape)\n",
    "\n",
    "    # return choice * (jnp.sin(2 * jnp.pi * x / 2)) + (1 - choice) * (jnp.cos(2 * jnp.pi * 2 * x)) + corrupt * noise\n",
    "    return choice * (-2 - jnp.cos(2 * jnp.pi * x)) + (1 - choice) * (2 + jnp.cos(2 * jnp.pi * x)) + corrupt * noise\n",
    "\n",
    "\n",
    "def initialize_np(rng, dataset_size, test_resolution=500):\n",
    "    rng, key_data, key_test, key_x = jax.random.split(rng, 4)\n",
    "\n",
    "    keys_data = jax.random.split(key_data, (dataset_size,))\n",
    "    keys_test = jax.random.split(key_test, (test_resolution,))\n",
    "\n",
    "    xs = jax.random.uniform(key_x, (dataset_size,)) * 2 - 1\n",
    "    ys = jax.vmap(f)(keys_data, xs)\n",
    "    embedding_xs = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "    embedding_ys = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "    embedding_both = MLP([64, 64], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "\n",
    "    projection_posterior = NonLinearMVN(\n",
    "        MLP([128, 64], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True))\n",
    "\n",
    "    # output_model = nn.Sequential([\n",
    "    #     ResBlock(\n",
    "    #         MLP([128, 128], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    #     ),\n",
    "    #     ResBlock(\n",
    "    #         MLP([128, 128], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    #     ),\n",
    "    #     nn.Dense(2)\n",
    "    # ])\n",
    "    output_model = MLP([128, 128, 2], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True)\n",
    "    projection_outputs = NonLinearMVN(output_model)\n",
    "\n",
    "    posterior_aggregator = MeanAggregator(projection_posterior)\n",
    "\n",
    "    model = MixtureNeuralProcess(\n",
    "        embedding_xs, embedding_ys, embedding_both,\n",
    "        posterior_aggregator,\n",
    "        projection_outputs\n",
    "    )\n",
    "\n",
    "    rng, key1, key2 = jax.random.split(rng, 3)\n",
    "    params = model.init({'params': key1, 'default': key2}, xs[:, None], ys[:, None], xs[:3, None])\n",
    "    return model, params\n",
    "\n",
    "@jax.jit\n",
    "def batch_to_screenernet_input(xs, ys):\n",
    "    xs = xs[:, :, 0]\n",
    "    ys = ys[:, :, 0]\n",
    "    return jnp.concatenate((xs, ys), axis=1)\n",
    "\n",
    "\n",
    "def initialize_optimizer(params):\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "    return optimizer, opt_state\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2, 5))\n",
    "def screenernet_loss(screenernet, screenernet_input, apply_fn, losses, flattened, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Computes the objective loss of ScreenerNet.\n",
    "    \"\"\"\n",
    "    weights = apply_fn(screenernet, screenernet_input).flatten()\n",
    "    def body_fun(i, loss_sn):\n",
    "        loss = losses[i]\n",
    "        weight = weights[i] # what is the value?\n",
    "        regularization_term = (1 - weight) * (1 - weight) * loss + weight * weight * jnp.maximum(1.5 - loss, 0)\n",
    "        return loss_sn + regularization_term\n",
    "    # flat_loss = jnp.sum(jnp.abs(flattened))\n",
    "    loss_screenernet = 0.0\n",
    "    loss_screenernet = jax.lax.fori_loop(0, len(losses), body_fun, loss_screenernet)\n",
    "    # loss_screenernet = loss_screenernet * (1 / len(losses)) + alpha * flat_loss\n",
    "    loss_screenernet = loss_screenernet * (1 / len(losses))\n",
    "    return loss_screenernet\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 9, 10))\n",
    "def np_losses_batch_elbo(apply_fn, elbo_fn, f_size, np_params, xs_context, ys_context, xs_target, ys_target,\n",
    "                    key, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the un-weighted ELBOs for all tasks in a batch.\n",
    "    \"\"\"\n",
    "    # Compute ELBO over batch of datasets\n",
    "    elbos = jax.vmap(partial(\n",
    "        apply_fn,\n",
    "        np_params,\n",
    "        beta=kl_penalty, k=num_posterior_mc,\n",
    "        method=elbo_fn\n",
    "    ))(\n",
    "        xs_context, ys_context, xs_target, ys_target, rngs={'default': jax.random.split(key, f_size)}\n",
    "    )\n",
    "    return elbos\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 7))\n",
    "def np_losses_batch_gll(apply_fn, np_params, xs_context, ys_context, xs_target, ys_target, key, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the un-weighted log likelihood loss for all tasks in a batch.\n",
    "    \"\"\"\n",
    "    key_ll, key_app = jax.random.split(key)\n",
    "    means_batch, stds_batch = jax.vmap(partial(\n",
    "        apply_fn,\n",
    "        np_params, \n",
    "        k=num_posterior_mc\n",
    "    ))(\n",
    "        xs_context, ys_context, xs_target, rngs={'default': jax.random.split(key, batch_size)}\n",
    "    )\n",
    "    #keys = jax.random.split(key_ll, ys_target.shape)\n",
    "    means_batch = jnp.reshape(means_batch, (means_batch.shape[0], means_batch.shape[1]))\n",
    "    vs_batch = jnp.square(jnp.reshape(stds_batch, (stds_batch.shape[0], stds_batch.shape[1])))\n",
    "    ys_target = jnp.reshape(ys_target, (ys_target.shape[0], ys_target.shape[1]))\n",
    "    losses = jax.vmap(sample_gaussian_ll_loss, in_axes=(0,0,0))(ys_target, means_batch, vs_batch)\n",
    "    return losses\n",
    "    \n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 8))\n",
    "def np_weighted_loss_gll(apply_fn, np_params, weights, xs_context, ys_context, xs_target, ys_target, key, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the weighted loss for a batch of tasks.\n",
    "    \"\"\"\n",
    "    losses = np_losses_batch_gll(apply_fn, np_params, xs_context, ys_context, xs_target, ys_target, key, num_posterior_mc)\n",
    "    # losses = losses - jnp.minimum(0, jnp.min(losses)) # remove\n",
    "    weighted_losses = losses * weights\n",
    "    return weighted_losses.mean() # try just *\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def elementwise_gaussian_ll_loss(y, mean, std):\n",
    "    eps = 1e-6\n",
    "    v = std * std\n",
    "    return jnp.log(jnp.maximum(v, eps)) + (y - mean)**2 / jnp.maximum(eps, v)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sample_gaussian_ll_loss(ys, means, stds):\n",
    "    losses = jax.vmap(elementwise_gaussian_ll_loss, in_axes=(0,0,0))(ys, means, stds)\n",
    "    res = 0.5 * jnp.mean(losses)\n",
    "    return res\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 10, 11))\n",
    "def np_weighted_loss_elbo(apply_fn, elbo_fn, f_size, np_params, weights, xs_context, ys_context, xs_target,\n",
    "                     ys_target, key, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Computes the weighted loss for a batch of tasks.\n",
    "    \"\"\"\n",
    "    elbos = np_losses_batch_elbo(apply_fn, elbo_fn, f_size, np_params, xs_context, ys_context,\n",
    "                            xs_target, ys_target, key, kl_penalty, num_posterior_mc)\n",
    "    weighted_elbos = elbos * weights\n",
    "    return -weighted_elbos.mean() # try just *\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 11, 12, 13))\n",
    "def update_np_elbo(\n",
    "        apply_fn,\n",
    "        elbo_fn,\n",
    "        f_size,\n",
    "        theta: flax.typing.VariableDict,\n",
    "        opt_state: optax.OptState,\n",
    "        weights,\n",
    "        xs_context,\n",
    "        ys_context,\n",
    "        xs_target,\n",
    "        ys_target,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "        optimizer,\n",
    "        kl_penalty,\n",
    "        num_posterior_mc\n",
    ") -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "    # Implements a generic SGD Step\n",
    "\n",
    "    value, grad = (jax.value_and_grad(np_weighted_loss_elbo, argnums=3)\n",
    "                   (apply_fn, elbo_fn, f_size, theta, weights, xs_context, ys_context, xs_target, ys_target,\n",
    "                    random_key, kl_penalty, num_posterior_mc))\n",
    "\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "    theta = optax.apply_updates(theta, updates)\n",
    "\n",
    "    return theta, opt_state, value\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 9, 10))\n",
    "def update_np_gll(\n",
    "        apply_fn,\n",
    "        theta: flax.typing.VariableDict,\n",
    "        opt_state: optax.OptState,\n",
    "        weights,\n",
    "        xs_context,\n",
    "        ys_context,\n",
    "        xs_target,\n",
    "        ys_target,\n",
    "        random_key: flax.typing.PRNGKey,\n",
    "        optimizer,\n",
    "        num_posterior_mc\n",
    ") -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "    # Implements a generic SGD Step\n",
    "    value, grad = (jax.value_and_grad(np_weighted_loss_gll, argnums=1)\n",
    "                   (apply_fn, theta, weights, xs_context, ys_context, xs_target, ys_target,\n",
    "                    random_key, num_posterior_mc))\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "    theta = optax.apply_updates(theta, updates)\n",
    "\n",
    "    return theta, opt_state, value\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 3))\n",
    "def update_screenernet(tx, screenernet_opt, screenernet_input, apply_fn, screenernet, losses, vars):\n",
    "    \"\"\"\n",
    "    Performs one gradient step on the ScreenerNet.\n",
    "    \"\"\"\n",
    "    loss_grad_fn = jax.value_and_grad(screenernet_loss, argnums=0)\n",
    "    loss_val, grads = loss_grad_fn(screenernet, screenernet_input, apply_fn, losses, vars)\n",
    "    updates, opt_state = tx.update(grads, screenernet_opt)\n",
    "    screenernet = optax.apply_updates(screenernet, updates)\n",
    "    return loss_val, screenernet\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def evaluate(apply_fn, np_params, key, batch):\n",
    "    X, y, x_test, y_test, distrib, noise, idx = batch\n",
    "    X = X.reshape((X.shape[1], X.shape[0], X.shape[2]))\n",
    "    y = y.reshape((y.shape[1], y.shape[0], y.shape[2]))\n",
    "    x_test = x_test.reshape((x_test.shape[1], x_test.shape[0], x_test.shape[2]))\n",
    "    y_test = y_test.reshape((y_test.shape[1], y_test.shape[0], y_test.shape[2]))\n",
    "    # key_ll, key_eval = jax.random.split(key)\n",
    "    means, stds = apply_fn(\n",
    "        np_params, \n",
    "        X[:, None], y[:, None], x_test[:, None],\n",
    "        k=1,\n",
    "        rngs={'default': key}\n",
    "    )\n",
    "    # keys = jax.random.split(key_ll, y_test.shape[0])\n",
    "    L = sample_gaussian_ll_loss(y_test, means, stds)\n",
    "    return L\n",
    "   \n",
    "\"\"\"\n",
    "def screenernet_loss(screenernet, screenernet_input, apply_fn, losses, flattened, alpha=0.0001):\n",
    "    \n",
    "    weights = apply_fn(screenernet, screenernet_input).flatten()\n",
    "    def body_fun(i, loss_sn):\n",
    "        loss = losses[i]\n",
    "        weight = weights[i] # what is the value?\n",
    "        regularization_term = (1 - weight) * (1 - weight) * loss + weight * weight * jnp.maximum(1.5 - loss, 0)\n",
    "        return loss_sn + regularization_term\n",
    "    # flat_loss = jnp.sum(jnp.abs(flattened))\n",
    "    loss_screenernet = 0.0\n",
    "    loss_screenernet = jax.lax.fori_loop(0, len(losses), body_fun, loss_screenernet)\n",
    "    # loss_screenernet = loss_screenernet * (1 / len(losses)) + alpha * flat_loss\n",
    "    loss_screenernet = loss_screenernet * (1 / len(losses))\n",
    "    return loss_screenernet\n",
    "    \"\"\"\n",
    "\n",
    "def batch_update_errs(losses, indices, ms, ss, counts, all_vars):\n",
    "    indices_jnp = jnp.array(indices)\n",
    "    def body_fun(i, agg):\n",
    "        err = losses[i]\n",
    "        sample_ind = indices_jnp[i]\n",
    "        agg_vars, agg_ms, agg_ss, agg_counts = agg\n",
    "        return update_vars(agg_counts, agg_ms, agg_ss, agg_vars, err, sample_ind)\n",
    "    \n",
    "    return jax.lax.fori_loop(0, len(losses), body_fun, (all_vars, ms, ss, counts))\n",
    "    \n",
    "\n",
    "def train(train_dataset, test_dataset, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Performs training of the NP and ScreenerNet.\n",
    "    \"\"\"\n",
    "    key, rng = jax.random.split(rng)\n",
    "    np_model, np_params = initialize_np(key, dataset_size)\n",
    "    key, rng = jax.random.split(rng)\n",
    "    # sn_model = MLP([2 * context_size, 128], activation=jax.nn.sigmoid, activate_final=False, use_layernorm=True)\n",
    "    sn_model = nn.Sequential([\n",
    "        MLP([2 * context_size, 64, 64, 128], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True),\n",
    "        MLP([128, 1], activation=jax.nn.relu, activate_final=True, use_layernorm=False)\n",
    "    ])\n",
    "    dummy = jax.random.normal(key, (2 * context_size,))\n",
    "    screenernet_params = sn_model.init(key, dummy)\n",
    "    optimizer, opt_state = initialize_optimizer(np_params)\n",
    "    tx = optax.adam(learning_rate=1e-3)\n",
    "    sn_opt_state = tx.init(screenernet_params)\n",
    "    best, best_params = jnp.inf, np_params\n",
    "    np_sn_losses = list()\n",
    "    baseline_losses = list()\n",
    "    screenernet_losses = list()\n",
    "    key, rng = jax.random.split(rng)\n",
    "    baseline_model, baseline_params = initialize_np(key, dataset_size)\n",
    "    baseline_optimizer, baseline_opt_state = initialize_optimizer(baseline_params)\n",
    "    best_baseline, best_baseline_params = jnp.inf, baseline_params\n",
    "    for epoch in (pbar := tqdm.trange(num_epochs, desc='Optimizing params. ')):\n",
    "        dl = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=numpy_collate)\n",
    "        test_dl = DataLoader(test_dataset, shuffle=True, batch_size=1, collate_fn=numpy_collate)\n",
    "        data_it = iter(dl)\n",
    "        for stp in range(int(dataset_size / batch_size)):\n",
    "            batch = next(data_it)\n",
    "            batch = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch)\n",
    "            xs_context, ys_context, xs_target, ys_target, distrib, noise, idx = batch\n",
    "            screenernet_input = batch_to_screenernet_input(xs_context, ys_context)\n",
    "            key, rng = jax.random.split(rng)\n",
    "            losses = np_losses_batch_elbo(np_model.apply, np_model.err, batch_size, np_params, xs_context, ys_context, xs_target, ys_target, key, kl_penalty, num_posterior_mc)  # z-score\n",
    "            # print(losses)\n",
    "            loss_np = losses.mean()\n",
    "            weights = sn_model.apply(screenernet_params, screenernet_input).flatten()\n",
    "            if epoch < num_epochs / 4:\n",
    "                weights = jnp.ones(weights.shape)\n",
    "            sum_weights = jnp.sum(weights, axis=None)\n",
    "            if sum_weights != 0:\n",
    "              weights = (batch_size / sum_weights) * weights # remove\n",
    "            rng, key = jax.random.split(rng)\n",
    "            np_params, opt_state, loss_np_weighted = update_np_elbo(np_model.apply, np_model.elbo, batch_size, np_params, opt_state, weights, xs_context, ys_context, xs_target, ys_target, key, optimizer, kl_penalty, num_posterior_mc) # elbo\n",
    "            vars = jnp.concatenate([arr.flatten() for arr in jax.tree_util.tree_leaves(screenernet_params[\"params\"])])\n",
    "            flattened = jnp.array(vars)\n",
    "            loss_sn, screenernet_params = update_screenernet(tx, sn_opt_state, screenernet_input,\n",
    "                                                             sn_model.apply, screenernet_params, losses, flattened)\n",
    "            rng, key = jax.random.split(rng)\n",
    "            baseline_params, baseline_opt_state, baseline_loss = update_np_elbo(baseline_model.apply, baseline_model.elbo,  batch_size, baseline_params, baseline_opt_state, jnp.ones(batch_size), xs_context, ys_context, xs_target, ys_target, key, baseline_optimizer, kl_penalty, num_posterior_mc)\n",
    "            if loss_np_weighted < best:\n",
    "                best = loss_np_weighted\n",
    "                best_params = np_params\n",
    "            if baseline_loss < best_baseline:\n",
    "                best_baseline = baseline_loss\n",
    "                best_baseline_params = baseline_params\n",
    "            screenernet_losses.append(loss_sn)\n",
    "            pbar.set_description(f'Optimizing params. Losses: {loss_sn:.4f} {loss_np:.4f} {baseline_loss: .4f} {loss_np_weighted: .4f}')\n",
    "        #if epoch % 10 == 9:\n",
    "        key1, key2, rng = jax.random.split(rng, 3)\n",
    "        batch1 = next(iter(test_dl))\n",
    "        batch2 = next(iter(test_dl))\n",
    "        batch1 = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch1)\n",
    "        batch2 = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch2)\n",
    "        loss_sn_np = evaluate(np_model.apply, best_params, key1, batch1)\n",
    "        loss_baseline = evaluate(baseline_model.apply, best_baseline_params, key2, batch2)\n",
    "        np_sn_losses.append(loss_sn_np)\n",
    "        baseline_losses.append(loss_baseline)\n",
    "    return np_model, best_params, best_baseline_params, screenernet_losses, np_sn_losses, baseline_losses\n",
    "\n",
    "\n",
    "def train_active_bias(train_dataset, test_dataset, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc):\n",
    "    \"\"\"\n",
    "    Performs training of Neural Processes using Active Bias sampling.\n",
    "    \"\"\"\n",
    "    ms = jnp.zeros(dataset_size)\n",
    "    ss = jnp.zeros(dataset_size)\n",
    "    counts = jnp.zeros(dataset_size)\n",
    "    all_vars = jnp.zeros(dataset_size)\n",
    "    key, rng = jax.random.split(rng)\n",
    "    np_model, np_params = initialize_np(key, dataset_size)\n",
    "    key, rng = jax.random.split(rng)\n",
    "    optimizer, opt_state = initialize_optimizer(np_params)\n",
    "    best, best_params = jnp.inf, np_params\n",
    "    np_sn_losses = list()\n",
    "    key, rng = jax.random.split(rng)\n",
    "    baseline_model, baseline_params = initialize_np(key, dataset_size)\n",
    "    baseline_optimizer, baseline_opt_state = initialize_optimizer(baseline_params)\n",
    "    best_baseline, best_baseline_params = jnp.inf, baseline_params\n",
    "    batch_sampler = ProbabilitySampler(jnp.ones(dataset_size))\n",
    "    losses_ab_train = list()\n",
    "    losses_ab_eval = list()\n",
    "    losses_baseline_train = list()\n",
    "    losses_baseline_eval = list()\n",
    "    for epoch in (pbar := tqdm.trange(num_epochs, desc='Optimizing params. ')):\n",
    "        if epoch >= int(num_epochs*0.10):\n",
    "            key, rng = jax.random.split(rng)\n",
    "            scores = get_prob_scores(all_vars, counts) # what is the window size?\n",
    "            batch_sampler.update_probs(scores)\n",
    "        dl_ab = DataLoader(train_dataset, sampler=batch_sampler, batch_size=batch_size, collate_fn=numpy_collate)\n",
    "        dl = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=numpy_collate)\n",
    "        test_dl = DataLoader(test_dataset, shuffle=True, batch_size=1, collate_fn=numpy_collate)\n",
    "        data_it = iter(dl)\n",
    "        data_it_ab = iter(dl_ab)\n",
    "        for stp in range(int(dataset_size / batch_size)):\n",
    "            batch = next(data_it)\n",
    "            batch_ab = next(data_it_ab)\n",
    "            batch = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch)\n",
    "            batch_ab = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch_ab)\n",
    "            xs_context, ys_context, xs_target, ys_target, distrib, noise, idx = batch\n",
    "            xs_context_ab, ys_context_ab, xs_target_ab, ys_target_ab, distrib_ab, noise_ab, idx_ab = batch_ab\n",
    "            key, rng = jax.random.split(rng)\n",
    "            losses = np_losses_batch_elbo(np_model.apply, np_model.elbo, batch_size, np_params, xs_context, ys_context, xs_target, ys_target, key, kl_penalty, num_posterior_mc)\n",
    "            all_vars, ms, ss, counts = batch_update_errs(losses, idx_ab, ms, ss, counts, all_vars)\n",
    "            weights = jnp.ones(batch_size, dtype=jnp.float32)\n",
    "            rng, key = jax.random.split(rng)\n",
    "            np_params, opt_state, loss_np = update_np_elbo(np_model.apply, np_model.elbo, batch_size, np_params, opt_state, weights, xs_context, ys_context, xs_target, ys_target, key, optimizer, kl_penalty, num_posterior_mc) # elbo\n",
    "            rng, key = jax.random.split(rng)\n",
    "            baseline_params, baseline_opt_state, baseline_loss = update_np_elbo(baseline_model.apply, baseline_model.elbo,  batch_size, baseline_params, baseline_opt_state, jnp.ones(batch_size), xs_context, ys_context, xs_target, ys_target, key, baseline_optimizer, kl_penalty, num_posterior_mc)\n",
    "            if loss_np < best:\n",
    "                best = loss_np\n",
    "                best_params = np_params\n",
    "            if baseline_loss < best_baseline:\n",
    "                best_baseline = baseline_loss\n",
    "                best_baseline_params = baseline_params\n",
    "            losses_ab_train.append(loss_np)\n",
    "            losses_baseline_train.append(baseline_loss)\n",
    "            pbar.set_description(f'Optimizing params. Losses: {loss_np:.4f} {baseline_loss: .4f}')\n",
    "        #if epoch % 10 == 9:\n",
    "        key1, key2, rng = jax.random.split(rng, 3)\n",
    "        batch1 = next(iter(test_dl))\n",
    "        batch2 = next(iter(test_dl))\n",
    "        batch1 = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch1)\n",
    "        batch2 = jax.tree_util.tree_map(lambda tensor: tensor.numpy(), batch2)\n",
    "        loss_ab = evaluate(np_model.apply, best_params, key1, batch1)\n",
    "        loss_baseline = evaluate(baseline_model.apply, best_baseline_params, key2, batch2)\n",
    "        losses_ab_eval.append(loss_ab)\n",
    "        losses_baseline_eval.append(loss_baseline)\n",
    "    return np_model, best_params, best_baseline_params, losses_ab_train, losses_ab_eval, losses_baseline_train, losses_baseline_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa9e43b6e81c85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "d0aa9e43b6e81c85",
    "outputId": "8c8880fb-329a-4e55-99bd-f9e27bd665c5",
    "ExecuteTime": {
     "end_time": "2024-06-04T20:31:19.331344200Z",
     "start_time": "2024-06-04T20:31:19.326445700Z"
    }
   },
   "outputs": [],
   "source": [
    "# model,params,baseline_params,screenernet_losses, np_sn_losses, baseline_losses=train(dataset_train, dataset_test, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc)\n",
    "np_model, best_params, best_baseline_params, losses_ab_train, losses_ab_eval, losses_baseline_train, losses_baseline_eval = train_active_bias(dataset_train, dataset_test, dataset_size, context_size, num_epochs, rng, kl_penalty, num_posterior_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67626662884793",
   "metadata": {
    "id": "7b67626662884793",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.320294400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test predictions on functions from the training-distribution\n",
    "def f(\n",
    "    key: flax.typing.PRNGKey,\n",
    "    x: jax.Array,\n",
    "    noise_scale: float = 0.01,\n",
    "    mixture_prob: float = 0.5,\n",
    "    corrupt: bool = True\n",
    "):\n",
    "    key_noise, key_mixture = jax.random.split(key)\n",
    "\n",
    "    noise = jax.random.normal(key, x.shape) * noise_scale\n",
    "\n",
    "    # return choice * (jnp.sin(2 * jnp.pi * x / 2)) + (1 - choice) * (jnp.cos(2 * jnp.pi * 2 * x)) + corrupt * noise\n",
    "    return(-2-jnp.cos(2 * jnp.pi * x)) + corrupt * noise\n",
    "key = jax.random.key(4)\n",
    "key1, key2, key, rng = jax.random.split(key, 4)\n",
    "keys_test = jax.random.split(key1, (test_resolution,))\n",
    "x_test = jnp.linspace(-1, 1, test_resolution)\n",
    "y_test = jax.vmap(partial(f, corrupt=False))(keys_test, x_test)\n",
    "x_train, y_train = data_sampler(key2)\n",
    "x_train, y_train = x_train[..., None], y_train[..., None]\n",
    "\n",
    "# Split into context- and target-points.\n",
    "X, x_predict_train = jnp.split(x_train, indices_or_sections=(context_size, ))\n",
    "y, y_predict_train = jnp.split(y_train, indices_or_sections=(context_size, ))\n",
    "# Compute ELBO over batch of datasets\n",
    "means, stds = np_model.apply(\n",
    "    best_params, \n",
    "    X[:, None], y[:, None], x_test[:, None],\n",
    "    k=1,\n",
    "    rngs={'default': key}\n",
    ")\n",
    "\n",
    "print('prediction-shape', means.shape, stds.shape)  # dim: (len(x_test), k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(means.shape[1]):\n",
    "    plt.plot(x_test, means[:, i], color='black', alpha=0.2)  # Mixture-components\n",
    "    plt.fill_between(\n",
    "        x_test,\n",
    "        means[:, i, 0] + stds[:, i, 0],\n",
    "        means[:, i, 0] - stds[:, i, 0],\n",
    "        color='blue', alpha=0.2\n",
    "    )\n",
    "\n",
    "plt.scatter(X, y, color='green', label='context')\n",
    "plt.scatter(x_predict_train, y_predict_train, color='red', label='target')\n",
    "\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.321292100Z"
    }
   },
   "id": "77d07e001644e28b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(losses_baseline_train, label='Baseline Training error')\n",
    "#plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(baseline_losses)))\n",
    "plt.plot(losses_ab_train, label='Active Bias Training error')\n",
    "#plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(np_sn_losses)))\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.321292100Z"
    }
   },
   "id": "d2671e6e55426978",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8973e513093a0c4",
   "metadata": {
    "id": "b8973e513093a0c4",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.322351800Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_baseline_eval, label='Baseline Training error')\n",
    "#plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(baseline_losses)))\n",
    "plt.plot(losses_ab_eval, label='Active Bias Training error')\n",
    "#plt.plot(jnp.ufunc(jnp.minimum, nin=2, nout=1).accumulate(jnp.asarray(np_sn_losses)))\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8711dd89e14a9",
   "metadata": {
    "id": "67d8711dd89e14a9",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.324337600Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(means.shape[1]):\n",
    "    plt.plot(x_test, means[:, i], color='black', alpha=0.2)  # Mixture-components\n",
    "    plt.fill_between(\n",
    "        x_test,\n",
    "        means[:, i, 0] + stds[:, i, 0],\n",
    "        means[:, i, 0] - stds[:, i, 0],\n",
    "        color='blue', alpha=0.2\n",
    "    )\n",
    "\n",
    "plt.scatter(X, y, color='green', label='context')\n",
    "plt.scatter(x_predict_train, y_predict_train, color='red', label='target')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.327323600Z"
    }
   },
   "id": "25d252c61957a47f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:31:19.329349700Z"
    }
   },
   "id": "49e0f0d9dbb80662",
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
